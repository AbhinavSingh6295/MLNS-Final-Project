{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LinkPrediction_YoutubeNetwork.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04bf5c2c461942b3942c6d24a2d08dfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_146470327a434e35984afa74a00b5f21","IPY_MODEL_0259b06bc3ff4e26b171d28fd0ec5993","IPY_MODEL_3f66e4519a014bf48c2f2ce9fc43f203"],"layout":"IPY_MODEL_0e9ca43690ee42b7b7bda96fa14187b9"}},"146470327a434e35984afa74a00b5f21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_60f613e73e144bfeb863b9ec827e9181","placeholder":"​","style":"IPY_MODEL_92d5a3e0e5d0405192946e506fa4a535","value":"Computing transition probabilities: 100%"}},"0259b06bc3ff4e26b171d28fd0ec5993":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cad84f28c15428d94c0818939805d84","max":18648,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba8dacea955149ca95b19fabc431fdac","value":18648}},"3f66e4519a014bf48c2f2ce9fc43f203":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb6fdcef59bd45ab89b0f6a99f9db51b","placeholder":"​","style":"IPY_MODEL_99d24eb917bf42308d3e3736ac94ece4","value":" 18648/18648 [00:01&lt;00:00, 16260.10it/s]"}},"0e9ca43690ee42b7b7bda96fa14187b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60f613e73e144bfeb863b9ec827e9181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92d5a3e0e5d0405192946e506fa4a535":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cad84f28c15428d94c0818939805d84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba8dacea955149ca95b19fabc431fdac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb6fdcef59bd45ab89b0f6a99f9db51b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99d24eb917bf42308d3e3736ac94ece4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"AoSjtrUTpmIx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqkU0sAJLBzJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9efefd9e-877c-41d5-acff-23fdd2372f3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting snap\n","  Downloading snap-0.5.tar.gz (24 kB)\n","Building wheels for collected packages: snap\n","  Building wheel for snap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for snap: filename=snap-0.5-py3-none-any.whl size=19398 sha256=73600c24091b3640a4f731af3d13d25b45a64e5f4b2eecb2707b061bbe409585\n","  Stored in directory: /root/.cache/pip/wheels/3e/5b/9c/8aabe4a690163d236a0010f861eaf746527761a5f4122ab466\n","Successfully built snap\n","Installing collected packages: snap\n","Successfully installed snap-0.5\n"]}],"source":["!pip install snap\n","import snap\n","import math\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import csv\n","from matplotlib import pyplot as plt\n","import csv\n","import random\n","from sklearn import linear_model\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier, MLPRegressor\n","from sklearn.metrics import classification_report,confusion_matrix\n","from collections import Counter\n","from tqdm import tqdm"]},{"cell_type":"code","source":["# Mouting Google drive for accessing the data\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kz1RphzHQugR","outputId":"41e8fb52-b776-4043-8303-d717ae301d03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Parameters"],"metadata":{"id":"msjVg3mKQT64"}},{"cell_type":"code","source":["# We have used the sample size of 0.1\n","sample_size = 0.1\n","test_size = 0.25\n","scale = True"],"metadata":{"id":"3QoRzF8rQTfO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load and prepare data"],"metadata":{"id":"TuwvqnvHpoYz"}},{"cell_type":"code","source":["df_0 = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/080329/0.txt\", sep = '\\t', header = None)\n","df_1 = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/080329/1.txt\", sep = \"\\t\", header = None)\n","df_2 = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/080329/2.txt\", sep = \"\\t\", header = None)\n","df_3 = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/080329/3.txt\", sep = \"\\t\", header = None)"],"metadata":{"id":"_bkUbCSXMi14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols = ['video_ID', \"uploader\", \"age\", \"category\", \"length\", \"views\", \"rate\", \"ratings\", \"comments\", \n","        \"related_video_1\", \"related_video_2\", \"related_video_3\", \"related_video_4\", \n","        \"related_video_5\", \"related_video_6\", \"related_video_7\", \"related_video_8\", \n","        \"related_video_9\", \"related_video_10\", \"related_video_11\", \"related_video_12\",\n","        \"related_video_13\", \"related_video_14\", \"related_video_15\", \"related_video_16\",\n","        \"related_video_17\", \"related_video_18\", \"related_video_19\", \"related_video_20\"]"],"metadata":{"id":"U_UsKbY0NQrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_0.columns = cols\n","df_1.columns = cols\n","df_2.columns = cols\n","df_3.columns = cols\n","node_information = pd.concat([df_0, df_1, df_2, df_3])"],"metadata":{"id":"KwPeLsImQVch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_information.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"u4E-HPiuRHpH","outputId":"eb079dc8-ae22-427f-c314-a321ea9f563e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      video_ID            uploader     age          category  length  \\\n","0  kAhMJHat0cI            LisaNova  1137.0     Entertainment   284.0   \n","1  RTNXfv5zNss           machinima  1137.0     Entertainment  1206.0   \n","2  74N_Yn27ZWc             sxephil  1137.0     Entertainment   275.0   \n","3  Oi9VzgDseL8  ukjonasbrothersfan  1137.0     Entertainment    62.0   \n","4  dznIgkR50Os          toniCHRYSA  1137.0  Film & Animation   389.0   \n","\n","      views  rate  ratings  comments related_video_1  ... related_video_11  \\\n","0  141189.0  4.30   3090.0    1976.0     RLBx7ccIOkM  ...      trcpKpl63Vk   \n","1  123367.0  4.91   2115.0    1822.0     vg_ZkoUteLM  ...      2M1Ji9ZijGI   \n","2  140875.0  4.55   2435.0    1636.0     3oUZSp7PDEw  ...      HQoodoBECx0   \n","3   81526.0  4.84   1089.0    1354.0     ArG3RwRuFjs  ...      wlqmzYWx3GM   \n","4  108391.0  4.84    702.0    1365.0     u0gLZq-doD0  ...      K2pu3JTwrIo   \n","\n","  related_video_12 related_video_13 related_video_14 related_video_15  \\\n","0      AMUwjdn7Dyg      cDq0B4zEUes      iRkiqb0qAw8      LSHHdD3Kfvk   \n","1      ylzYrQQERNU      ouZOrqhbYlo      Owxw44ONFE8      Hlhfx0wda1Y   \n","2      KC-noDgddJo      oRLpMlVCqZ4      wZeTFrsxKDg      HEYoRm8IlgY   \n","3      7UKQCfifJdQ      PG8tzJQ0vok      qBhY3aaR5RM      6q1O07LHG0I   \n","4      zUtdxCxQjJo      _I49oMFLcwM      ZYsjGiYUHTg      SSeOPuGFxWs   \n","\n","  related_video_16 related_video_17 related_video_18 related_video_19  \\\n","0      83mHSIdbKD8      zq_geLMFpf0      X46looQt4kg      -mIe0h3zFVM   \n","1      qpKbJX9FwZQ      ikJpyjfiBVA      5DbePZddEuU      dNwVEo0YWyI   \n","2      MRVmULZNcf8      sw4b8NejuVM      ZlS_2UNURR8      jKfblDxvV08   \n","3      cEG2QFxi8kw      NR8D2udWwmg      7GWo3kyhp-8      hWannl8LuOw   \n","4      SvkWyelwTBg      KtiI7vcIYc0      qGpFuTggwL8      ngKtowElOx8   \n","\n","  related_video_20  \n","0      D9HeJYGdmRE  \n","1      U8dCpYhN528  \n","2      7LBEdFWCYlU  \n","3      M35thjwHVaw  \n","4      DAfV3mbVeec  \n","\n","[5 rows x 29 columns]"],"text/html":["\n","  <div id=\"df-8ff8ad28-c46c-42ef-85ad-39b3f002d608\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_ID</th>\n","      <th>uploader</th>\n","      <th>age</th>\n","      <th>category</th>\n","      <th>length</th>\n","      <th>views</th>\n","      <th>rate</th>\n","      <th>ratings</th>\n","      <th>comments</th>\n","      <th>related_video_1</th>\n","      <th>...</th>\n","      <th>related_video_11</th>\n","      <th>related_video_12</th>\n","      <th>related_video_13</th>\n","      <th>related_video_14</th>\n","      <th>related_video_15</th>\n","      <th>related_video_16</th>\n","      <th>related_video_17</th>\n","      <th>related_video_18</th>\n","      <th>related_video_19</th>\n","      <th>related_video_20</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kAhMJHat0cI</td>\n","      <td>LisaNova</td>\n","      <td>1137.0</td>\n","      <td>Entertainment</td>\n","      <td>284.0</td>\n","      <td>141189.0</td>\n","      <td>4.30</td>\n","      <td>3090.0</td>\n","      <td>1976.0</td>\n","      <td>RLBx7ccIOkM</td>\n","      <td>...</td>\n","      <td>trcpKpl63Vk</td>\n","      <td>AMUwjdn7Dyg</td>\n","      <td>cDq0B4zEUes</td>\n","      <td>iRkiqb0qAw8</td>\n","      <td>LSHHdD3Kfvk</td>\n","      <td>83mHSIdbKD8</td>\n","      <td>zq_geLMFpf0</td>\n","      <td>X46looQt4kg</td>\n","      <td>-mIe0h3zFVM</td>\n","      <td>D9HeJYGdmRE</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>RTNXfv5zNss</td>\n","      <td>machinima</td>\n","      <td>1137.0</td>\n","      <td>Entertainment</td>\n","      <td>1206.0</td>\n","      <td>123367.0</td>\n","      <td>4.91</td>\n","      <td>2115.0</td>\n","      <td>1822.0</td>\n","      <td>vg_ZkoUteLM</td>\n","      <td>...</td>\n","      <td>2M1Ji9ZijGI</td>\n","      <td>ylzYrQQERNU</td>\n","      <td>ouZOrqhbYlo</td>\n","      <td>Owxw44ONFE8</td>\n","      <td>Hlhfx0wda1Y</td>\n","      <td>qpKbJX9FwZQ</td>\n","      <td>ikJpyjfiBVA</td>\n","      <td>5DbePZddEuU</td>\n","      <td>dNwVEo0YWyI</td>\n","      <td>U8dCpYhN528</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>74N_Yn27ZWc</td>\n","      <td>sxephil</td>\n","      <td>1137.0</td>\n","      <td>Entertainment</td>\n","      <td>275.0</td>\n","      <td>140875.0</td>\n","      <td>4.55</td>\n","      <td>2435.0</td>\n","      <td>1636.0</td>\n","      <td>3oUZSp7PDEw</td>\n","      <td>...</td>\n","      <td>HQoodoBECx0</td>\n","      <td>KC-noDgddJo</td>\n","      <td>oRLpMlVCqZ4</td>\n","      <td>wZeTFrsxKDg</td>\n","      <td>HEYoRm8IlgY</td>\n","      <td>MRVmULZNcf8</td>\n","      <td>sw4b8NejuVM</td>\n","      <td>ZlS_2UNURR8</td>\n","      <td>jKfblDxvV08</td>\n","      <td>7LBEdFWCYlU</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Oi9VzgDseL8</td>\n","      <td>ukjonasbrothersfan</td>\n","      <td>1137.0</td>\n","      <td>Entertainment</td>\n","      <td>62.0</td>\n","      <td>81526.0</td>\n","      <td>4.84</td>\n","      <td>1089.0</td>\n","      <td>1354.0</td>\n","      <td>ArG3RwRuFjs</td>\n","      <td>...</td>\n","      <td>wlqmzYWx3GM</td>\n","      <td>7UKQCfifJdQ</td>\n","      <td>PG8tzJQ0vok</td>\n","      <td>qBhY3aaR5RM</td>\n","      <td>6q1O07LHG0I</td>\n","      <td>cEG2QFxi8kw</td>\n","      <td>NR8D2udWwmg</td>\n","      <td>7GWo3kyhp-8</td>\n","      <td>hWannl8LuOw</td>\n","      <td>M35thjwHVaw</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>dznIgkR50Os</td>\n","      <td>toniCHRYSA</td>\n","      <td>1137.0</td>\n","      <td>Film &amp; Animation</td>\n","      <td>389.0</td>\n","      <td>108391.0</td>\n","      <td>4.84</td>\n","      <td>702.0</td>\n","      <td>1365.0</td>\n","      <td>u0gLZq-doD0</td>\n","      <td>...</td>\n","      <td>K2pu3JTwrIo</td>\n","      <td>zUtdxCxQjJo</td>\n","      <td>_I49oMFLcwM</td>\n","      <td>ZYsjGiYUHTg</td>\n","      <td>SSeOPuGFxWs</td>\n","      <td>SvkWyelwTBg</td>\n","      <td>KtiI7vcIYc0</td>\n","      <td>qGpFuTggwL8</td>\n","      <td>ngKtowElOx8</td>\n","      <td>DAfV3mbVeec</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 29 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ff8ad28-c46c-42ef-85ad-39b3f002d608')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8ff8ad28-c46c-42ef-85ad-39b3f002d608 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8ff8ad28-c46c-42ef-85ad-39b3f002d608');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["node_information = node_information[node_information['age'].notna()]"],"metadata":{"id":"OhMmz4PXOPYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_information = node_information.sample(frac=sample_size)"],"metadata":{"id":"7XefHwyoQlrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_information.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0Lh8AkCJPB9","outputId":"21898244-a8cb-43df-f97e-b56686cbdd59"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18648, 29)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# Create graph"],"metadata":{"id":"Da3m1v_xpwWV"}},{"cell_type":"code","source":["def buildDirectedGraph(df):\n","    G = nx.DiGraph()\n","    views = {}\n","    print(\"Adding nodes to Graph\")\n","    edgeList = open('youtube.edgelist', 'w') \n","    for _, row in df.iterrows():\n","        nodeID = row[0]\n","        views[nodeID] = row[5] if row[5] == 0 else math.log(row[5], 10)\n","        if not G.has_node(nodeID):\n","            G.add_node(nodeID)\n","\n","    newToOld = {}\n","    oldToNew = {}\n","    newIDIter = 0\n","    for _, row in df.iterrows():\n","        edges = row[9:]\n","        nodeID = row[0]\n","        if nodeID not in oldToNew:\n","            oldToNew[nodeID] = newIDIter\n","            newToOld[newIDIter] = nodeID\n","            newIDIter += 1\n","        for edge in edges:\n","            if not G.has_node(edge):\n","                continue\n","            G.add_edge(nodeID, edge)\n","            if edge not in oldToNew:\n","                oldToNew[edge] = newIDIter\n","                newToOld[newIDIter] = edge\n","                newIDIter += 1\n","            edgeList.write(str(oldToNew[nodeID]) + ' ' + str(oldToNew[edge]) + '\\n')\n","\n","    print(\"Graph loaded\")\n","    print(str(nx.number_of_nodes(G)) + \" nodes\")\n","    edgeList.close()\n","    np.save(\"newToOldMap.npy\", newToOld)\n","    return G, views"],"metadata":{"id":"ItuGTMT8Tp7W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def buildUndirectedGraph(df):\n","    G = nx.Graph()\n","    views = {}\n","    print(\"Adding nodes to Graph\")\n","    edgeList = open('youtube.edgelist', 'w') \n","    for _, row in df.iterrows():\n","        nodeID = row[0]\n","        views[nodeID] = row[5] if row[5] == 0 else math.log(row[5], 10)\n","        if not G.has_node(nodeID):\n","            G.add_node(nodeID)\n","\n","    newToOld = {}\n","    oldToNew = {}\n","    newIDIter = 0\n","    for _, row in df.iterrows():\n","        edges = row[9:]\n","        nodeID = row[0]\n","        if nodeID not in oldToNew:\n","            oldToNew[nodeID] = newIDIter\n","            newToOld[newIDIter] = nodeID\n","            newIDIter += 1\n","        for edge in edges:\n","            if not G.has_node(edge):\n","                continue\n","            G.add_edge(nodeID, edge)\n","            if edge not in oldToNew:\n","                oldToNew[edge] = newIDIter\n","                newToOld[newIDIter] = edge\n","                newIDIter += 1\n","            edgeList.write(str(oldToNew[nodeID]) + ' ' + str(oldToNew[edge]) + '\\n')\n","\n","    print(\"Graph loaded\")\n","    print(str(nx.number_of_nodes(G)) + \" nodes\")\n","    edgeList.close()\n","    np.save(\"newToOldMap.npy\", newToOld)\n","    return G, views"],"metadata":{"id":"BfCKOkqWMMSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["G, views = buildDirectedGraph(node_information)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eKyOspZWYe-","outputId":"c4c46774-d1ca-4a6c-bbaa-67cdeadd1976"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adding nodes to Graph\n","Graph loaded\n","18648 nodes\n"]}]},{"cell_type":"code","source":["G_undirected, views = buildUndirectedGraph(node_information)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vARjv-nlMBtB","outputId":"6febc8f6-3866-44ab-de69-962bceac6b26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Adding nodes to Graph\n","Graph loaded\n","18648 nodes\n"]}]},{"cell_type":"markdown","source":["# Create source-target link dataset"],"metadata":{"id":"GYRrORiNp4Ow"}},{"cell_type":"code","source":["nodelist = [i for i in G.nodes]"],"metadata":{"id":"9coXOo1UYqaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepareDatasets(G, nodelist):\n","  allEdges = list(G.edges())\n","  edgesSet = set(allEdges)\n","  numEdges = len(allEdges)\n","  goalPositiveExamples = 0.5*numEdges\n","  goalNegativeExamples = 0.5*numEdges\n","  numPositiveExamples = 0\n","  numNegativeExamples = 0\n","  df_linkset = pd.DataFrame()\n","  source = []\n","  target = []\n","  link = []\n","  nodePairsInDataset = set()\n","  while numNegativeExamples < goalNegativeExamples:\n","    node1 = random.choice(nodelist)\n","    node2 = random.choice(nodelist)\n","    if node1 == node2:\n","      continue\n","    potentialEdge = node1, node2\n","    if potentialEdge not in edgesSet and potentialEdge not in nodePairsInDataset:\n","      source.append(node1)\n","      target.append(node2)\n","      link.append(0)\n","      nodePairsInDataset.add(potentialEdge)\n","      numNegativeExamples += 1\n","  random.shuffle(allEdges)\n","  for i in range(int(goalPositiveExamples)):\n","    edge = allEdges[i]\n","    source.append(edge[0])\n","    target.append(edge[1])\n","    link.append(1)\n","\n","  df_linkset['source'] = []\n","  df_linkset['target'] = []\n","  df_linkset['link'] = []\n","  df_linkset['source'] = source\n","  df_linkset['target'] = target\n","  df_linkset['link'] = link\n","\n","  return df_linkset"],"metadata":{"id":"dvxE0Ey17TES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_linkset = prepareDatasets(G, nodelist)"],"metadata":{"id":"N7J-X12p9Jfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#shuffle the dataframe:\n","df = df_linkset.sample(frac=1).reset_index(drop=True)"],"metadata":{"id":"te27fmDD-b1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/source_target_links.csv\")"],"metadata":{"id":"Pr5XqBVfqKDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnvZzspRwtXi","outputId":"ca029600-a0fb-4354-c59b-0014495a38d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10077 entries, 0 to 10076\n","Data columns (total 3 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   source  10077 non-null  object\n"," 1   target  10077 non-null  object\n"," 2   link    10077 non-null  int64 \n","dtypes: int64(1), object(2)\n","memory usage: 236.3+ KB\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"WZTJZdxyVbp3","outputId":"c7005ecf-8ffa-459d-93d6-eb728ea881ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        source       target  link\n","0  rkfzhYMj35k  qN3kC_4xURA     0\n","1  J85b993voac  UoYLkSk6k04     1\n","2  r_JC8mFoBPI  PsM7xLkORWg     1\n","3  U5Ojfo2qkNY  FAOEOgrpRZk     0\n","4  SjqRGPG2Yas  84Fng-NSWys     0"],"text/html":["\n","  <div id=\"df-bfa7908d-8f3b-4c5f-a8c6-0c30984eb851\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rkfzhYMj35k</td>\n","      <td>qN3kC_4xURA</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>J85b993voac</td>\n","      <td>UoYLkSk6k04</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>r_JC8mFoBPI</td>\n","      <td>PsM7xLkORWg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U5Ojfo2qkNY</td>\n","      <td>FAOEOgrpRZk</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SjqRGPG2Yas</td>\n","      <td>84Fng-NSWys</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bfa7908d-8f3b-4c5f-a8c6-0c30984eb851')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bfa7908d-8f3b-4c5f-a8c6-0c30984eb851 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bfa7908d-8f3b-4c5f-a8c6-0c30984eb851');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tm8tOQ88Jfey","outputId":"174cc0aa-8fce-476c-9380-75e17a19d360"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10077, 3)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["# Node2Vec Embeddings"],"metadata":{"id":"BQW-8f7XsyUA"}},{"cell_type":"code","source":["!pip install node2vec\n","from node2vec import Node2Vec as n2v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wiBF6-zys3WH","outputId":"fcb1be21-c09f-4d7d-bd36-adbc0af7715d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting node2vec\n","  Downloading node2vec-0.4.3.tar.gz (4.6 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from node2vec) (2.6.3)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from node2vec) (3.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.21.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from node2vec) (4.63.0)\n","Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.1.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (5.2.1)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (1.4.1)\n","Building wheels for collected packages: node2vec\n","  Building wheel for node2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for node2vec: filename=node2vec-0.4.3-py3-none-any.whl size=5980 sha256=5fc99c54af09f34daa0d437a330ba061b8a318652537905a0feabfd8fa64948a\n","  Stored in directory: /root/.cache/pip/wheels/07/62/78/5202cb8c03cbf1593b48a8a442fca8ceec2a8c80e22318bae9\n","Successfully built node2vec\n","Installing collected packages: node2vec\n","Successfully installed node2vec-0.4.3\n"]}]},{"cell_type":"code","source":["# Time ~ 5 mins for 10% sample\n","graph_emb = n2v(G_undirected, dimensions = 16)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["04bf5c2c461942b3942c6d24a2d08dfa","146470327a434e35984afa74a00b5f21","0259b06bc3ff4e26b171d28fd0ec5993","3f66e4519a014bf48c2f2ce9fc43f203","0e9ca43690ee42b7b7bda96fa14187b9","60f613e73e144bfeb863b9ec827e9181","92d5a3e0e5d0405192946e506fa4a535","3cad84f28c15428d94c0818939805d84","ba8dacea955149ca95b19fabc431fdac","eb6fdcef59bd45ab89b0f6a99f9db51b","99d24eb917bf42308d3e3736ac94ece4"]},"id":"IpDsed-js3Yu","outputId":"227582a9-87f7-4253-866f-2104cdfce3e1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Computing transition probabilities:   0%|          | 0/18648 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04bf5c2c461942b3942c6d24a2d08dfa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Generating walks (CPU: 1): 100%|██████████| 10/10 [04:41<00:00, 28.17s/it]\n"]}]},{"cell_type":"code","source":["window_size = 1\n","minimum_count = 1\n","batchofwords = 4\n","\n","model = graph_emb.fit(window=window_size, min_count=minimum_count, batch_words=batchofwords)"],"metadata":{"id":"HMDPUyRvJsQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_df = (pd.DataFrame([model.wv.get_vector(str(n)) for n in G_undirected.nodes()], \n","                       index = G_undirected.nodes()))"],"metadata":{"id":"w2fnIAiqs3bL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#emb_df['Node'] = G_undirected.nodes()"],"metadata":{"id":"rERhzuGHVOOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_df.to_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/node_embedding_10perc_sample.csv\", index=True)"],"metadata":{"id":"fH5D41vAL690"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":250},"id":"TUBYOziYs3fc","outputId":"4dc78097-4349-425d-c8be-fb069191cf92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   0         1         2         3         4         5   \\\n","glu82z2gtUE -0.956772 -1.795235  0.327472 -1.950309 -2.374627 -0.255067   \n","JiWpxoamaPw -0.019680  0.001526  0.017977 -0.003966  0.022122 -0.028697   \n","easbmYk6354 -0.016780  0.024632 -0.017792  0.006114 -0.025209  0.021780   \n","Vt246AnI4N4  0.013505  0.015744  0.015141 -0.006178  0.008078 -0.006827   \n","Mn4Z7lY00rM -0.003796 -0.027584  0.030226  0.018386  0.000260 -0.025266   \n","\n","                   6         7         8         9         10        11  \\\n","glu82z2gtUE  0.357089  2.419735  0.864712  2.360615 -0.258884 -1.660069   \n","JiWpxoamaPw -0.011315 -0.003962  0.025423  0.003222  0.014192  0.029313   \n","easbmYk6354  0.022251 -0.011884 -0.005356  0.016459 -0.009872 -0.007997   \n","Vt246AnI4N4  0.017503  0.000102 -0.010080  0.007186  0.013232  0.002477   \n","Mn4Z7lY00rM -0.029702 -0.017094 -0.011433  0.018266  0.029322  0.025810   \n","\n","                   12        13        14        15  \n","glu82z2gtUE -2.121730 -0.490644  0.647794 -0.822499  \n","JiWpxoamaPw  0.023789 -0.026975 -0.031161  0.016586  \n","easbmYk6354  0.030256 -0.007121  0.012483  0.000437  \n","Vt246AnI4N4 -0.030334 -0.005677  0.006790 -0.021627  \n","Mn4Z7lY00rM -0.003986  0.021151  0.029245  0.008653  "],"text/html":["\n","  <div id=\"df-a2b9f7a1-26e6-48ad-afda-3f425de7b67f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>glu82z2gtUE</th>\n","      <td>-0.956772</td>\n","      <td>-1.795235</td>\n","      <td>0.327472</td>\n","      <td>-1.950309</td>\n","      <td>-2.374627</td>\n","      <td>-0.255067</td>\n","      <td>0.357089</td>\n","      <td>2.419735</td>\n","      <td>0.864712</td>\n","      <td>2.360615</td>\n","      <td>-0.258884</td>\n","      <td>-1.660069</td>\n","      <td>-2.121730</td>\n","      <td>-0.490644</td>\n","      <td>0.647794</td>\n","      <td>-0.822499</td>\n","    </tr>\n","    <tr>\n","      <th>JiWpxoamaPw</th>\n","      <td>-0.019680</td>\n","      <td>0.001526</td>\n","      <td>0.017977</td>\n","      <td>-0.003966</td>\n","      <td>0.022122</td>\n","      <td>-0.028697</td>\n","      <td>-0.011315</td>\n","      <td>-0.003962</td>\n","      <td>0.025423</td>\n","      <td>0.003222</td>\n","      <td>0.014192</td>\n","      <td>0.029313</td>\n","      <td>0.023789</td>\n","      <td>-0.026975</td>\n","      <td>-0.031161</td>\n","      <td>0.016586</td>\n","    </tr>\n","    <tr>\n","      <th>easbmYk6354</th>\n","      <td>-0.016780</td>\n","      <td>0.024632</td>\n","      <td>-0.017792</td>\n","      <td>0.006114</td>\n","      <td>-0.025209</td>\n","      <td>0.021780</td>\n","      <td>0.022251</td>\n","      <td>-0.011884</td>\n","      <td>-0.005356</td>\n","      <td>0.016459</td>\n","      <td>-0.009872</td>\n","      <td>-0.007997</td>\n","      <td>0.030256</td>\n","      <td>-0.007121</td>\n","      <td>0.012483</td>\n","      <td>0.000437</td>\n","    </tr>\n","    <tr>\n","      <th>Vt246AnI4N4</th>\n","      <td>0.013505</td>\n","      <td>0.015744</td>\n","      <td>0.015141</td>\n","      <td>-0.006178</td>\n","      <td>0.008078</td>\n","      <td>-0.006827</td>\n","      <td>0.017503</td>\n","      <td>0.000102</td>\n","      <td>-0.010080</td>\n","      <td>0.007186</td>\n","      <td>0.013232</td>\n","      <td>0.002477</td>\n","      <td>-0.030334</td>\n","      <td>-0.005677</td>\n","      <td>0.006790</td>\n","      <td>-0.021627</td>\n","    </tr>\n","    <tr>\n","      <th>Mn4Z7lY00rM</th>\n","      <td>-0.003796</td>\n","      <td>-0.027584</td>\n","      <td>0.030226</td>\n","      <td>0.018386</td>\n","      <td>0.000260</td>\n","      <td>-0.025266</td>\n","      <td>-0.029702</td>\n","      <td>-0.017094</td>\n","      <td>-0.011433</td>\n","      <td>0.018266</td>\n","      <td>0.029322</td>\n","      <td>0.025810</td>\n","      <td>-0.003986</td>\n","      <td>0.021151</td>\n","      <td>0.029245</td>\n","      <td>0.008653</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2b9f7a1-26e6-48ad-afda-3f425de7b67f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a2b9f7a1-26e6-48ad-afda-3f425de7b67f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a2b9f7a1-26e6-48ad-afda-3f425de7b67f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["def emb_features(df):\n","  sums = []\n","  avgs = []\n","  prods = []\n","\n","  for i in tqdm(range(len(df))):\n","    rep_source = list(np.array(emb_df[emb_df.index == df.iloc[i]['source']])[0])\n","    rep_target = list(np.array(emb_df[emb_df.index == df.iloc[i]['target']])[0]) \n","    sum = [a+b for a,b in zip(rep_source, rep_target)]\n","    sums.append(np.array(sum))\n","    avg = [(a+b)/2 for a,b in zip(rep_source, rep_target)]\n","    avgs.append(np.array(avg))\n","    prod = [a*b for a,b in zip(rep_source, rep_target)]\n","    prods.append(np.array(prod))\n","  emb_sum = pd.DataFrame(sums, columns = ['sum_0', 'sum_1', 'sum_2', 'sum_3', 'sum_4', 'sum_5', 'sum_6', 'sum_7', 'sum_8', 'sum_9', 'sum_10', 'sum_11', 'sum_12', 'sum_13', 'sum_14', 'sum_15'])\n","  emb_avg = pd.DataFrame(avgs, columns = ['avg_0', 'avg_1', 'avg_2', 'avg_3', 'avg_4', 'avg_5', 'avg_6', 'avg_7', 'avg_8', 'avg_9', 'avg_10', 'avg_11', 'avg_12', 'avg_13', 'avg_14', 'avg_15'])\n","  emb_prod = pd.DataFrame(prods, columns = ['prod_0', 'prod_1', 'prod_2', 'prod_3', 'prod_4', 'prod_5', 'prod_6', 'prod_7', 'prod_8', 'prod_9', 'prod_10', 'prod_11', 'prod_12', 'prod_13', 'prod_14', 'prod_15'])\n","  final_df = pd.concat([df, emb_sum, emb_avg, emb_prod], axis=1)\n","  return final_df"],"metadata":{"id":"Wzo2rr_4NoKR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = emb_features(df)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"KlKNfqe8QLLQ","outputId":"376844dc-49e3-4c18-c907-6731e7890f45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        source       target  link     sum_0     sum_1     sum_2     sum_3  \\\n","0  rkfzhYMj35k  qN3kC_4xURA     0 -1.413013 -1.778044 -0.372599 -1.539065   \n","1  J85b993voac  UoYLkSk6k04     1 -1.626465 -2.162876 -1.854273 -3.671918   \n","2  r_JC8mFoBPI  PsM7xLkORWg     1  3.892650  0.486121 -1.137511 -2.722686   \n","3  U5Ojfo2qkNY  FAOEOgrpRZk     0 -1.054151 -1.225188  0.573736 -4.958475   \n","4  SjqRGPG2Yas  84Fng-NSWys     0 -0.667670 -2.896195 -2.572122 -2.121985   \n","\n","      sum_4     sum_5     sum_6  ...    prod_6    prod_7    prod_8    prod_9  \\\n","0 -2.383949  1.203595 -0.034141  ... -0.000121  0.016491  0.007669  0.047752   \n","1 -6.044376  0.502586 -2.740177  ...  1.289787  3.212803 -1.625392  0.004091   \n","2 -5.124178  3.502509 -4.314734  ...  4.624921  0.642589  0.416287  8.001205   \n","3 -3.577811  0.855308 -1.804483  ... -1.023890  0.321491  1.472359  0.550672   \n","4 -0.676632  3.341443 -0.198435  ... -1.635641  5.770884  0.554119  2.297662   \n","\n","    prod_10   prod_11   prod_12   prod_13   prod_14   prod_15  \n","0  0.015984  0.041321 -0.011748  0.025275  0.002080 -0.005934  \n","1 -0.226572 -0.966053 -0.657066  0.724920 -1.298165  2.810447  \n","2  4.392770  0.068829  2.667431  0.438939  1.149678  0.007986  \n","3  0.112404  0.261672 -0.786307 -0.126158 -0.929809  2.448215  \n","4  0.066693  0.828228 -1.184759  1.327445 -0.588942  4.710648  \n","\n","[5 rows x 51 columns]"],"text/html":["\n","  <div id=\"df-13a9e570-931e-4f74-bf76-505c7a9d13da\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>link</th>\n","      <th>sum_0</th>\n","      <th>sum_1</th>\n","      <th>sum_2</th>\n","      <th>sum_3</th>\n","      <th>sum_4</th>\n","      <th>sum_5</th>\n","      <th>sum_6</th>\n","      <th>...</th>\n","      <th>prod_6</th>\n","      <th>prod_7</th>\n","      <th>prod_8</th>\n","      <th>prod_9</th>\n","      <th>prod_10</th>\n","      <th>prod_11</th>\n","      <th>prod_12</th>\n","      <th>prod_13</th>\n","      <th>prod_14</th>\n","      <th>prod_15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rkfzhYMj35k</td>\n","      <td>qN3kC_4xURA</td>\n","      <td>0</td>\n","      <td>-1.413013</td>\n","      <td>-1.778044</td>\n","      <td>-0.372599</td>\n","      <td>-1.539065</td>\n","      <td>-2.383949</td>\n","      <td>1.203595</td>\n","      <td>-0.034141</td>\n","      <td>...</td>\n","      <td>-0.000121</td>\n","      <td>0.016491</td>\n","      <td>0.007669</td>\n","      <td>0.047752</td>\n","      <td>0.015984</td>\n","      <td>0.041321</td>\n","      <td>-0.011748</td>\n","      <td>0.025275</td>\n","      <td>0.002080</td>\n","      <td>-0.005934</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>J85b993voac</td>\n","      <td>UoYLkSk6k04</td>\n","      <td>1</td>\n","      <td>-1.626465</td>\n","      <td>-2.162876</td>\n","      <td>-1.854273</td>\n","      <td>-3.671918</td>\n","      <td>-6.044376</td>\n","      <td>0.502586</td>\n","      <td>-2.740177</td>\n","      <td>...</td>\n","      <td>1.289787</td>\n","      <td>3.212803</td>\n","      <td>-1.625392</td>\n","      <td>0.004091</td>\n","      <td>-0.226572</td>\n","      <td>-0.966053</td>\n","      <td>-0.657066</td>\n","      <td>0.724920</td>\n","      <td>-1.298165</td>\n","      <td>2.810447</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>r_JC8mFoBPI</td>\n","      <td>PsM7xLkORWg</td>\n","      <td>1</td>\n","      <td>3.892650</td>\n","      <td>0.486121</td>\n","      <td>-1.137511</td>\n","      <td>-2.722686</td>\n","      <td>-5.124178</td>\n","      <td>3.502509</td>\n","      <td>-4.314734</td>\n","      <td>...</td>\n","      <td>4.624921</td>\n","      <td>0.642589</td>\n","      <td>0.416287</td>\n","      <td>8.001205</td>\n","      <td>4.392770</td>\n","      <td>0.068829</td>\n","      <td>2.667431</td>\n","      <td>0.438939</td>\n","      <td>1.149678</td>\n","      <td>0.007986</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U5Ojfo2qkNY</td>\n","      <td>FAOEOgrpRZk</td>\n","      <td>0</td>\n","      <td>-1.054151</td>\n","      <td>-1.225188</td>\n","      <td>0.573736</td>\n","      <td>-4.958475</td>\n","      <td>-3.577811</td>\n","      <td>0.855308</td>\n","      <td>-1.804483</td>\n","      <td>...</td>\n","      <td>-1.023890</td>\n","      <td>0.321491</td>\n","      <td>1.472359</td>\n","      <td>0.550672</td>\n","      <td>0.112404</td>\n","      <td>0.261672</td>\n","      <td>-0.786307</td>\n","      <td>-0.126158</td>\n","      <td>-0.929809</td>\n","      <td>2.448215</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SjqRGPG2Yas</td>\n","      <td>84Fng-NSWys</td>\n","      <td>0</td>\n","      <td>-0.667670</td>\n","      <td>-2.896195</td>\n","      <td>-2.572122</td>\n","      <td>-2.121985</td>\n","      <td>-0.676632</td>\n","      <td>3.341443</td>\n","      <td>-0.198435</td>\n","      <td>...</td>\n","      <td>-1.635641</td>\n","      <td>5.770884</td>\n","      <td>0.554119</td>\n","      <td>2.297662</td>\n","      <td>0.066693</td>\n","      <td>0.828228</td>\n","      <td>-1.184759</td>\n","      <td>1.327445</td>\n","      <td>-0.588942</td>\n","      <td>4.710648</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 51 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13a9e570-931e-4f74-bf76-505c7a9d13da')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-13a9e570-931e-4f74-bf76-505c7a9d13da button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-13a9e570-931e-4f74-bf76-505c7a9d13da');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["# Creating features"],"metadata":{"id":"3WsyZKt4tGOx"}},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/source_target_links.csv\")"],"metadata":{"id":"wE2Ue7wfuxi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(\"Unnamed: 0\", inplace = True, axis = 1)"],"metadata":{"id":"nbWijgLTCmS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYYHMfJcCuJK","outputId":"964d4340-ddc9-415d-f9c1-bcdfdec825ec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10077, 51)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["page_rank = nx.pagerank_scipy(G)\n","hub_score, authority_score = nx.hits(G)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6audRtOotHyN","outputId":"c2c5a55c-c7c5-4443-b33f-3977c64e046f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: networkx.pagerank_scipy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}]},{"cell_type":"code","source":["from networkx.algorithms.link_prediction import preferential_attachment\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from tqdm import tqdm\n","\n","def features(df):\n","\n","  age_diff = []\n","  jaccard_coef = []\n","  adamic_adar_coef = []\n","  pref_attachement_coef = []\n","  common_neigh = []\n","  # triad_features = []\n","  source_indegree = []\n","  target_indegree = []\n","  source_outdegree = []\n","  target_outdegree = []\n","  pagerank = []\n","  source_hubs = []\n","  target_authorities = []\n","\n","  for i in tqdm(range(len(df))):\n","\n","    source = df_linkset.iloc[i]['source']\n","    target = df_linkset.iloc[i]['target']\n","    idx_source, idx_target = nodelist.index(source), nodelist.index(target)\n","\n","    age_diff.append(int(list(node_information['age'])[idx_source] - list(node_information['age'])[idx_target]))\n","\n","    jaccard = nx.jaccard_coefficient(G_undirected, [(source, target)])\n","    for u, v, p in jaccard:\n","      jaccard_coef.append(p)\n","\n","    adamic_adar = nx.adamic_adar_index(G_undirected, [(source, target)])\n","    for u, v, p in adamic_adar:\n","      adamic_adar_coef.append(p)\n","\n","    pref_attachement = nx.preferential_attachment(G_undirected, [(source, target)])\n","    for u, v, p in pref_attachement:\n","      pref_attachement_coef.append(p)\n","\n","    common_neigh.append(len(sorted(nx.common_neighbors(G_undirected, source, target))))\n","\n","    source_indegree.append(G.in_degree(source))\n","    source_outdegree.append(G.out_degree(source))\n","    target_indegree.append(G.in_degree(target))\n","    target_outdegree.append(G.out_degree(target))\n","\n","    pagerank.append(page_rank.get(target))\n","    source_hubs.append(hub_score.get(source))\n","    target_authorities.append(hub_score.get(target))\n","\n","  df['age_diff'] = age_diff\n","  df['jaccard_coef'] = jaccard_coef\n","  df['adamic_adar_coef'] = adamic_adar_coef\n","  df['pref_attachment_coef'] = pref_attachement_coef\n","  df['common_neigh'] = common_neigh\n","  df['source_indegree'] = source_indegree\n","  df['source_outdegree'] = source_outdegree\n","  df['target_indgree'] = target_indegree\n","  df['target_outdegree'] = target_outdegree\n","  df['pagerank'] = pagerank\n","  df['source_hubs'] = source_hubs\n","  df['target_authorities'] = target_authorities\n","\n","  return df"],"metadata":{"id":"Bw4u62Pcu4lP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extend_features(df):\n","\n","  triad_features_0 = []\n","  triad_features_1 = []\n","  triad_features_2 = []\n","  triad_features_3 = []\n","  triad_features_4 = []\n","  triad_features_5 = []\n","  triad_features_6 = []\n","  triad_features_7 = []\n","\n","  for i in tqdm(range(len(df))):   \n","\n","    source = df.iloc[i]['source']\n","    target = df.iloc[i]['target']\n","    idx_source, idx_target = nodelist.index(source), nodelist.index(target)\n","\n","    triad_feature = [0.0]*8\n","\n","    for w in sorted(nx.common_neighbors(G_undirected, source, target)):\n","      if G_undirected.has_edge(source, w) and G_undirected.has_edge(w, target):\n","        triad_feature[0] += 1\n","      if G_undirected.has_edge(source, w) and G_undirected.has_edge(target, w):\n","        triad_feature[1] += 1   \n","      if G_undirected.has_edge(w, source) and G_undirected.has_edge(w, target):\n","        triad_feature[2] += 1\n","      if G_undirected.has_edge(w, source) and G_undirected.has_edge(target, w):\n","        triad_feature[3] += 1\n","      \n","    for i in range(4,8):\n","      if triad_feature[i-4] != 0:\n","        triad_feature[i] = triad_feature[i-4]/len(sorted(nx.common_neighbors(G_undirected, source, target)))\n","\n","    triad_features_0.append(triad_feature[0])\n","    triad_features_1.append(triad_feature[1])\n","    triad_features_2.append(triad_feature[2])\n","    triad_features_3.append(triad_feature[3])\n","    triad_features_4.append(triad_feature[4])\n","    triad_features_5.append(triad_feature[5])\n","    triad_features_6.append(triad_feature[6])\n","    triad_features_7.append(triad_feature[7])\n","\n","  df['triad_feature_0'] = triad_features_0\n","  df['triad_feature_1'] = triad_features_1\n","  df['triad_feature_2'] = triad_features_2\n","  df['triad_feature_3'] = triad_features_3\n","  df['triad_feature_4'] = triad_features_4\n","  df['triad_feature_5'] = triad_features_5\n","  df['triad_feature_6'] = triad_features_6\n","  df['triad_feature_7'] = triad_features_7\n","\n","  return df"],"metadata":{"id":"ji_hKS0bPTuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = features(df)"],"metadata":{"id":"xk6N_tOMO1-k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dd49535-156e-408e-9d84-53330802b660"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10077/10077 [01:06<00:00, 151.30it/s]\n"]}]},{"cell_type":"code","source":["df = extend_features(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSulgSGuWgdM","outputId":"e776df0b-3cbf-43d2-e5f6-e7282d037e06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10077/10077 [00:15<00:00, 656.76it/s]\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"iQy3FljrL_M6","outputId":"1f621139-6f6f-4bab-ca0d-a04cc3625a57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        source       target  link     sum_0     sum_1     sum_2     sum_3  \\\n","0  rkfzhYMj35k  qN3kC_4xURA     0 -1.413013 -1.778044 -0.372599 -1.539065   \n","1  J85b993voac  UoYLkSk6k04     1 -1.626465 -2.162876 -1.854273 -3.671918   \n","2  r_JC8mFoBPI  PsM7xLkORWg     1  3.892650  0.486121 -1.137511 -2.722686   \n","3  U5Ojfo2qkNY  FAOEOgrpRZk     0 -1.054151 -1.225188  0.573736 -4.958475   \n","4  SjqRGPG2Yas  84Fng-NSWys     0 -0.667670 -2.896195 -2.572122 -2.121985   \n","\n","      sum_4     sum_5     sum_6  ...  source_hubs  target_authorities  \\\n","0 -2.383949  1.203595 -0.034141  ...         -0.0       -0.000000e+00   \n","1 -6.044376  0.502586 -2.740177  ...         -0.0       -0.000000e+00   \n","2 -5.124178  3.502509 -4.314734  ...         -0.0       -0.000000e+00   \n","3 -3.577811  0.855308 -1.804483  ...         -0.0       -0.000000e+00   \n","4 -0.676632  3.341443 -0.198435  ...         -0.0       -2.702190e-20   \n","\n","   triad_feature_0  triad_feature_1  triad_feature_2  triad_feature_3  \\\n","0              0.0              0.0              0.0              0.0   \n","1              0.0              0.0              0.0              0.0   \n","2              2.0              2.0              2.0              2.0   \n","3              0.0              0.0              0.0              0.0   \n","4              0.0              0.0              0.0              0.0   \n","\n","   triad_feature_4  triad_feature_5  triad_feature_6  triad_feature_7  \n","0              0.0              0.0              0.0              0.0  \n","1              0.0              0.0              0.0              0.0  \n","2              1.0              1.0              1.0              1.0  \n","3              0.0              0.0              0.0              0.0  \n","4              0.0              0.0              0.0              0.0  \n","\n","[5 rows x 71 columns]"],"text/html":["\n","  <div id=\"df-7e0cc3f4-93bf-49fe-89aa-a40a7e52bfb9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","      <th>link</th>\n","      <th>sum_0</th>\n","      <th>sum_1</th>\n","      <th>sum_2</th>\n","      <th>sum_3</th>\n","      <th>sum_4</th>\n","      <th>sum_5</th>\n","      <th>sum_6</th>\n","      <th>...</th>\n","      <th>source_hubs</th>\n","      <th>target_authorities</th>\n","      <th>triad_feature_0</th>\n","      <th>triad_feature_1</th>\n","      <th>triad_feature_2</th>\n","      <th>triad_feature_3</th>\n","      <th>triad_feature_4</th>\n","      <th>triad_feature_5</th>\n","      <th>triad_feature_6</th>\n","      <th>triad_feature_7</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rkfzhYMj35k</td>\n","      <td>qN3kC_4xURA</td>\n","      <td>0</td>\n","      <td>-1.413013</td>\n","      <td>-1.778044</td>\n","      <td>-0.372599</td>\n","      <td>-1.539065</td>\n","      <td>-2.383949</td>\n","      <td>1.203595</td>\n","      <td>-0.034141</td>\n","      <td>...</td>\n","      <td>-0.0</td>\n","      <td>-0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>J85b993voac</td>\n","      <td>UoYLkSk6k04</td>\n","      <td>1</td>\n","      <td>-1.626465</td>\n","      <td>-2.162876</td>\n","      <td>-1.854273</td>\n","      <td>-3.671918</td>\n","      <td>-6.044376</td>\n","      <td>0.502586</td>\n","      <td>-2.740177</td>\n","      <td>...</td>\n","      <td>-0.0</td>\n","      <td>-0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>r_JC8mFoBPI</td>\n","      <td>PsM7xLkORWg</td>\n","      <td>1</td>\n","      <td>3.892650</td>\n","      <td>0.486121</td>\n","      <td>-1.137511</td>\n","      <td>-2.722686</td>\n","      <td>-5.124178</td>\n","      <td>3.502509</td>\n","      <td>-4.314734</td>\n","      <td>...</td>\n","      <td>-0.0</td>\n","      <td>-0.000000e+00</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U5Ojfo2qkNY</td>\n","      <td>FAOEOgrpRZk</td>\n","      <td>0</td>\n","      <td>-1.054151</td>\n","      <td>-1.225188</td>\n","      <td>0.573736</td>\n","      <td>-4.958475</td>\n","      <td>-3.577811</td>\n","      <td>0.855308</td>\n","      <td>-1.804483</td>\n","      <td>...</td>\n","      <td>-0.0</td>\n","      <td>-0.000000e+00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SjqRGPG2Yas</td>\n","      <td>84Fng-NSWys</td>\n","      <td>0</td>\n","      <td>-0.667670</td>\n","      <td>-2.896195</td>\n","      <td>-2.572122</td>\n","      <td>-2.121985</td>\n","      <td>-0.676632</td>\n","      <td>3.341443</td>\n","      <td>-0.198435</td>\n","      <td>...</td>\n","      <td>-0.0</td>\n","      <td>-2.702190e-20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 71 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7e0cc3f4-93bf-49fe-89aa-a40a7e52bfb9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7e0cc3f4-93bf-49fe-89aa-a40a7e52bfb9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7e0cc3f4-93bf-49fe-89aa-a40a7e52bfb9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XMMR2cwpMEGy","outputId":"26783df3-1c04-4742-f27e-b71f338c4bff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10077, 71)"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["df.to_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/df_with_features_10perc.csv\")"],"metadata":{"id":"w7z0C1UmSDzo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Train/test split"],"metadata":{"id":"9aaD9NaurlIH"}},{"cell_type":"code","source":["df = pd.read_csv(\"/content/drive/Shareddrives/MLNS/Final Project/Data/df_with_features.csv\")"],"metadata":{"id":"BlwNyBgYSUHT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(df.drop(['source', 'target', 'link'], axis=1), df['link'], test_size = 0.1)"],"metadata":{"id":"h2ZXMa_Drmta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models"],"metadata":{"id":"2ayv-6UjPKCG"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score"],"metadata":{"id":"gG7nVt_gPLsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if scale == True:\n","  X_train, X_test = preprocessing.scale(X_train), preprocessing.scale(X_test)"],"metadata":{"id":"r1VOaEJPTQvN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Linear SVC"],"metadata":{"id":"0GGWJnS7oE9i"}},{"cell_type":"code","source":["from sklearn.svm import LinearSVC"],"metadata":{"id":"RC9Ru4iNxACw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvIyawI9WK7D","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cd1513d-a4f2-4b07-8ae5-5e1c95475bf2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8948412698412699"]},"metadata":{},"execution_count":45}],"source":["lsvc= LinearSVC()\n","lsvc.fit(X_train, y_train)\n","lsvc.score(X_test, y_test)"]},{"cell_type":"code","source":["print(classification_report(y_test, lsvc.predict(X_test)))"],"metadata":{"id":"8oZTbNwMhA78","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b9cf94d-bc67-4357-aa06-ea5b8a887661"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.80      0.89       524\n","           1       0.82      1.00      0.90       484\n","\n","    accuracy                           0.89      1008\n","   macro avg       0.91      0.90      0.89      1008\n","weighted avg       0.91      0.89      0.89      1008\n","\n"]}]},{"cell_type":"code","source":["LinearSVC().get_params()"],"metadata":{"id":"GR9ArYn7hAyg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f523f87d-6702-4d13-98a5-d7aaa992869b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'C': 1.0,\n"," 'class_weight': None,\n"," 'dual': True,\n"," 'fit_intercept': True,\n"," 'intercept_scaling': 1,\n"," 'loss': 'squared_hinge',\n"," 'max_iter': 1000,\n"," 'multi_class': 'ovr',\n"," 'penalty': 'l2',\n"," 'random_state': None,\n"," 'tol': 0.0001,\n"," 'verbose': 0}"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# defining parameter range\n","param_grid = {'C': [0.01, 0.1, 1],\n","              'penalty': ['l1', 'l2'],\n","              'dual': [False] ## Documentation says prefer False when n_samples > n_features\n","              }\n"," \n","grid = GridSearchCV(LinearSVC(), param_grid, refit = True, verbose = 3)\n"," \n","# fitting the model for grid search\n","grid.fit(X_train, y_train)"],"metadata":{"id":"9FMUeiMxC6WD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f7bdc45-73e5-4580-dda0-bf07ceccea63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 6 candidates, totalling 30 fits\n","[CV 1/5] END ....C=0.01, dual=False, penalty=l1;, score=0.905 total time=   0.4s\n","[CV 2/5] END ....C=0.01, dual=False, penalty=l1;, score=0.910 total time=   0.3s\n","[CV 3/5] END ....C=0.01, dual=False, penalty=l1;, score=0.895 total time=   0.3s\n","[CV 4/5] END ....C=0.01, dual=False, penalty=l1;, score=0.901 total time=   0.3s\n","[CV 5/5] END ....C=0.01, dual=False, penalty=l1;, score=0.897 total time=   0.3s\n","[CV 1/5] END ....C=0.01, dual=False, penalty=l2;, score=0.908 total time=   0.1s\n","[CV 2/5] END ....C=0.01, dual=False, penalty=l2;, score=0.912 total time=   0.1s\n","[CV 3/5] END ....C=0.01, dual=False, penalty=l2;, score=0.897 total time=   0.4s\n","[CV 4/5] END ....C=0.01, dual=False, penalty=l2;, score=0.902 total time=   0.4s\n","[CV 5/5] END ....C=0.01, dual=False, penalty=l2;, score=0.905 total time=   0.2s\n","[CV 1/5] END .....C=0.1, dual=False, penalty=l1;, score=0.907 total time=   0.9s\n","[CV 2/5] END .....C=0.1, dual=False, penalty=l1;, score=0.913 total time=   0.8s\n","[CV 3/5] END .....C=0.1, dual=False, penalty=l1;, score=0.899 total time=   0.8s\n","[CV 4/5] END .....C=0.1, dual=False, penalty=l1;, score=0.902 total time=   1.2s\n","[CV 5/5] END .....C=0.1, dual=False, penalty=l1;, score=0.905 total time=   0.6s\n","[CV 1/5] END .....C=0.1, dual=False, penalty=l2;, score=0.911 total time=   0.1s\n","[CV 2/5] END .....C=0.1, dual=False, penalty=l2;, score=0.913 total time=   0.1s\n","[CV 3/5] END .....C=0.1, dual=False, penalty=l2;, score=0.899 total time=   0.1s\n","[CV 4/5] END .....C=0.1, dual=False, penalty=l2;, score=0.904 total time=   0.1s\n","[CV 5/5] END .....C=0.1, dual=False, penalty=l2;, score=0.906 total time=   0.1s\n","[CV 1/5] END .......C=1, dual=False, penalty=l1;, score=0.911 total time=   1.8s\n","[CV 2/5] END .......C=1, dual=False, penalty=l1;, score=0.913 total time=   2.1s\n","[CV 3/5] END .......C=1, dual=False, penalty=l1;, score=0.899 total time=   4.8s\n","[CV 4/5] END .......C=1, dual=False, penalty=l1;, score=0.904 total time=   4.5s\n","[CV 5/5] END .......C=1, dual=False, penalty=l1;, score=0.906 total time=   2.6s\n","[CV 1/5] END .......C=1, dual=False, penalty=l2;, score=0.911 total time=   0.1s\n","[CV 2/5] END .......C=1, dual=False, penalty=l2;, score=0.913 total time=   0.1s\n","[CV 3/5] END .......C=1, dual=False, penalty=l2;, score=0.899 total time=   0.1s\n","[CV 4/5] END .......C=1, dual=False, penalty=l2;, score=0.903 total time=   0.2s\n","[CV 5/5] END .......C=1, dual=False, penalty=l2;, score=0.906 total time=   0.1s\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  ConvergenceWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(estimator=LinearSVC(),\n","             param_grid={'C': [0.01, 0.1, 1], 'dual': [False],\n","                         'penalty': ['l1', 'l2']},\n","             verbose=3)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["grid.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzRq8_gBC9P2","outputId":"1b2128df-e8bb-49f7-a2f6-a1834446d1ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8948412698412699"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["grid.best_estimator_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMoaO2RNC9GS","outputId":"3506757b-e987-4e64-d364-a42fdc0da5b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearSVC(C=1, dual=False, penalty='l1')"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["## Random Forest\n"],"metadata":{"id":"Ea7rJ7Eo8Doa"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CJABge-VrRq","outputId":"a9ec2ae3-382e-4a5a-e9b5-8836ef490d9c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8809523809523809"]},"metadata":{},"execution_count":51}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","RF =  RandomForestClassifier(n_estimators= 1000)\n","RF.fit(X_train, y_train)\n","RF.score(X_test, y_test)"]},{"cell_type":"code","source":["# Checking score on training data as well\n","print(f1_score(list(y_train), list(RF.predict(X_train))))"],"metadata":{"id":"DgxqlBvRvO_O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"289bc157-666c-4c76-d85e-522570f23204"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"0EPgUpeR9zY2","outputId":"5d9ab785-9175-480d-9400-cdbfa63bff79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-5f9f5cfe1ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                       n_jobs = -1, scoring='accuracy')\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mclf_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#hyperparameter grid search for randomforest\n","n_estimators = [200, 400, 600, 800, 1000]\n","max_depth = [5, 10, 15]\n","min_samples_split = [2,4,6]\n","min_samples_leaf = [1, 3, 6]\n","\n","hyperF = dict(n_estimators = n_estimators, \n","              min_samples_split = min_samples_split,\n","             min_samples_leaf = min_samples_leaf)\n","clf = GridSearchCV(RandomForestClassifier(), hyperF, cv = 3, verbose = 1,\n","                      n_jobs = -1, scoring='accuracy')\n","\n","clf.fit(X_train, y_train)\n","clf_best = clf.best_estimator_"]},{"cell_type":"code","source":["clf_best.score(X_test, y_test)"],"metadata":{"id":"KuWsWnww-gcQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decision Tree Classifier"],"metadata":{"id":"iSZlLqx6obka"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier"],"metadata":{"id":"shfNh2TvwoS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gihw9NtjWK6y"},"outputs":[],"source":["dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)\n","dt.score(X_test, y_test)"]},{"cell_type":"code","source":["print(classification_report(y_test, dt.predict(X_test)))"],"metadata":{"id":"FPxPqXWthFNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DecisionTreeClassifier().get_params()"],"metadata":{"id":"E6tgwQoYvjgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining parameter range\n","param_grid = {'max_depth': [10,11,12],\n","              'min_samples_leaf': [1,4,6],\n","              # 'criterion': ['gini', 'entropy']\n","              }\n"," \n","grid = GridSearchCV(DecisionTreeClassifier(), param_grid, refit = True, verbose = 3)\n"," \n","# fitting the model for grid search\n","grid.fit(X_train, y_train)"],"metadata":{"id":"WWXCFktQzCKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid.score(X_test, y_test)"],"metadata":{"id":"E3CrWaJzz1dz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid.best_estimator_"],"metadata":{"id":"Lq8WSpNC1Kd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Light GBM"],"metadata":{"id":"M2hydbQdSv2J"}},{"cell_type":"code","source":["import lightgbm as lgb"],"metadata":{"id":"8q0AigO_SgMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf = lgb.LGBMClassifier(random_state=0)\n","clf.fit(X_train, y_train)\n","clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gk2l4HQwTHSr","outputId":"4e96e130-a264-48f8-e724-15551156b2b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9740382115934494"]},"metadata":{},"execution_count":213}]},{"cell_type":"code","source":["lgb.LGBMClassifier().get_params()"],"metadata":{"id":"RGFJSIA7SgBW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining parameter range\n","param_grid = {'max_depth': [5,10,15],\n","              'learning_rate': [0.1, 0.01],\n","              'n_estimators': [100,200,500]\n","              }\n"," \n","grid = GridSearchCV(lgb.LGBMClassifier(), param_grid, refit = True, verbose = 3)\n"," \n","# fitting the model for grid search\n","grid.fit(X_train, y_train)\n","grid.score(X_test, y_test)"],"metadata":{"id":"Mg-INZHXSf6U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid.best_estimator_"],"metadata":{"id":"tD_fOPKESf0g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCeeAZr2mVow"},"source":["## AdaBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Lc_7FsQmVox"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","abc = AdaBoostClassifier(n_estimators=100)\n","abc = abc.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMcbaRWGmVoy"},"outputs":[],"source":["y_pred = abc.predict(X_test)\n","print('F1: %f' %(f1_score(y_pred, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQGng-A4mVoy"},"outputs":[],"source":["model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n","\n","param_grid = {\n","    'base_estimator__max_depth':[i for i in range(2,11,2)],            \n","    'base_estimator__min_samples_leaf':[5,10],\n","    'n_estimators':[10,50,100],\n","    'learning_rate':[0.01,0.1]\n","}\n","abc_hp = GridSearchCV(estimator=model,param_grid=param_grid, cv=5, n_jobs=-1,verbose=2)\n","abc_hp.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U0qelUOwmVoz"},"outputs":[],"source":["print(abc_hp.best_params_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tex6zaP9mVo0"},"outputs":[],"source":["abc = AdaBoostClassifier(**abc_hp.best_params_)\n","abc = abc.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"manGaj0TmVo0"},"outputs":[],"source":["y_pred = abc.predict(X_test)\n","print('F1: %f' %(f1_score(y_pred, y_test)))"]},{"cell_type":"markdown","metadata":{"id":"x0om6OXNmVo1"},"source":["## Gradient Boosting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckk5a4rsmVo3"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","gbc = GradientBoostingClassifier()\n","gbc = gbc.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0WAq4HsmVo4"},"outputs":[],"source":["y_pred = gbc.predict(X_test)\n","print('F1: %f' %(f1_score(y_pred, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Go1x3RbmVo7"},"outputs":[],"source":["param_grid = {\n","    \"loss\":[\"deviance\"],\n","    \"learning_rate\": [0.01, 0.1],\n","    \"min_samples_split\": np.linspace(0.1, 0.5, 3),\n","    \"min_samples_leaf\": np.linspace(0.1, 0.5, 3),\n","    \"max_depth\":[i for i in range(2,11,2)],\n","    \"n_estimators\":[10, 100]\n","}\n","\n","gbc_hp = GridSearchCV(estimator = gbc, param_grid = param_grid, cv = 5)\n","gbc_hp.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k-5fBVZ_mVo8"},"outputs":[],"source":["gbc_hp.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HY8YO-KFmVo9"},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingClassifier\n","gbc = GradientBoostingClassifier(**gbc_hp.best_params_)\n","gbc = gbc.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4IjhhQdmVo-","outputId":"97045c39-1985-43ff-d030-bfd868e3d647"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1: 0.972644\n"]}],"source":["y_pred = gbc.predict(X_test)\n","print('F1: %f' %(f1_score(y_pred, y_test)))"]},{"cell_type":"markdown","source":["## XGBoost\n"],"metadata":{"id":"6RXHQaN48sdO"}},{"cell_type":"code","source":["from xgboost import XGBClassifier"],"metadata":{"id":"lpfzgUWU8uTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kYYFf3VWrqG","outputId":"4093c2d9-0372-4d30-dee1-bd2ff542eb6b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["XGBClassifier(n_estimators=1000)"]},"metadata":{},"execution_count":11}],"source":["xgb = XGBClassifier(n_estimators=1000)\n","xgb.fit(X_train, y_train)"]},{"cell_type":"code","source":["xgb.score(X_test, y_test)"],"metadata":{"id":"Ld_1Y-lYsnEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUdJ3hVCVyYV"},"outputs":[],"source":["# checking score on training data as well\n","\n","print(f1_score(list(y_train), list(xgb.predict(X_train))))"]},{"cell_type":"markdown","source":["## Multi layer perceptron"],"metadata":{"id":"kyomyckNp-Q5"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","mlp = MLPClassifier()\n","parameters = {'solver':['sgd','adam'],\n","              'alpha':[0.0001, 0.0005, 0.005],\n","              'activation':['relu', 'tanh'],\n","              'hidden_layer_sizes':[(60,20), (30, 10)],\n","              'tol':[5e-5],\n","              'max_iter':[150],\n","              'verbose':[1]\n","              }\n","clf = GridSearchCV(mlp, parameters)"],"metadata":{"id":"TG05YOAUvg3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.fit(X_train, y_train)"],"metadata":{"id":"TidyG-ETvg3V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6d5e575d-1690-4824-9154-cfd58e2e32bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.59724413\n","Iteration 2, loss = 0.46833360\n","Iteration 3, loss = 0.40067336\n","Iteration 4, loss = 0.35856110\n","Iteration 5, loss = 0.32900461\n","Iteration 6, loss = 0.30741508\n","Iteration 7, loss = 0.29127450\n","Iteration 8, loss = 0.27911847\n","Iteration 9, loss = 0.26982294\n","Iteration 10, loss = 0.26253208\n","Iteration 11, loss = 0.25678660\n","Iteration 12, loss = 0.25215458\n","Iteration 13, loss = 0.24835803\n","Iteration 14, loss = 0.24517558\n","Iteration 15, loss = 0.24247343\n","Iteration 16, loss = 0.24011018\n","Iteration 17, loss = 0.23804221\n","Iteration 18, loss = 0.23620073\n","Iteration 19, loss = 0.23459178\n","Iteration 20, loss = 0.23311388\n","Iteration 21, loss = 0.23185493\n","Iteration 22, loss = 0.23054421\n","Iteration 23, loss = 0.22946742\n","Iteration 24, loss = 0.22837515\n","Iteration 25, loss = 0.22743515\n","Iteration 26, loss = 0.22652166\n","Iteration 27, loss = 0.22566825\n","Iteration 28, loss = 0.22487724\n","Iteration 29, loss = 0.22412827\n","Iteration 30, loss = 0.22343590\n","Iteration 31, loss = 0.22273802\n","Iteration 32, loss = 0.22220343\n","Iteration 33, loss = 0.22149180\n","Iteration 34, loss = 0.22088083\n","Iteration 35, loss = 0.22036074\n","Iteration 36, loss = 0.21977000\n","Iteration 37, loss = 0.21924621\n","Iteration 38, loss = 0.21874803\n","Iteration 39, loss = 0.21826176\n","Iteration 40, loss = 0.21776736\n","Iteration 41, loss = 0.21735309\n","Iteration 42, loss = 0.21688752\n","Iteration 43, loss = 0.21651290\n","Iteration 44, loss = 0.21606976\n","Iteration 45, loss = 0.21563327\n","Iteration 46, loss = 0.21527038\n","Iteration 47, loss = 0.21491698\n","Iteration 48, loss = 0.21446399\n","Iteration 49, loss = 0.21414210\n","Iteration 50, loss = 0.21379361\n","Iteration 51, loss = 0.21338146\n","Iteration 52, loss = 0.21300076\n","Iteration 53, loss = 0.21262982\n","Iteration 54, loss = 0.21231964\n","Iteration 55, loss = 0.21196013\n","Iteration 56, loss = 0.21166122\n","Iteration 57, loss = 0.21133952\n","Iteration 58, loss = 0.21105000\n","Iteration 59, loss = 0.21065952\n","Iteration 60, loss = 0.21034738\n","Iteration 61, loss = 0.21004364\n","Iteration 62, loss = 0.20976521\n","Iteration 63, loss = 0.20942697\n","Iteration 64, loss = 0.20914991\n","Iteration 65, loss = 0.20886932\n","Iteration 66, loss = 0.20856630\n","Iteration 67, loss = 0.20826466\n","Iteration 68, loss = 0.20797763\n","Iteration 69, loss = 0.20769277\n","Iteration 70, loss = 0.20744189\n","Iteration 71, loss = 0.20715666\n","Iteration 72, loss = 0.20692101\n","Iteration 73, loss = 0.20666970\n","Iteration 74, loss = 0.20637423\n","Iteration 75, loss = 0.20606508\n","Iteration 76, loss = 0.20581036\n","Iteration 77, loss = 0.20554773\n","Iteration 78, loss = 0.20528606\n","Iteration 79, loss = 0.20506369\n","Iteration 80, loss = 0.20481714\n","Iteration 81, loss = 0.20463305\n","Iteration 82, loss = 0.20430128\n","Iteration 83, loss = 0.20405510\n","Iteration 84, loss = 0.20387342\n","Iteration 85, loss = 0.20357609\n","Iteration 86, loss = 0.20334586\n","Iteration 87, loss = 0.20312689\n","Iteration 88, loss = 0.20288486\n","Iteration 89, loss = 0.20271523\n","Iteration 90, loss = 0.20241853\n","Iteration 91, loss = 0.20222308\n","Iteration 92, loss = 0.20195823\n","Iteration 93, loss = 0.20173976\n","Iteration 94, loss = 0.20155094\n","Iteration 95, loss = 0.20129635\n","Iteration 96, loss = 0.20109191\n","Iteration 97, loss = 0.20089186\n","Iteration 98, loss = 0.20063469\n","Iteration 99, loss = 0.20039701\n","Iteration 100, loss = 0.20017461\n","Iteration 101, loss = 0.19999170\n","Iteration 102, loss = 0.19973380\n","Iteration 103, loss = 0.19951582\n","Iteration 104, loss = 0.19936284\n","Iteration 105, loss = 0.19911499\n","Iteration 106, loss = 0.19890340\n","Iteration 107, loss = 0.19865191\n","Iteration 108, loss = 0.19845056\n","Iteration 109, loss = 0.19823443\n","Iteration 110, loss = 0.19797387\n","Iteration 111, loss = 0.19780543\n","Iteration 112, loss = 0.19757604\n","Iteration 113, loss = 0.19742911\n","Iteration 114, loss = 0.19712762\n","Iteration 115, loss = 0.19700814\n","Iteration 116, loss = 0.19677618\n","Iteration 117, loss = 0.19653352\n","Iteration 118, loss = 0.19636007\n","Iteration 119, loss = 0.19616501\n","Iteration 120, loss = 0.19595416\n","Iteration 121, loss = 0.19573311\n","Iteration 122, loss = 0.19560176\n","Iteration 123, loss = 0.19534831\n","Iteration 124, loss = 0.19515913\n","Iteration 125, loss = 0.19500645\n","Iteration 126, loss = 0.19478705\n","Iteration 127, loss = 0.19460036\n","Iteration 128, loss = 0.19438719\n","Iteration 129, loss = 0.19415979\n","Iteration 130, loss = 0.19400490\n","Iteration 131, loss = 0.19381685\n","Iteration 132, loss = 0.19363042\n","Iteration 133, loss = 0.19344783\n","Iteration 134, loss = 0.19332575\n","Iteration 135, loss = 0.19305917\n","Iteration 136, loss = 0.19292862\n","Iteration 137, loss = 0.19277023\n","Iteration 138, loss = 0.19251999\n","Iteration 139, loss = 0.19230019\n","Iteration 140, loss = 0.19213520\n","Iteration 141, loss = 0.19197696\n","Iteration 142, loss = 0.19175308\n","Iteration 143, loss = 0.19163359\n","Iteration 144, loss = 0.19142647\n","Iteration 145, loss = 0.19122532\n","Iteration 146, loss = 0.19106891\n","Iteration 147, loss = 0.19090314\n","Iteration 148, loss = 0.19075350\n","Iteration 149, loss = 0.19052244\n","Iteration 150, loss = 0.19041254\n","Iteration 1, loss = 0.83310424\n","Iteration 2, loss = 0.52129641\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.41883618\n","Iteration 4, loss = 0.36057895\n","Iteration 5, loss = 0.32175126\n","Iteration 6, loss = 0.29588151\n","Iteration 7, loss = 0.27824629\n","Iteration 8, loss = 0.26583555\n","Iteration 9, loss = 0.25672997\n","Iteration 10, loss = 0.24986242\n","Iteration 11, loss = 0.24459878\n","Iteration 12, loss = 0.24042685\n","Iteration 13, loss = 0.23699884\n","Iteration 14, loss = 0.23414142\n","Iteration 15, loss = 0.23175799\n","Iteration 16, loss = 0.22967706\n","Iteration 17, loss = 0.22790030\n","Iteration 18, loss = 0.22634162\n","Iteration 19, loss = 0.22489063\n","Iteration 20, loss = 0.22374084\n","Iteration 21, loss = 0.22257855\n","Iteration 22, loss = 0.22161721\n","Iteration 23, loss = 0.22068239\n","Iteration 24, loss = 0.21978983\n","Iteration 25, loss = 0.21893176\n","Iteration 26, loss = 0.21817798\n","Iteration 27, loss = 0.21748399\n","Iteration 28, loss = 0.21679472\n","Iteration 29, loss = 0.21614825\n","Iteration 30, loss = 0.21555143\n","Iteration 31, loss = 0.21496679\n","Iteration 32, loss = 0.21440408\n","Iteration 33, loss = 0.21388941\n","Iteration 34, loss = 0.21337298\n","Iteration 35, loss = 0.21286459\n","Iteration 36, loss = 0.21240653\n","Iteration 37, loss = 0.21191320\n","Iteration 38, loss = 0.21144658\n","Iteration 39, loss = 0.21105819\n","Iteration 40, loss = 0.21056523\n","Iteration 41, loss = 0.21016372\n","Iteration 42, loss = 0.20978716\n","Iteration 43, loss = 0.20937250\n","Iteration 44, loss = 0.20899662\n","Iteration 45, loss = 0.20859513\n","Iteration 46, loss = 0.20822424\n","Iteration 47, loss = 0.20785008\n","Iteration 48, loss = 0.20750388\n","Iteration 49, loss = 0.20718571\n","Iteration 50, loss = 0.20688281\n","Iteration 51, loss = 0.20650894\n","Iteration 52, loss = 0.20615903\n","Iteration 53, loss = 0.20580531\n","Iteration 54, loss = 0.20550359\n","Iteration 55, loss = 0.20518763\n","Iteration 56, loss = 0.20484728\n","Iteration 57, loss = 0.20455925\n","Iteration 58, loss = 0.20428851\n","Iteration 59, loss = 0.20398265\n","Iteration 60, loss = 0.20368559\n","Iteration 61, loss = 0.20339767\n","Iteration 62, loss = 0.20315954\n","Iteration 63, loss = 0.20280646\n","Iteration 64, loss = 0.20255587\n","Iteration 65, loss = 0.20229119\n","Iteration 66, loss = 0.20197465\n","Iteration 67, loss = 0.20171293\n","Iteration 68, loss = 0.20147736\n","Iteration 69, loss = 0.20123698\n","Iteration 70, loss = 0.20092372\n","Iteration 71, loss = 0.20069487\n","Iteration 72, loss = 0.20046425\n","Iteration 73, loss = 0.20016918\n","Iteration 74, loss = 0.19991763\n","Iteration 75, loss = 0.19968512\n","Iteration 76, loss = 0.19943220\n","Iteration 77, loss = 0.19918941\n","Iteration 78, loss = 0.19895290\n","Iteration 79, loss = 0.19874837\n","Iteration 80, loss = 0.19850052\n","Iteration 81, loss = 0.19826650\n","Iteration 82, loss = 0.19804983\n","Iteration 83, loss = 0.19777830\n","Iteration 84, loss = 0.19756001\n","Iteration 85, loss = 0.19735220\n","Iteration 86, loss = 0.19709614\n","Iteration 87, loss = 0.19692068\n","Iteration 88, loss = 0.19670002\n","Iteration 89, loss = 0.19645361\n","Iteration 90, loss = 0.19624015\n","Iteration 91, loss = 0.19598944\n","Iteration 92, loss = 0.19582144\n","Iteration 93, loss = 0.19555734\n","Iteration 94, loss = 0.19536721\n","Iteration 95, loss = 0.19512900\n","Iteration 96, loss = 0.19494208\n","Iteration 97, loss = 0.19471528\n","Iteration 98, loss = 0.19449966\n","Iteration 99, loss = 0.19432131\n","Iteration 100, loss = 0.19409707\n","Iteration 101, loss = 0.19389370\n","Iteration 102, loss = 0.19367366\n","Iteration 103, loss = 0.19345167\n","Iteration 104, loss = 0.19325978\n","Iteration 105, loss = 0.19305363\n","Iteration 106, loss = 0.19284762\n","Iteration 107, loss = 0.19267214\n","Iteration 108, loss = 0.19243868\n","Iteration 109, loss = 0.19226985\n","Iteration 110, loss = 0.19208961\n","Iteration 111, loss = 0.19186988\n","Iteration 112, loss = 0.19173515\n","Iteration 113, loss = 0.19146205\n","Iteration 114, loss = 0.19128996\n","Iteration 115, loss = 0.19113130\n","Iteration 116, loss = 0.19089376\n","Iteration 117, loss = 0.19069393\n","Iteration 118, loss = 0.19055307\n","Iteration 119, loss = 0.19031612\n","Iteration 120, loss = 0.19012266\n","Iteration 121, loss = 0.18992308\n","Iteration 122, loss = 0.18984090\n","Iteration 123, loss = 0.18973936\n","Iteration 124, loss = 0.18941508\n","Iteration 125, loss = 0.18918050\n","Iteration 126, loss = 0.18902326\n","Iteration 127, loss = 0.18880429\n","Iteration 128, loss = 0.18863875\n","Iteration 129, loss = 0.18847305\n","Iteration 130, loss = 0.18823923\n","Iteration 131, loss = 0.18805329\n","Iteration 132, loss = 0.18786166\n","Iteration 133, loss = 0.18768916\n","Iteration 134, loss = 0.18746838\n","Iteration 135, loss = 0.18729186\n","Iteration 136, loss = 0.18712054\n","Iteration 137, loss = 0.18694520\n","Iteration 138, loss = 0.18669854\n","Iteration 139, loss = 0.18654809\n","Iteration 140, loss = 0.18645226\n","Iteration 141, loss = 0.18616480\n","Iteration 142, loss = 0.18603708\n","Iteration 143, loss = 0.18587208\n","Iteration 144, loss = 0.18566817\n","Iteration 145, loss = 0.18555983\n","Iteration 146, loss = 0.18535713\n","Iteration 147, loss = 0.18510686\n","Iteration 148, loss = 0.18500305\n","Iteration 149, loss = 0.18480259\n","Iteration 150, loss = 0.18463302\n","Iteration 1, loss = 0.62511958\n","Iteration 2, loss = 0.49033941\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.40848397\n","Iteration 4, loss = 0.35582996\n","Iteration 5, loss = 0.32068836\n","Iteration 6, loss = 0.29684322\n","Iteration 7, loss = 0.28017073\n","Iteration 8, loss = 0.26812105\n","Iteration 9, loss = 0.25913964\n","Iteration 10, loss = 0.25221608\n","Iteration 11, loss = 0.24682617\n","Iteration 12, loss = 0.24241017\n","Iteration 13, loss = 0.23888294\n","Iteration 14, loss = 0.23579037\n","Iteration 15, loss = 0.23322659\n","Iteration 16, loss = 0.23100957\n","Iteration 17, loss = 0.22909358\n","Iteration 18, loss = 0.22737844\n","Iteration 19, loss = 0.22585868\n","Iteration 20, loss = 0.22448023\n","Iteration 21, loss = 0.22323425\n","Iteration 22, loss = 0.22210550\n","Iteration 23, loss = 0.22104351\n","Iteration 24, loss = 0.22009806\n","Iteration 25, loss = 0.21921208\n","Iteration 26, loss = 0.21837998\n","Iteration 27, loss = 0.21761270\n","Iteration 28, loss = 0.21684113\n","Iteration 29, loss = 0.21611061\n","Iteration 30, loss = 0.21542263\n","Iteration 31, loss = 0.21477032\n","Iteration 32, loss = 0.21422551\n","Iteration 33, loss = 0.21352108\n","Iteration 34, loss = 0.21298739\n","Iteration 35, loss = 0.21242563\n","Iteration 36, loss = 0.21185216\n","Iteration 37, loss = 0.21136587\n","Iteration 38, loss = 0.21083703\n","Iteration 39, loss = 0.21037346\n","Iteration 40, loss = 0.20987591\n","Iteration 41, loss = 0.20940482\n","Iteration 42, loss = 0.20892098\n","Iteration 43, loss = 0.20849140\n","Iteration 44, loss = 0.20806443\n","Iteration 45, loss = 0.20761398\n","Iteration 46, loss = 0.20723185\n","Iteration 47, loss = 0.20681174\n","Iteration 48, loss = 0.20643300\n","Iteration 49, loss = 0.20601833\n","Iteration 50, loss = 0.20568449\n","Iteration 51, loss = 0.20529052\n","Iteration 52, loss = 0.20487830\n","Iteration 53, loss = 0.20450129\n","Iteration 54, loss = 0.20409885\n","Iteration 55, loss = 0.20371004\n","Iteration 56, loss = 0.20337881\n","Iteration 57, loss = 0.20303712\n","Iteration 58, loss = 0.20266613\n","Iteration 59, loss = 0.20231195\n","Iteration 60, loss = 0.20198388\n","Iteration 61, loss = 0.20167730\n","Iteration 62, loss = 0.20136884\n","Iteration 63, loss = 0.20103670\n","Iteration 64, loss = 0.20072159\n","Iteration 65, loss = 0.20041831\n","Iteration 66, loss = 0.20014555\n","Iteration 67, loss = 0.19978181\n","Iteration 68, loss = 0.19947250\n","Iteration 69, loss = 0.19914711\n","Iteration 70, loss = 0.19884872\n","Iteration 71, loss = 0.19859278\n","Iteration 72, loss = 0.19836823\n","Iteration 73, loss = 0.19801175\n","Iteration 74, loss = 0.19771249\n","Iteration 75, loss = 0.19740786\n","Iteration 76, loss = 0.19717497\n","Iteration 77, loss = 0.19682548\n","Iteration 78, loss = 0.19661313\n","Iteration 79, loss = 0.19630876\n","Iteration 80, loss = 0.19601543\n","Iteration 81, loss = 0.19573410\n","Iteration 82, loss = 0.19548543\n","Iteration 83, loss = 0.19524675\n","Iteration 84, loss = 0.19499514\n","Iteration 85, loss = 0.19475382\n","Iteration 86, loss = 0.19444648\n","Iteration 87, loss = 0.19421625\n","Iteration 88, loss = 0.19392435\n","Iteration 89, loss = 0.19376012\n","Iteration 90, loss = 0.19347872\n","Iteration 91, loss = 0.19317527\n","Iteration 92, loss = 0.19295146\n","Iteration 93, loss = 0.19269982\n","Iteration 94, loss = 0.19248001\n","Iteration 95, loss = 0.19221984\n","Iteration 96, loss = 0.19192832\n","Iteration 97, loss = 0.19174247\n","Iteration 98, loss = 0.19145429\n","Iteration 99, loss = 0.19124223\n","Iteration 100, loss = 0.19099869\n","Iteration 101, loss = 0.19080663\n","Iteration 102, loss = 0.19054670\n","Iteration 103, loss = 0.19036535\n","Iteration 104, loss = 0.19007660\n","Iteration 105, loss = 0.18991446\n","Iteration 106, loss = 0.18972007\n","Iteration 107, loss = 0.18946010\n","Iteration 108, loss = 0.18919643\n","Iteration 109, loss = 0.18897692\n","Iteration 110, loss = 0.18879015\n","Iteration 111, loss = 0.18855469\n","Iteration 112, loss = 0.18834632\n","Iteration 113, loss = 0.18808370\n","Iteration 114, loss = 0.18797424\n","Iteration 115, loss = 0.18766096\n","Iteration 116, loss = 0.18746481\n","Iteration 117, loss = 0.18722951\n","Iteration 118, loss = 0.18700441\n","Iteration 119, loss = 0.18677061\n","Iteration 120, loss = 0.18661643\n","Iteration 121, loss = 0.18640029\n","Iteration 122, loss = 0.18618938\n","Iteration 123, loss = 0.18598523\n","Iteration 124, loss = 0.18577198\n","Iteration 125, loss = 0.18555816\n","Iteration 126, loss = 0.18533535\n","Iteration 127, loss = 0.18514206\n","Iteration 128, loss = 0.18494867\n","Iteration 129, loss = 0.18483364\n","Iteration 130, loss = 0.18455107\n","Iteration 131, loss = 0.18432020\n","Iteration 132, loss = 0.18414557\n","Iteration 133, loss = 0.18388216\n","Iteration 134, loss = 0.18369120\n","Iteration 135, loss = 0.18352783\n","Iteration 136, loss = 0.18335272\n","Iteration 137, loss = 0.18305932\n","Iteration 138, loss = 0.18290747\n","Iteration 139, loss = 0.18272124\n","Iteration 140, loss = 0.18253928\n","Iteration 141, loss = 0.18233601\n","Iteration 142, loss = 0.18208629\n","Iteration 143, loss = 0.18193698\n","Iteration 144, loss = 0.18171112\n","Iteration 145, loss = 0.18152598\n","Iteration 146, loss = 0.18130280\n","Iteration 147, loss = 0.18113996\n","Iteration 148, loss = 0.18097253\n","Iteration 149, loss = 0.18075767\n","Iteration 150, loss = 0.18059744\n","Iteration 1, loss = 0.68361019\n","Iteration 2, loss = 0.50137896\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.41062143\n","Iteration 4, loss = 0.35762186\n","Iteration 5, loss = 0.32375248\n","Iteration 6, loss = 0.30111514\n","Iteration 7, loss = 0.28515539\n","Iteration 8, loss = 0.27354446\n","Iteration 9, loss = 0.26476420\n","Iteration 10, loss = 0.25802570\n","Iteration 11, loss = 0.25249815\n","Iteration 12, loss = 0.24808276\n","Iteration 13, loss = 0.24434516\n","Iteration 14, loss = 0.24115730\n","Iteration 15, loss = 0.23840688\n","Iteration 16, loss = 0.23599560\n","Iteration 17, loss = 0.23386033\n","Iteration 18, loss = 0.23191932\n","Iteration 19, loss = 0.23016242\n","Iteration 20, loss = 0.22852848\n","Iteration 21, loss = 0.22711724\n","Iteration 22, loss = 0.22576564\n","Iteration 23, loss = 0.22452553\n","Iteration 24, loss = 0.22343284\n","Iteration 25, loss = 0.22235348\n","Iteration 26, loss = 0.22139707\n","Iteration 27, loss = 0.22049677\n","Iteration 28, loss = 0.21970089\n","Iteration 29, loss = 0.21883561\n","Iteration 30, loss = 0.21805926\n","Iteration 31, loss = 0.21736716\n","Iteration 32, loss = 0.21670395\n","Iteration 33, loss = 0.21600664\n","Iteration 34, loss = 0.21538420\n","Iteration 35, loss = 0.21481372\n","Iteration 36, loss = 0.21423043\n","Iteration 37, loss = 0.21364216\n","Iteration 38, loss = 0.21310668\n","Iteration 39, loss = 0.21262634\n","Iteration 40, loss = 0.21206358\n","Iteration 41, loss = 0.21157601\n","Iteration 42, loss = 0.21109640\n","Iteration 43, loss = 0.21069556\n","Iteration 44, loss = 0.21022223\n","Iteration 45, loss = 0.20980419\n","Iteration 46, loss = 0.20937984\n","Iteration 47, loss = 0.20898275\n","Iteration 48, loss = 0.20858794\n","Iteration 49, loss = 0.20818722\n","Iteration 50, loss = 0.20777672\n","Iteration 51, loss = 0.20744868\n","Iteration 52, loss = 0.20705367\n","Iteration 53, loss = 0.20668644\n","Iteration 54, loss = 0.20635982\n","Iteration 55, loss = 0.20600441\n","Iteration 56, loss = 0.20567462\n","Iteration 57, loss = 0.20535817\n","Iteration 58, loss = 0.20502301\n","Iteration 59, loss = 0.20473650\n","Iteration 60, loss = 0.20441826\n","Iteration 61, loss = 0.20415871\n","Iteration 62, loss = 0.20376684\n","Iteration 63, loss = 0.20351297\n","Iteration 64, loss = 0.20317662\n","Iteration 65, loss = 0.20289174\n","Iteration 66, loss = 0.20262842\n","Iteration 67, loss = 0.20230952\n","Iteration 68, loss = 0.20206931\n","Iteration 69, loss = 0.20172206\n","Iteration 70, loss = 0.20148130\n","Iteration 71, loss = 0.20123802\n","Iteration 72, loss = 0.20092260\n","Iteration 73, loss = 0.20064247\n","Iteration 74, loss = 0.20039054\n","Iteration 75, loss = 0.20011950\n","Iteration 76, loss = 0.19986062\n","Iteration 77, loss = 0.19964127\n","Iteration 78, loss = 0.19946298\n","Iteration 79, loss = 0.19914933\n","Iteration 80, loss = 0.19887416\n","Iteration 81, loss = 0.19872806\n","Iteration 82, loss = 0.19838029\n","Iteration 83, loss = 0.19813947\n","Iteration 84, loss = 0.19793276\n","Iteration 85, loss = 0.19763315\n","Iteration 86, loss = 0.19746680\n","Iteration 87, loss = 0.19721419\n","Iteration 88, loss = 0.19697124\n","Iteration 89, loss = 0.19676794\n","Iteration 90, loss = 0.19653513\n","Iteration 91, loss = 0.19633059\n","Iteration 92, loss = 0.19610886\n","Iteration 93, loss = 0.19586779\n","Iteration 94, loss = 0.19568240\n","Iteration 95, loss = 0.19543736\n","Iteration 96, loss = 0.19533478\n","Iteration 97, loss = 0.19503661\n","Iteration 98, loss = 0.19481998\n","Iteration 99, loss = 0.19462598\n","Iteration 100, loss = 0.19439438\n","Iteration 101, loss = 0.19421688\n","Iteration 102, loss = 0.19404729\n","Iteration 103, loss = 0.19375880\n","Iteration 104, loss = 0.19355974\n","Iteration 105, loss = 0.19335655\n","Iteration 106, loss = 0.19314372\n","Iteration 107, loss = 0.19295582\n","Iteration 108, loss = 0.19270820\n","Iteration 109, loss = 0.19250085\n","Iteration 110, loss = 0.19230980\n","Iteration 111, loss = 0.19216959\n","Iteration 112, loss = 0.19193600\n","Iteration 113, loss = 0.19169784\n","Iteration 114, loss = 0.19148202\n","Iteration 115, loss = 0.19128976\n","Iteration 116, loss = 0.19111348\n","Iteration 117, loss = 0.19089423\n","Iteration 118, loss = 0.19069919\n","Iteration 119, loss = 0.19048892\n","Iteration 120, loss = 0.19029486\n","Iteration 121, loss = 0.19008576\n","Iteration 122, loss = 0.18991974\n","Iteration 123, loss = 0.18971106\n","Iteration 124, loss = 0.18952601\n","Iteration 125, loss = 0.18934135\n","Iteration 126, loss = 0.18916204\n","Iteration 127, loss = 0.18894336\n","Iteration 128, loss = 0.18876955\n","Iteration 129, loss = 0.18853399\n","Iteration 130, loss = 0.18836761\n","Iteration 131, loss = 0.18825437\n","Iteration 132, loss = 0.18806883\n","Iteration 133, loss = 0.18781989\n","Iteration 134, loss = 0.18763926\n","Iteration 135, loss = 0.18741051\n","Iteration 136, loss = 0.18725353\n","Iteration 137, loss = 0.18704199\n","Iteration 138, loss = 0.18687956\n","Iteration 139, loss = 0.18670729\n","Iteration 140, loss = 0.18650940\n","Iteration 141, loss = 0.18631253\n","Iteration 142, loss = 0.18611799\n","Iteration 143, loss = 0.18596524\n","Iteration 144, loss = 0.18582233\n","Iteration 145, loss = 0.18560295\n","Iteration 146, loss = 0.18541188\n","Iteration 147, loss = 0.18520239\n","Iteration 148, loss = 0.18499943\n","Iteration 149, loss = 0.18486012\n","Iteration 150, loss = 0.18468858\n","Iteration 1, loss = 0.68817399\n","Iteration 2, loss = 0.52432201\n","Iteration 3, loss = 0.44069949\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.38605389\n","Iteration 5, loss = 0.34486208\n","Iteration 6, loss = 0.31400618\n","Iteration 7, loss = 0.29173967\n","Iteration 8, loss = 0.27538517\n","Iteration 9, loss = 0.26348101\n","Iteration 10, loss = 0.25450912\n","Iteration 11, loss = 0.24764758\n","Iteration 12, loss = 0.24229398\n","Iteration 13, loss = 0.23806535\n","Iteration 14, loss = 0.23470359\n","Iteration 15, loss = 0.23181633\n","Iteration 16, loss = 0.22944104\n","Iteration 17, loss = 0.22741005\n","Iteration 18, loss = 0.22567839\n","Iteration 19, loss = 0.22410409\n","Iteration 20, loss = 0.22274272\n","Iteration 21, loss = 0.22155156\n","Iteration 22, loss = 0.22051845\n","Iteration 23, loss = 0.21951737\n","Iteration 24, loss = 0.21855332\n","Iteration 25, loss = 0.21771553\n","Iteration 26, loss = 0.21689803\n","Iteration 27, loss = 0.21621921\n","Iteration 28, loss = 0.21553118\n","Iteration 29, loss = 0.21487561\n","Iteration 30, loss = 0.21426626\n","Iteration 31, loss = 0.21367637\n","Iteration 32, loss = 0.21314121\n","Iteration 33, loss = 0.21257534\n","Iteration 34, loss = 0.21203724\n","Iteration 35, loss = 0.21159668\n","Iteration 36, loss = 0.21116208\n","Iteration 37, loss = 0.21066873\n","Iteration 38, loss = 0.21021499\n","Iteration 39, loss = 0.20979231\n","Iteration 40, loss = 0.20938467\n","Iteration 41, loss = 0.20900529\n","Iteration 42, loss = 0.20859853\n","Iteration 43, loss = 0.20825292\n","Iteration 44, loss = 0.20783234\n","Iteration 45, loss = 0.20750618\n","Iteration 46, loss = 0.20717340\n","Iteration 47, loss = 0.20682720\n","Iteration 48, loss = 0.20647372\n","Iteration 49, loss = 0.20617244\n","Iteration 50, loss = 0.20578680\n","Iteration 51, loss = 0.20548422\n","Iteration 52, loss = 0.20519239\n","Iteration 53, loss = 0.20489253\n","Iteration 54, loss = 0.20454054\n","Iteration 55, loss = 0.20424832\n","Iteration 56, loss = 0.20399762\n","Iteration 57, loss = 0.20362152\n","Iteration 58, loss = 0.20336310\n","Iteration 59, loss = 0.20308638\n","Iteration 60, loss = 0.20281811\n","Iteration 61, loss = 0.20253441\n","Iteration 62, loss = 0.20232148\n","Iteration 63, loss = 0.20198510\n","Iteration 64, loss = 0.20169338\n","Iteration 65, loss = 0.20139207\n","Iteration 66, loss = 0.20113101\n","Iteration 67, loss = 0.20091093\n","Iteration 68, loss = 0.20062826\n","Iteration 69, loss = 0.20035262\n","Iteration 70, loss = 0.20013297\n","Iteration 71, loss = 0.19987970\n","Iteration 72, loss = 0.19962323\n","Iteration 73, loss = 0.19940968\n","Iteration 74, loss = 0.19915493\n","Iteration 75, loss = 0.19890950\n","Iteration 76, loss = 0.19864347\n","Iteration 77, loss = 0.19843273\n","Iteration 78, loss = 0.19822835\n","Iteration 79, loss = 0.19793213\n","Iteration 80, loss = 0.19767147\n","Iteration 81, loss = 0.19745137\n","Iteration 82, loss = 0.19724724\n","Iteration 83, loss = 0.19699832\n","Iteration 84, loss = 0.19680267\n","Iteration 85, loss = 0.19657738\n","Iteration 86, loss = 0.19635031\n","Iteration 87, loss = 0.19615187\n","Iteration 88, loss = 0.19595135\n","Iteration 89, loss = 0.19570758\n","Iteration 90, loss = 0.19551715\n","Iteration 91, loss = 0.19524678\n","Iteration 92, loss = 0.19504280\n","Iteration 93, loss = 0.19485931\n","Iteration 94, loss = 0.19464803\n","Iteration 95, loss = 0.19447398\n","Iteration 96, loss = 0.19422461\n","Iteration 97, loss = 0.19404779\n","Iteration 98, loss = 0.19385850\n","Iteration 99, loss = 0.19365555\n","Iteration 100, loss = 0.19346338\n","Iteration 101, loss = 0.19329369\n","Iteration 102, loss = 0.19308169\n","Iteration 103, loss = 0.19289658\n","Iteration 104, loss = 0.19267439\n","Iteration 105, loss = 0.19252012\n","Iteration 106, loss = 0.19230783\n","Iteration 107, loss = 0.19212673\n","Iteration 108, loss = 0.19192187\n","Iteration 109, loss = 0.19175748\n","Iteration 110, loss = 0.19156896\n","Iteration 111, loss = 0.19132820\n","Iteration 112, loss = 0.19118209\n","Iteration 113, loss = 0.19101630\n","Iteration 114, loss = 0.19080464\n","Iteration 115, loss = 0.19064670\n","Iteration 116, loss = 0.19048987\n","Iteration 117, loss = 0.19022474\n","Iteration 118, loss = 0.19007081\n","Iteration 119, loss = 0.18988330\n","Iteration 120, loss = 0.18974757\n","Iteration 121, loss = 0.18953644\n","Iteration 122, loss = 0.18937649\n","Iteration 123, loss = 0.18928507\n","Iteration 124, loss = 0.18897497\n","Iteration 125, loss = 0.18883070\n","Iteration 126, loss = 0.18869470\n","Iteration 127, loss = 0.18850771\n","Iteration 128, loss = 0.18830959\n","Iteration 129, loss = 0.18816457\n","Iteration 130, loss = 0.18799715\n","Iteration 131, loss = 0.18784520\n","Iteration 132, loss = 0.18763195\n","Iteration 133, loss = 0.18756126\n","Iteration 134, loss = 0.18732283\n","Iteration 135, loss = 0.18716034\n","Iteration 136, loss = 0.18701198\n","Iteration 137, loss = 0.18682741\n","Iteration 138, loss = 0.18667968\n","Iteration 139, loss = 0.18657560\n","Iteration 140, loss = 0.18636899\n","Iteration 141, loss = 0.18618421\n","Iteration 142, loss = 0.18601875\n","Iteration 143, loss = 0.18590374\n","Iteration 144, loss = 0.18572896\n","Iteration 145, loss = 0.18557461\n","Iteration 146, loss = 0.18538705\n","Iteration 147, loss = 0.18523566\n","Iteration 148, loss = 0.18510752\n","Iteration 149, loss = 0.18493244\n","Iteration 150, loss = 0.18477254\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.44449976\n","Iteration 2, loss = 0.23788343\n","Iteration 3, loss = 0.21877583\n","Iteration 4, loss = 0.21088355\n","Iteration 5, loss = 0.20560659\n","Iteration 6, loss = 0.20118246\n","Iteration 7, loss = 0.19696566\n","Iteration 8, loss = 0.19250708\n","Iteration 9, loss = 0.18936746\n","Iteration 10, loss = 0.18598922\n","Iteration 11, loss = 0.18183204\n","Iteration 12, loss = 0.18018815\n","Iteration 13, loss = 0.17647816\n","Iteration 14, loss = 0.17310093\n","Iteration 15, loss = 0.16960789\n","Iteration 16, loss = 0.16705498\n","Iteration 17, loss = 0.16308424\n","Iteration 18, loss = 0.16104552\n","Iteration 19, loss = 0.15819678\n","Iteration 20, loss = 0.15505887\n","Iteration 21, loss = 0.15180366\n","Iteration 22, loss = 0.14891577\n","Iteration 23, loss = 0.14535884\n","Iteration 24, loss = 0.14392247\n","Iteration 25, loss = 0.14047857\n","Iteration 26, loss = 0.13698710\n","Iteration 27, loss = 0.13387348\n","Iteration 28, loss = 0.13110058\n","Iteration 29, loss = 0.12862769\n","Iteration 30, loss = 0.12651522\n","Iteration 31, loss = 0.12261391\n","Iteration 32, loss = 0.12021935\n","Iteration 33, loss = 0.11644830\n","Iteration 34, loss = 0.11576637\n","Iteration 35, loss = 0.11160077\n","Iteration 36, loss = 0.10802990\n","Iteration 37, loss = 0.10638791\n","Iteration 38, loss = 0.10255114\n","Iteration 39, loss = 0.10008709\n","Iteration 40, loss = 0.09850312\n","Iteration 41, loss = 0.09465487\n","Iteration 42, loss = 0.09301465\n","Iteration 43, loss = 0.08967229\n","Iteration 44, loss = 0.08765059\n","Iteration 45, loss = 0.08462463\n","Iteration 46, loss = 0.08365533\n","Iteration 47, loss = 0.08042829\n","Iteration 48, loss = 0.07800806\n","Iteration 49, loss = 0.07533528\n","Iteration 50, loss = 0.07398750\n","Iteration 51, loss = 0.07221157\n","Iteration 52, loss = 0.07107501\n","Iteration 53, loss = 0.06871872\n","Iteration 54, loss = 0.06630074\n","Iteration 55, loss = 0.06545961\n","Iteration 56, loss = 0.06265681\n","Iteration 57, loss = 0.06069299\n","Iteration 58, loss = 0.05944418\n","Iteration 59, loss = 0.05766775\n","Iteration 60, loss = 0.05539350\n","Iteration 61, loss = 0.05405361\n","Iteration 62, loss = 0.05174955\n","Iteration 63, loss = 0.04967369\n","Iteration 64, loss = 0.04988562\n","Iteration 65, loss = 0.04930335\n","Iteration 66, loss = 0.04691877\n","Iteration 67, loss = 0.04453761\n","Iteration 68, loss = 0.04408404\n","Iteration 69, loss = 0.04240715\n","Iteration 70, loss = 0.04132834\n","Iteration 71, loss = 0.04065797\n","Iteration 72, loss = 0.03862799\n","Iteration 73, loss = 0.03793579\n","Iteration 74, loss = 0.03883619\n","Iteration 75, loss = 0.03455036\n","Iteration 76, loss = 0.03496410\n","Iteration 77, loss = 0.03437189\n","Iteration 78, loss = 0.03226398\n","Iteration 79, loss = 0.03204940\n","Iteration 80, loss = 0.03007266\n","Iteration 81, loss = 0.02934522\n","Iteration 82, loss = 0.02753825\n","Iteration 83, loss = 0.02686382\n","Iteration 84, loss = 0.02518466\n","Iteration 85, loss = 0.02471312\n","Iteration 86, loss = 0.02475227\n","Iteration 87, loss = 0.02390348\n","Iteration 88, loss = 0.02285336\n","Iteration 89, loss = 0.02185631\n","Iteration 90, loss = 0.02075603\n","Iteration 91, loss = 0.01996729\n","Iteration 92, loss = 0.01977391\n","Iteration 93, loss = 0.01945531\n","Iteration 94, loss = 0.01818568\n","Iteration 95, loss = 0.01736999\n","Iteration 96, loss = 0.01690315\n","Iteration 97, loss = 0.01641595\n","Iteration 98, loss = 0.01592906\n","Iteration 99, loss = 0.01564557\n","Iteration 100, loss = 0.01457681\n","Iteration 101, loss = 0.01422934\n","Iteration 102, loss = 0.01379123\n","Iteration 103, loss = 0.01373578\n","Iteration 104, loss = 0.01373682\n","Iteration 105, loss = 0.01246253\n","Iteration 106, loss = 0.01221642\n","Iteration 107, loss = 0.01192749\n","Iteration 108, loss = 0.01127876\n","Iteration 109, loss = 0.01060467\n","Iteration 110, loss = 0.01042403\n","Iteration 111, loss = 0.01065019\n","Iteration 112, loss = 0.01010985\n","Iteration 113, loss = 0.00974740\n","Iteration 114, loss = 0.00935741\n","Iteration 115, loss = 0.00886313\n","Iteration 116, loss = 0.00879311\n","Iteration 117, loss = 0.00826589\n","Iteration 118, loss = 0.00796517\n","Iteration 119, loss = 0.00758667\n","Iteration 120, loss = 0.00738534\n","Iteration 121, loss = 0.00713954\n","Iteration 122, loss = 0.00716111\n","Iteration 123, loss = 0.00677238\n","Iteration 124, loss = 0.00653306\n","Iteration 125, loss = 0.00656709\n","Iteration 126, loss = 0.00618354\n","Iteration 127, loss = 0.00585722\n","Iteration 128, loss = 0.00563731\n","Iteration 129, loss = 0.00547491\n","Iteration 130, loss = 0.00524529\n","Iteration 131, loss = 0.00513353\n","Iteration 132, loss = 0.00503672\n","Iteration 133, loss = 0.00483354\n","Iteration 134, loss = 0.00463840\n","Iteration 135, loss = 0.00454697\n","Iteration 136, loss = 0.00435154\n","Iteration 137, loss = 0.00420437\n","Iteration 138, loss = 0.00418048\n","Iteration 139, loss = 0.00402551\n","Iteration 140, loss = 0.00392922\n","Iteration 141, loss = 0.00377292\n","Iteration 142, loss = 0.00369941\n","Iteration 143, loss = 0.00374321\n","Iteration 144, loss = 0.00348128\n","Iteration 145, loss = 0.00344441\n","Iteration 146, loss = 0.00333141\n","Iteration 147, loss = 0.00319996\n","Iteration 148, loss = 0.00309317\n","Iteration 149, loss = 0.00298357\n","Iteration 150, loss = 0.00285304\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.38236775\n","Iteration 2, loss = 0.23872471\n","Iteration 3, loss = 0.22022763\n","Iteration 4, loss = 0.21082989\n","Iteration 5, loss = 0.20521748\n","Iteration 6, loss = 0.20073284\n","Iteration 7, loss = 0.19695250\n","Iteration 8, loss = 0.19281871\n","Iteration 9, loss = 0.18994324\n","Iteration 10, loss = 0.18668612\n","Iteration 11, loss = 0.18336661\n","Iteration 12, loss = 0.18041407\n","Iteration 13, loss = 0.17687265\n","Iteration 14, loss = 0.17401251\n","Iteration 15, loss = 0.17098452\n","Iteration 16, loss = 0.16893629\n","Iteration 17, loss = 0.16558619\n","Iteration 18, loss = 0.16250924\n","Iteration 19, loss = 0.16005299\n","Iteration 20, loss = 0.15708288\n","Iteration 21, loss = 0.15357422\n","Iteration 22, loss = 0.15016598\n","Iteration 23, loss = 0.14713588\n","Iteration 24, loss = 0.14423717\n","Iteration 25, loss = 0.14070480\n","Iteration 26, loss = 0.13812736\n","Iteration 27, loss = 0.13530436\n","Iteration 28, loss = 0.13309311\n","Iteration 29, loss = 0.13025006\n","Iteration 30, loss = 0.12831736\n","Iteration 31, loss = 0.12515644\n","Iteration 32, loss = 0.12076188\n","Iteration 33, loss = 0.11752125\n","Iteration 34, loss = 0.11858285\n","Iteration 35, loss = 0.11416184\n","Iteration 36, loss = 0.11087241\n","Iteration 37, loss = 0.10648836\n","Iteration 38, loss = 0.10540369\n","Iteration 39, loss = 0.10244150\n","Iteration 40, loss = 0.10046251\n","Iteration 41, loss = 0.09822916\n","Iteration 42, loss = 0.09484466\n","Iteration 43, loss = 0.09338455\n","Iteration 44, loss = 0.09248899\n","Iteration 45, loss = 0.08677796\n","Iteration 46, loss = 0.08640106\n","Iteration 47, loss = 0.08329524\n","Iteration 48, loss = 0.08212767\n","Iteration 49, loss = 0.08004006\n","Iteration 50, loss = 0.07700646\n","Iteration 51, loss = 0.07592369\n","Iteration 52, loss = 0.07471237\n","Iteration 53, loss = 0.07333044\n","Iteration 54, loss = 0.07050066\n","Iteration 55, loss = 0.06958989\n","Iteration 56, loss = 0.06693529\n","Iteration 57, loss = 0.06344630\n","Iteration 58, loss = 0.06324889\n","Iteration 59, loss = 0.06033726\n","Iteration 60, loss = 0.05833037\n","Iteration 61, loss = 0.05740435\n","Iteration 62, loss = 0.05587503\n","Iteration 63, loss = 0.05413397\n","Iteration 64, loss = 0.05283158\n","Iteration 65, loss = 0.05058021\n","Iteration 66, loss = 0.04943986\n","Iteration 67, loss = 0.04652804\n","Iteration 68, loss = 0.04751274\n","Iteration 69, loss = 0.04518632\n","Iteration 70, loss = 0.04321888\n","Iteration 71, loss = 0.04303983\n","Iteration 72, loss = 0.04051593\n","Iteration 73, loss = 0.04096052\n","Iteration 74, loss = 0.03861133\n","Iteration 75, loss = 0.03666314\n","Iteration 76, loss = 0.03710398\n","Iteration 77, loss = 0.03573913\n","Iteration 78, loss = 0.03382252\n","Iteration 79, loss = 0.03241104\n","Iteration 80, loss = 0.03194616\n","Iteration 81, loss = 0.03071414\n","Iteration 82, loss = 0.03082506\n","Iteration 83, loss = 0.03157474\n","Iteration 84, loss = 0.03076083\n","Iteration 85, loss = 0.02845867\n","Iteration 86, loss = 0.02573721\n","Iteration 87, loss = 0.02525627\n","Iteration 88, loss = 0.02415609\n","Iteration 89, loss = 0.02365922\n","Iteration 90, loss = 0.02258210\n","Iteration 91, loss = 0.02270721\n","Iteration 92, loss = 0.02510489\n","Iteration 93, loss = 0.02231737\n","Iteration 94, loss = 0.02058035\n","Iteration 95, loss = 0.02104681\n","Iteration 96, loss = 0.02001105\n","Iteration 97, loss = 0.01906950\n","Iteration 98, loss = 0.01729360\n","Iteration 99, loss = 0.01640700\n","Iteration 100, loss = 0.01562517\n","Iteration 101, loss = 0.01527708\n","Iteration 102, loss = 0.01454665\n","Iteration 103, loss = 0.01374905\n","Iteration 104, loss = 0.01317025\n","Iteration 105, loss = 0.01298433\n","Iteration 106, loss = 0.01298355\n","Iteration 107, loss = 0.01349780\n","Iteration 108, loss = 0.01282018\n","Iteration 109, loss = 0.01181808\n","Iteration 110, loss = 0.01111151\n","Iteration 111, loss = 0.01079605\n","Iteration 112, loss = 0.01026786\n","Iteration 113, loss = 0.00994544\n","Iteration 114, loss = 0.00978984\n","Iteration 115, loss = 0.00964571\n","Iteration 116, loss = 0.00938593\n","Iteration 117, loss = 0.00915760\n","Iteration 118, loss = 0.00858106\n","Iteration 119, loss = 0.00841562\n","Iteration 120, loss = 0.00823995\n","Iteration 121, loss = 0.00774585\n","Iteration 122, loss = 0.00757484\n","Iteration 123, loss = 0.00710707\n","Iteration 124, loss = 0.00695767\n","Iteration 125, loss = 0.00644133\n","Iteration 126, loss = 0.00640151\n","Iteration 127, loss = 0.00641062\n","Iteration 128, loss = 0.00615858\n","Iteration 129, loss = 0.00572684\n","Iteration 130, loss = 0.00553363\n","Iteration 131, loss = 0.00543312\n","Iteration 132, loss = 0.00529965\n","Iteration 133, loss = 0.00529264\n","Iteration 134, loss = 0.00525867\n","Iteration 135, loss = 0.00567325\n","Iteration 136, loss = 0.00506372\n","Iteration 137, loss = 0.00451878\n","Iteration 138, loss = 0.00433866\n","Iteration 139, loss = 0.00426655\n","Iteration 140, loss = 0.00433778\n","Iteration 141, loss = 0.00406749\n","Iteration 142, loss = 0.00385956\n","Iteration 143, loss = 0.00371164\n","Iteration 144, loss = 0.00378274\n","Iteration 145, loss = 0.00364716\n","Iteration 146, loss = 0.00340637\n","Iteration 147, loss = 0.00342780\n","Iteration 148, loss = 0.00321941\n","Iteration 149, loss = 0.00336846\n","Iteration 150, loss = 0.00314153\n","Iteration 1, loss = 0.41074107\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.23955672\n","Iteration 3, loss = 0.21638093\n","Iteration 4, loss = 0.20773550\n","Iteration 5, loss = 0.20177930\n","Iteration 6, loss = 0.19681243\n","Iteration 7, loss = 0.19326653\n","Iteration 8, loss = 0.18961764\n","Iteration 9, loss = 0.18573818\n","Iteration 10, loss = 0.18168902\n","Iteration 11, loss = 0.17880287\n","Iteration 12, loss = 0.17606509\n","Iteration 13, loss = 0.17277552\n","Iteration 14, loss = 0.16909346\n","Iteration 15, loss = 0.16578515\n","Iteration 16, loss = 0.16313393\n","Iteration 17, loss = 0.15951195\n","Iteration 18, loss = 0.15686305\n","Iteration 19, loss = 0.15421293\n","Iteration 20, loss = 0.15106514\n","Iteration 21, loss = 0.14910536\n","Iteration 22, loss = 0.14401900\n","Iteration 23, loss = 0.14265098\n","Iteration 24, loss = 0.13883030\n","Iteration 25, loss = 0.13514093\n","Iteration 26, loss = 0.13262041\n","Iteration 27, loss = 0.13052477\n","Iteration 28, loss = 0.12724495\n","Iteration 29, loss = 0.12316765\n","Iteration 30, loss = 0.12168594\n","Iteration 31, loss = 0.11870439\n","Iteration 32, loss = 0.11507171\n","Iteration 33, loss = 0.11255818\n","Iteration 34, loss = 0.11168226\n","Iteration 35, loss = 0.10544122\n","Iteration 36, loss = 0.10463915\n","Iteration 37, loss = 0.10233978\n","Iteration 38, loss = 0.09889027\n","Iteration 39, loss = 0.09677752\n","Iteration 40, loss = 0.09525642\n","Iteration 41, loss = 0.09169431\n","Iteration 42, loss = 0.08818761\n","Iteration 43, loss = 0.08547379\n","Iteration 44, loss = 0.08441469\n","Iteration 45, loss = 0.08156584\n","Iteration 46, loss = 0.07922045\n","Iteration 47, loss = 0.07707154\n","Iteration 48, loss = 0.07474705\n","Iteration 49, loss = 0.07354875\n","Iteration 50, loss = 0.07104049\n","Iteration 51, loss = 0.06785640\n","Iteration 52, loss = 0.06470120\n","Iteration 53, loss = 0.06371713\n","Iteration 54, loss = 0.06104337\n","Iteration 55, loss = 0.05848911\n","Iteration 56, loss = 0.05828074\n","Iteration 57, loss = 0.05653554\n","Iteration 58, loss = 0.05529851\n","Iteration 59, loss = 0.05204460\n","Iteration 60, loss = 0.05067121\n","Iteration 61, loss = 0.04972772\n","Iteration 62, loss = 0.04699150\n","Iteration 63, loss = 0.04567335\n","Iteration 64, loss = 0.04404343\n","Iteration 65, loss = 0.04399174\n","Iteration 66, loss = 0.04230992\n","Iteration 67, loss = 0.04012777\n","Iteration 68, loss = 0.03959683\n","Iteration 69, loss = 0.03756732\n","Iteration 70, loss = 0.03548676\n","Iteration 71, loss = 0.03527375\n","Iteration 72, loss = 0.03708304\n","Iteration 73, loss = 0.03248398\n","Iteration 74, loss = 0.03075693\n","Iteration 75, loss = 0.03020191\n","Iteration 76, loss = 0.02841769\n","Iteration 77, loss = 0.02872908\n","Iteration 78, loss = 0.02772939\n","Iteration 79, loss = 0.02597192\n","Iteration 80, loss = 0.02495361\n","Iteration 81, loss = 0.02390693\n","Iteration 82, loss = 0.02403265\n","Iteration 83, loss = 0.02326708\n","Iteration 84, loss = 0.02172794\n","Iteration 85, loss = 0.02096646\n","Iteration 86, loss = 0.01985546\n","Iteration 87, loss = 0.01897723\n","Iteration 88, loss = 0.01859986\n","Iteration 89, loss = 0.01846828\n","Iteration 90, loss = 0.01721783\n","Iteration 91, loss = 0.01649208\n","Iteration 92, loss = 0.01608021\n","Iteration 93, loss = 0.01531569\n","Iteration 94, loss = 0.01612191\n","Iteration 95, loss = 0.01465525\n","Iteration 96, loss = 0.01353760\n","Iteration 97, loss = 0.01344790\n","Iteration 98, loss = 0.01307908\n","Iteration 99, loss = 0.01250684\n","Iteration 100, loss = 0.01197459\n","Iteration 101, loss = 0.01134933\n","Iteration 102, loss = 0.01118984\n","Iteration 103, loss = 0.01083407\n","Iteration 104, loss = 0.01018286\n","Iteration 105, loss = 0.01029177\n","Iteration 106, loss = 0.00998287\n","Iteration 107, loss = 0.00962581\n","Iteration 108, loss = 0.00913622\n","Iteration 109, loss = 0.00867486\n","Iteration 110, loss = 0.00831497\n","Iteration 111, loss = 0.00835988\n","Iteration 112, loss = 0.00780442\n","Iteration 113, loss = 0.00735970\n","Iteration 114, loss = 0.00720266\n","Iteration 115, loss = 0.00686152\n","Iteration 116, loss = 0.00673379\n","Iteration 117, loss = 0.00663794\n","Iteration 118, loss = 0.00624864\n","Iteration 119, loss = 0.00604485\n","Iteration 120, loss = 0.00590680\n","Iteration 121, loss = 0.00564499\n","Iteration 122, loss = 0.00564523\n","Iteration 123, loss = 0.00528584\n","Iteration 124, loss = 0.00525454\n","Iteration 125, loss = 0.00510138\n","Iteration 126, loss = 0.00474780\n","Iteration 127, loss = 0.00455052\n","Iteration 128, loss = 0.00434139\n","Iteration 129, loss = 0.00420993\n","Iteration 130, loss = 0.00405850\n","Iteration 131, loss = 0.00393450\n","Iteration 132, loss = 0.00380812\n","Iteration 133, loss = 0.00362807\n","Iteration 134, loss = 0.00362642\n","Iteration 135, loss = 0.00359108\n","Iteration 136, loss = 0.00333149\n","Iteration 137, loss = 0.00327340\n","Iteration 138, loss = 0.00312142\n","Iteration 139, loss = 0.00304515\n","Iteration 140, loss = 0.00306120\n","Iteration 141, loss = 0.00285382\n","Iteration 142, loss = 0.00272378\n","Iteration 143, loss = 0.00276291\n","Iteration 144, loss = 0.00255139\n","Iteration 145, loss = 0.00248178\n","Iteration 146, loss = 0.00239294\n","Iteration 147, loss = 0.00229826\n","Iteration 148, loss = 0.00223695\n","Iteration 149, loss = 0.00217294\n","Iteration 150, loss = 0.00207317\n","Iteration 1, loss = 0.35379733\n","Iteration 2, loss = 0.22759051\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.21462385\n","Iteration 4, loss = 0.20770213\n","Iteration 5, loss = 0.20321792\n","Iteration 6, loss = 0.19970794\n","Iteration 7, loss = 0.19427969\n","Iteration 8, loss = 0.19031976\n","Iteration 9, loss = 0.18688368\n","Iteration 10, loss = 0.18340979\n","Iteration 11, loss = 0.18002894\n","Iteration 12, loss = 0.17611508\n","Iteration 13, loss = 0.17341924\n","Iteration 14, loss = 0.17008216\n","Iteration 15, loss = 0.16693250\n","Iteration 16, loss = 0.16506568\n","Iteration 17, loss = 0.16093240\n","Iteration 18, loss = 0.15881228\n","Iteration 19, loss = 0.15530347\n","Iteration 20, loss = 0.15149696\n","Iteration 21, loss = 0.14824857\n","Iteration 22, loss = 0.14618427\n","Iteration 23, loss = 0.14221479\n","Iteration 24, loss = 0.13931618\n","Iteration 25, loss = 0.13665039\n","Iteration 26, loss = 0.13352143\n","Iteration 27, loss = 0.12991804\n","Iteration 28, loss = 0.12642599\n","Iteration 29, loss = 0.12287098\n","Iteration 30, loss = 0.12131458\n","Iteration 31, loss = 0.11789246\n","Iteration 32, loss = 0.11538978\n","Iteration 33, loss = 0.11140990\n","Iteration 34, loss = 0.10880830\n","Iteration 35, loss = 0.10594732\n","Iteration 36, loss = 0.10389681\n","Iteration 37, loss = 0.10053610\n","Iteration 38, loss = 0.09737686\n","Iteration 39, loss = 0.09540909\n","Iteration 40, loss = 0.09281790\n","Iteration 41, loss = 0.08985686\n","Iteration 42, loss = 0.08701451\n","Iteration 43, loss = 0.08403103\n","Iteration 44, loss = 0.08273764\n","Iteration 45, loss = 0.07973372\n","Iteration 46, loss = 0.07832441\n","Iteration 47, loss = 0.07489041\n","Iteration 48, loss = 0.07361648\n","Iteration 49, loss = 0.07260378\n","Iteration 50, loss = 0.06798975\n","Iteration 51, loss = 0.06648315\n","Iteration 52, loss = 0.06643149\n","Iteration 53, loss = 0.06305933\n","Iteration 54, loss = 0.06166077\n","Iteration 55, loss = 0.05871465\n","Iteration 56, loss = 0.05563251\n","Iteration 57, loss = 0.05535911\n","Iteration 58, loss = 0.05272558\n","Iteration 59, loss = 0.05129283\n","Iteration 60, loss = 0.05187472\n","Iteration 61, loss = 0.04784065\n","Iteration 62, loss = 0.04700447\n","Iteration 63, loss = 0.04514815\n","Iteration 64, loss = 0.04206147\n","Iteration 65, loss = 0.04195746\n","Iteration 66, loss = 0.04117184\n","Iteration 67, loss = 0.03976470\n","Iteration 68, loss = 0.03794280\n","Iteration 69, loss = 0.03597782\n","Iteration 70, loss = 0.03430167\n","Iteration 71, loss = 0.03441290\n","Iteration 72, loss = 0.03303044\n","Iteration 73, loss = 0.03084059\n","Iteration 74, loss = 0.03084120\n","Iteration 75, loss = 0.02887544\n","Iteration 76, loss = 0.02763835\n","Iteration 77, loss = 0.02597280\n","Iteration 78, loss = 0.02548095\n","Iteration 79, loss = 0.02538957\n","Iteration 80, loss = 0.02362655\n","Iteration 81, loss = 0.02236778\n","Iteration 82, loss = 0.02181872\n","Iteration 83, loss = 0.02135304\n","Iteration 84, loss = 0.01987691\n","Iteration 85, loss = 0.01920581\n","Iteration 86, loss = 0.01850674\n","Iteration 87, loss = 0.01770591\n","Iteration 88, loss = 0.01707188\n","Iteration 89, loss = 0.01614509\n","Iteration 90, loss = 0.01618174\n","Iteration 91, loss = 0.01521474\n","Iteration 92, loss = 0.01517733\n","Iteration 93, loss = 0.01462599\n","Iteration 94, loss = 0.01419045\n","Iteration 95, loss = 0.01347567\n","Iteration 96, loss = 0.01191541\n","Iteration 97, loss = 0.01196184\n","Iteration 98, loss = 0.01118135\n","Iteration 99, loss = 0.01183155\n","Iteration 100, loss = 0.01035702\n","Iteration 101, loss = 0.01019517\n","Iteration 102, loss = 0.00972304\n","Iteration 103, loss = 0.00918409\n","Iteration 104, loss = 0.00880027\n","Iteration 105, loss = 0.00857850\n","Iteration 106, loss = 0.00821277\n","Iteration 107, loss = 0.00783294\n","Iteration 108, loss = 0.00750855\n","Iteration 109, loss = 0.00729708\n","Iteration 110, loss = 0.00717051\n","Iteration 111, loss = 0.00679889\n","Iteration 112, loss = 0.00657222\n","Iteration 113, loss = 0.00637471\n","Iteration 114, loss = 0.00643323\n","Iteration 115, loss = 0.00600794\n","Iteration 116, loss = 0.00572167\n","Iteration 117, loss = 0.00550822\n","Iteration 118, loss = 0.00533219\n","Iteration 119, loss = 0.00512537\n","Iteration 120, loss = 0.00483920\n","Iteration 121, loss = 0.00474583\n","Iteration 122, loss = 0.00457831\n","Iteration 123, loss = 0.00454320\n","Iteration 124, loss = 0.00429558\n","Iteration 125, loss = 0.00412106\n","Iteration 126, loss = 0.00399775\n","Iteration 127, loss = 0.00386286\n","Iteration 128, loss = 0.00375961\n","Iteration 129, loss = 0.00354286\n","Iteration 130, loss = 0.00365589\n","Iteration 131, loss = 0.00351246\n","Iteration 132, loss = 0.00346140\n","Iteration 133, loss = 0.00322313\n","Iteration 134, loss = 0.00306217\n","Iteration 135, loss = 0.00299669\n","Iteration 136, loss = 0.00286022\n","Iteration 137, loss = 0.00271188\n","Iteration 138, loss = 0.00274835\n","Iteration 139, loss = 0.00271268\n","Iteration 140, loss = 0.00251411\n","Iteration 141, loss = 0.00242399\n","Iteration 142, loss = 0.00233820\n","Iteration 143, loss = 0.00227629\n","Iteration 144, loss = 0.00222137\n","Iteration 145, loss = 0.00216890\n","Iteration 146, loss = 0.00212528\n","Iteration 147, loss = 0.00202334\n","Iteration 148, loss = 0.00197922\n","Iteration 149, loss = 0.00192384\n","Iteration 150, loss = 0.00187245\n","Iteration 1, loss = 0.43400148\n","Iteration 2, loss = 0.23587232\n","Iteration 3, loss = 0.21644796"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration 4, loss = 0.20979482\n","Iteration 5, loss = 0.20458197\n","Iteration 6, loss = 0.20051365\n","Iteration 7, loss = 0.19660324\n","Iteration 8, loss = 0.19316171\n","Iteration 9, loss = 0.19065495\n","Iteration 10, loss = 0.18715342\n","Iteration 11, loss = 0.18501587\n","Iteration 12, loss = 0.18116102\n","Iteration 13, loss = 0.17831671\n","Iteration 14, loss = 0.17628765\n","Iteration 15, loss = 0.17354550\n","Iteration 16, loss = 0.16991163\n","Iteration 17, loss = 0.16807789\n","Iteration 18, loss = 0.16459230\n","Iteration 19, loss = 0.16158936\n","Iteration 20, loss = 0.15864069\n","Iteration 21, loss = 0.15581284\n","Iteration 22, loss = 0.15288041\n","Iteration 23, loss = 0.15018921\n","Iteration 24, loss = 0.14751375\n","Iteration 25, loss = 0.14436387\n","Iteration 26, loss = 0.14044577\n","Iteration 27, loss = 0.13851038\n","Iteration 28, loss = 0.13443792\n","Iteration 29, loss = 0.13163203\n","Iteration 30, loss = 0.12912652\n","Iteration 31, loss = 0.12490793\n","Iteration 32, loss = 0.12178387\n","Iteration 33, loss = 0.11822935\n","Iteration 34, loss = 0.11595844\n","Iteration 35, loss = 0.11233399\n","Iteration 36, loss = 0.11177450\n","Iteration 37, loss = 0.10911199\n","Iteration 38, loss = 0.10373055\n","Iteration 39, loss = 0.10160752\n","Iteration 40, loss = 0.09787564\n","Iteration 41, loss = 0.09490295\n","Iteration 42, loss = 0.09290677\n","Iteration 43, loss = 0.09152258\n","Iteration 44, loss = 0.08715741\n","Iteration 45, loss = 0.08620931\n","Iteration 46, loss = 0.08225356\n","Iteration 47, loss = 0.08035896\n","Iteration 48, loss = 0.07627530\n","Iteration 49, loss = 0.07411024\n","Iteration 50, loss = 0.07135909\n","Iteration 51, loss = 0.07029775\n","Iteration 52, loss = 0.06698084\n","Iteration 53, loss = 0.06652853\n","Iteration 54, loss = 0.06338108\n","Iteration 55, loss = 0.06227727\n","Iteration 56, loss = 0.06049563\n","Iteration 57, loss = 0.05676028\n","Iteration 58, loss = 0.05563861\n","Iteration 59, loss = 0.05280319\n","Iteration 60, loss = 0.05090308\n","Iteration 61, loss = 0.04942157\n","Iteration 62, loss = 0.04809017\n","Iteration 63, loss = 0.04769236\n","Iteration 64, loss = 0.04471706\n","Iteration 65, loss = 0.04374612\n","Iteration 66, loss = 0.04077942\n","Iteration 67, loss = 0.03998296\n","Iteration 68, loss = 0.03819827\n","Iteration 69, loss = 0.03698429\n","Iteration 70, loss = 0.03560544\n","Iteration 71, loss = 0.03416587\n","Iteration 72, loss = 0.03273417\n","Iteration 73, loss = 0.03272075\n","Iteration 74, loss = 0.03081123\n","Iteration 75, loss = 0.02957096\n","Iteration 76, loss = 0.02828322\n","Iteration 77, loss = 0.02765130\n","Iteration 78, loss = 0.02757407\n","Iteration 79, loss = 0.02573050\n","Iteration 80, loss = 0.02530622\n","Iteration 81, loss = 0.02380190\n","Iteration 82, loss = 0.02250376\n","Iteration 83, loss = 0.02159761\n","Iteration 84, loss = 0.02046908\n","Iteration 85, loss = 0.02027081\n","Iteration 86, loss = 0.01956277\n","Iteration 87, loss = 0.01844987\n","Iteration 88, loss = 0.01770326\n","Iteration 89, loss = 0.01920819\n","Iteration 90, loss = 0.01700408\n","Iteration 91, loss = 0.01640341\n","Iteration 92, loss = 0.01535864\n","Iteration 93, loss = 0.01465878\n","Iteration 94, loss = 0.01391632\n","Iteration 95, loss = 0.01303268\n","Iteration 96, loss = 0.01288926\n","Iteration 97, loss = 0.01249961\n","Iteration 98, loss = 0.01216382\n","Iteration 99, loss = 0.01134816\n","Iteration 100, loss = 0.01087663\n","Iteration 101, loss = 0.01105902\n","Iteration 102, loss = 0.01038447\n","Iteration 103, loss = 0.00982843\n","Iteration 104, loss = 0.00948096\n","Iteration 105, loss = 0.00928276\n","Iteration 106, loss = 0.00858437\n","Iteration 107, loss = 0.00863270\n","Iteration 108, loss = 0.00807442\n","Iteration 109, loss = 0.00806425\n","Iteration 110, loss = 0.00753503\n","Iteration 111, loss = 0.00754009\n","Iteration 112, loss = 0.00679849\n","Iteration 113, loss = 0.00668939\n","Iteration 114, loss = 0.00683029\n","Iteration 115, loss = 0.00627805\n","Iteration 116, loss = 0.00603395\n","Iteration 117, loss = 0.00574688\n","Iteration 118, loss = 0.00562572\n","Iteration 119, loss = 0.00551707\n","Iteration 120, loss = 0.00521800\n","Iteration 121, loss = 0.00496189\n","Iteration 122, loss = 0.00483623\n","Iteration 123, loss = 0.00471058\n","Iteration 124, loss = 0.00452479\n","Iteration 125, loss = 0.00455380\n","Iteration 126, loss = 0.00442545\n","Iteration 127, loss = 0.00408229\n","Iteration 128, loss = 0.00396171\n","Iteration 129, loss = 0.00381869\n","Iteration 130, loss = 0.00367852\n","Iteration 131, loss = 0.00360993\n","Iteration 132, loss = 0.00348344\n","Iteration 133, loss = 0.00341056\n","Iteration 134, loss = 0.00329917\n","Iteration 135, loss = 0.00321179\n","Iteration 136, loss = 0.00305951\n","Iteration 137, loss = 0.00306900\n","Iteration 138, loss = 0.00294413\n","Iteration 139, loss = 0.00278723\n","Iteration 140, loss = 0.00275501\n","Iteration 141, loss = 0.00258955\n","Iteration 142, loss = 0.00253788\n","Iteration 143, loss = 0.00263168\n","Iteration 144, loss = 0.00243015\n","Iteration 145, loss = 0.00225078\n","Iteration 146, loss = 0.00225750\n","Iteration 147, loss = 0.00224519\n","Iteration 148, loss = 0.00212566\n","Iteration 149, loss = 0.00208838\n","Iteration 150, loss = 0.00208775\n","Iteration 1, loss = 0.62210815\n","Iteration 2, loss = 0.53483555\n","Iteration 3, loss = 0.46556091\n","Iteration 4, loss = 0.41163311\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 5, loss = 0.37100588\n","Iteration 6, loss = 0.34054807\n","Iteration 7, loss = 0.31766372\n","Iteration 8, loss = 0.30028865\n","Iteration 9, loss = 0.28707773\n","Iteration 10, loss = 0.27676344\n","Iteration 11, loss = 0.26874624\n","Iteration 12, loss = 0.26227006\n","Iteration 13, loss = 0.25692301\n","Iteration 14, loss = 0.25257367\n","Iteration 15, loss = 0.24887033\n","Iteration 16, loss = 0.24573680\n","Iteration 17, loss = 0.24304989\n","Iteration 18, loss = 0.24070091\n","Iteration 19, loss = 0.23863922\n","Iteration 20, loss = 0.23681901\n","Iteration 21, loss = 0.23512192\n","Iteration 22, loss = 0.23355760\n","Iteration 23, loss = 0.23222981\n","Iteration 24, loss = 0.23099912\n","Iteration 25, loss = 0.22985737\n","Iteration 26, loss = 0.22879712\n","Iteration 27, loss = 0.22785604\n","Iteration 28, loss = 0.22694256\n","Iteration 29, loss = 0.22609117\n","Iteration 30, loss = 0.22532954\n","Iteration 31, loss = 0.22457438\n","Iteration 32, loss = 0.22385145\n","Iteration 33, loss = 0.22315571\n","Iteration 34, loss = 0.22252982\n","Iteration 35, loss = 0.22191179\n","Iteration 36, loss = 0.22135514\n","Iteration 37, loss = 0.22079392\n","Iteration 38, loss = 0.22025648\n","Iteration 39, loss = 0.21976684\n","Iteration 40, loss = 0.21930528\n","Iteration 41, loss = 0.21881406\n","Iteration 42, loss = 0.21837580\n","Iteration 43, loss = 0.21795188\n","Iteration 44, loss = 0.21752815\n","Iteration 45, loss = 0.21711767\n","Iteration 46, loss = 0.21671637\n","Iteration 47, loss = 0.21635122\n","Iteration 48, loss = 0.21594565\n","Iteration 49, loss = 0.21558222\n","Iteration 50, loss = 0.21524527\n","Iteration 51, loss = 0.21491503\n","Iteration 52, loss = 0.21455596\n","Iteration 53, loss = 0.21421926\n","Iteration 54, loss = 0.21391234\n","Iteration 55, loss = 0.21363084\n","Iteration 56, loss = 0.21331776\n","Iteration 57, loss = 0.21301007\n","Iteration 58, loss = 0.21269827\n","Iteration 59, loss = 0.21240424\n","Iteration 60, loss = 0.21214938\n","Iteration 61, loss = 0.21185794\n","Iteration 62, loss = 0.21160698\n","Iteration 63, loss = 0.21131158\n","Iteration 64, loss = 0.21106457\n","Iteration 65, loss = 0.21081801\n","Iteration 66, loss = 0.21054525\n","Iteration 67, loss = 0.21031523\n","Iteration 68, loss = 0.21005949\n","Iteration 69, loss = 0.20981843\n","Iteration 70, loss = 0.20955075\n","Iteration 71, loss = 0.20937322\n","Iteration 72, loss = 0.20909828\n","Iteration 73, loss = 0.20885604\n","Iteration 74, loss = 0.20861527\n","Iteration 75, loss = 0.20838702\n","Iteration 76, loss = 0.20820977\n","Iteration 77, loss = 0.20800417\n","Iteration 78, loss = 0.20778067\n","Iteration 79, loss = 0.20757348\n","Iteration 80, loss = 0.20738567\n","Iteration 81, loss = 0.20716659\n","Iteration 82, loss = 0.20698270\n","Iteration 83, loss = 0.20677769\n","Iteration 84, loss = 0.20660563\n","Iteration 85, loss = 0.20639768\n","Iteration 86, loss = 0.20623038\n","Iteration 87, loss = 0.20603365\n","Iteration 88, loss = 0.20587432\n","Iteration 89, loss = 0.20566022\n","Iteration 90, loss = 0.20552633\n","Iteration 91, loss = 0.20535503\n","Iteration 92, loss = 0.20516310\n","Iteration 93, loss = 0.20499325\n","Iteration 94, loss = 0.20479747\n","Iteration 95, loss = 0.20465529\n","Iteration 96, loss = 0.20451094\n","Iteration 97, loss = 0.20429442\n","Iteration 98, loss = 0.20411276\n","Iteration 99, loss = 0.20399381\n","Iteration 100, loss = 0.20380689\n","Iteration 101, loss = 0.20363274\n","Iteration 102, loss = 0.20349093\n","Iteration 103, loss = 0.20333059\n","Iteration 104, loss = 0.20319481\n","Iteration 105, loss = 0.20298483\n","Iteration 106, loss = 0.20287226\n","Iteration 107, loss = 0.20273849\n","Iteration 108, loss = 0.20251868\n","Iteration 109, loss = 0.20241674\n","Iteration 110, loss = 0.20224577\n","Iteration 111, loss = 0.20205400\n","Iteration 112, loss = 0.20188150\n","Iteration 113, loss = 0.20170944\n","Iteration 114, loss = 0.20155754\n","Iteration 115, loss = 0.20142753\n","Iteration 116, loss = 0.20121385\n","Iteration 117, loss = 0.20105132\n","Iteration 118, loss = 0.20094951\n","Iteration 119, loss = 0.20077171\n","Iteration 120, loss = 0.20057819\n","Iteration 121, loss = 0.20043723\n","Iteration 122, loss = 0.20027016\n","Iteration 123, loss = 0.20012320\n","Iteration 124, loss = 0.20000315\n","Iteration 125, loss = 0.19981433\n","Iteration 126, loss = 0.19965995\n","Iteration 127, loss = 0.19953521\n","Iteration 128, loss = 0.19936765\n","Iteration 129, loss = 0.19922528\n","Iteration 130, loss = 0.19907572\n","Iteration 131, loss = 0.19891920\n","Iteration 132, loss = 0.19883165\n","Iteration 133, loss = 0.19865744\n","Iteration 134, loss = 0.19849040\n","Iteration 135, loss = 0.19831147\n","Iteration 136, loss = 0.19819443\n","Iteration 137, loss = 0.19809121\n","Iteration 138, loss = 0.19790347\n","Iteration 139, loss = 0.19774554\n","Iteration 140, loss = 0.19765666\n","Iteration 141, loss = 0.19749176\n","Iteration 142, loss = 0.19731992\n","Iteration 143, loss = 0.19718895\n","Iteration 144, loss = 0.19707167\n","Iteration 145, loss = 0.19693502\n","Iteration 146, loss = 0.19677113\n","Iteration 147, loss = 0.19672264\n","Iteration 148, loss = 0.19650521\n","Iteration 149, loss = 0.19644314\n","Iteration 150, loss = 0.19625598\n","Iteration 1, loss = 0.68910489\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.55522335\n","Iteration 3, loss = 0.47277073\n","Iteration 4, loss = 0.41937170\n","Iteration 5, loss = 0.38233411\n","Iteration 6, loss = 0.35448519\n","Iteration 7, loss = 0.33273477\n","Iteration 8, loss = 0.31564028\n","Iteration 9, loss = 0.30202730\n","Iteration 10, loss = 0.29099500\n","Iteration 11, loss = 0.28203279\n","Iteration 12, loss = 0.27467249\n","Iteration 13, loss = 0.26844265\n","Iteration 14, loss = 0.26330933\n","Iteration 15, loss = 0.25886613\n","Iteration 16, loss = 0.25514198\n","Iteration 17, loss = 0.25183785\n","Iteration 18, loss = 0.24896972\n","Iteration 19, loss = 0.24647925\n","Iteration 20, loss = 0.24420326\n","Iteration 21, loss = 0.24219866\n","Iteration 22, loss = 0.24044283\n","Iteration 23, loss = 0.23882426\n","Iteration 24, loss = 0.23733502\n","Iteration 25, loss = 0.23593993\n","Iteration 26, loss = 0.23472358\n","Iteration 27, loss = 0.23357187\n","Iteration 28, loss = 0.23252789\n","Iteration 29, loss = 0.23149291\n","Iteration 30, loss = 0.23056434\n","Iteration 31, loss = 0.22971782\n","Iteration 32, loss = 0.22884492\n","Iteration 33, loss = 0.22805276\n","Iteration 34, loss = 0.22727301\n","Iteration 35, loss = 0.22657464\n","Iteration 36, loss = 0.22588225\n","Iteration 37, loss = 0.22523747\n","Iteration 38, loss = 0.22459354\n","Iteration 39, loss = 0.22399329\n","Iteration 40, loss = 0.22339511\n","Iteration 41, loss = 0.22282519\n","Iteration 42, loss = 0.22231898\n","Iteration 43, loss = 0.22180175\n","Iteration 44, loss = 0.22125291\n","Iteration 45, loss = 0.22079199\n","Iteration 46, loss = 0.22031604\n","Iteration 47, loss = 0.21985885\n","Iteration 48, loss = 0.21939132\n","Iteration 49, loss = 0.21895496\n","Iteration 50, loss = 0.21854457\n","Iteration 51, loss = 0.21812222\n","Iteration 52, loss = 0.21772225\n","Iteration 53, loss = 0.21732679\n","Iteration 54, loss = 0.21693875\n","Iteration 55, loss = 0.21656029\n","Iteration 56, loss = 0.21624658\n","Iteration 57, loss = 0.21586402\n","Iteration 58, loss = 0.21555356\n","Iteration 59, loss = 0.21516987\n","Iteration 60, loss = 0.21484151\n","Iteration 61, loss = 0.21449296\n","Iteration 62, loss = 0.21415878\n","Iteration 63, loss = 0.21380069\n","Iteration 64, loss = 0.21348931\n","Iteration 65, loss = 0.21320811\n","Iteration 66, loss = 0.21287874\n","Iteration 67, loss = 0.21256225\n","Iteration 68, loss = 0.21225621\n","Iteration 69, loss = 0.21196030\n","Iteration 70, loss = 0.21168526\n","Iteration 71, loss = 0.21140892\n","Iteration 72, loss = 0.21111504\n","Iteration 73, loss = 0.21082737\n","Iteration 74, loss = 0.21057750\n","Iteration 75, loss = 0.21030788\n","Iteration 76, loss = 0.21004551\n","Iteration 77, loss = 0.20976611\n","Iteration 78, loss = 0.20949291\n","Iteration 79, loss = 0.20925074\n","Iteration 80, loss = 0.20899327\n","Iteration 81, loss = 0.20871344\n","Iteration 82, loss = 0.20845232\n","Iteration 83, loss = 0.20817615\n","Iteration 84, loss = 0.20794274\n","Iteration 85, loss = 0.20768357\n","Iteration 86, loss = 0.20742747\n","Iteration 87, loss = 0.20718626\n","Iteration 88, loss = 0.20695387\n","Iteration 89, loss = 0.20672475\n","Iteration 90, loss = 0.20646308\n","Iteration 91, loss = 0.20622928\n","Iteration 92, loss = 0.20600171\n","Iteration 93, loss = 0.20580609\n","Iteration 94, loss = 0.20556765\n","Iteration 95, loss = 0.20533620\n","Iteration 96, loss = 0.20512632\n","Iteration 97, loss = 0.20491599\n","Iteration 98, loss = 0.20466692\n","Iteration 99, loss = 0.20445348\n","Iteration 100, loss = 0.20423725\n","Iteration 101, loss = 0.20404126\n","Iteration 102, loss = 0.20383918\n","Iteration 103, loss = 0.20362330\n","Iteration 104, loss = 0.20344742\n","Iteration 105, loss = 0.20324220\n","Iteration 106, loss = 0.20304951\n","Iteration 107, loss = 0.20283368\n","Iteration 108, loss = 0.20266033\n","Iteration 109, loss = 0.20246918\n","Iteration 110, loss = 0.20233844\n","Iteration 111, loss = 0.20208812\n","Iteration 112, loss = 0.20193565\n","Iteration 113, loss = 0.20177330\n","Iteration 114, loss = 0.20157449\n","Iteration 115, loss = 0.20140571\n","Iteration 116, loss = 0.20129247\n","Iteration 117, loss = 0.20112471\n","Iteration 118, loss = 0.20093716\n","Iteration 119, loss = 0.20078121\n","Iteration 120, loss = 0.20061059\n","Iteration 121, loss = 0.20045516\n","Iteration 122, loss = 0.20030781\n","Iteration 123, loss = 0.20012081\n","Iteration 124, loss = 0.20001254\n","Iteration 125, loss = 0.19984654\n","Iteration 126, loss = 0.19965588\n","Iteration 127, loss = 0.19949629\n","Iteration 128, loss = 0.19942889\n","Iteration 129, loss = 0.19923240\n","Iteration 130, loss = 0.19906123\n","Iteration 131, loss = 0.19894616\n","Iteration 132, loss = 0.19875402\n","Iteration 133, loss = 0.19861336\n","Iteration 134, loss = 0.19845281\n","Iteration 135, loss = 0.19831126\n","Iteration 136, loss = 0.19815716\n","Iteration 137, loss = 0.19805065\n","Iteration 138, loss = 0.19788682\n","Iteration 139, loss = 0.19769991\n","Iteration 140, loss = 0.19760898\n","Iteration 141, loss = 0.19744292\n","Iteration 142, loss = 0.19730843\n","Iteration 143, loss = 0.19718237\n","Iteration 144, loss = 0.19703997\n","Iteration 145, loss = 0.19689431\n","Iteration 146, loss = 0.19676232\n","Iteration 147, loss = 0.19661758\n","Iteration 148, loss = 0.19647173\n","Iteration 149, loss = 0.19633352\n","Iteration 150, loss = 0.19620134\n","Iteration 1, loss = 0.87997898\n","Iteration 2, loss = 0.57673335\n","Iteration 3, loss = 0.45984670\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.39270155\n","Iteration 5, loss = 0.34739349\n","Iteration 6, loss = 0.31637431\n","Iteration 7, loss = 0.29445280\n","Iteration 8, loss = 0.27872217\n","Iteration 9, loss = 0.26722339\n","Iteration 10, loss = 0.25859796\n","Iteration 11, loss = 0.25203458\n","Iteration 12, loss = 0.24699487\n","Iteration 13, loss = 0.24295811\n","Iteration 14, loss = 0.23962024\n","Iteration 15, loss = 0.23682897\n","Iteration 16, loss = 0.23446563\n","Iteration 17, loss = 0.23245417\n","Iteration 18, loss = 0.23062429\n","Iteration 19, loss = 0.22898257\n","Iteration 20, loss = 0.22755995\n","Iteration 21, loss = 0.22626769\n","Iteration 22, loss = 0.22507370\n","Iteration 23, loss = 0.22396370\n","Iteration 24, loss = 0.22298244\n","Iteration 25, loss = 0.22204834\n","Iteration 26, loss = 0.22120323\n","Iteration 27, loss = 0.22036344\n","Iteration 28, loss = 0.21958435\n","Iteration 29, loss = 0.21890797\n","Iteration 30, loss = 0.21822401\n","Iteration 31, loss = 0.21756078\n","Iteration 32, loss = 0.21696802\n","Iteration 33, loss = 0.21634288\n","Iteration 34, loss = 0.21581008\n","Iteration 35, loss = 0.21528411\n","Iteration 36, loss = 0.21478265\n","Iteration 37, loss = 0.21425575\n","Iteration 38, loss = 0.21378434\n","Iteration 39, loss = 0.21327933\n","Iteration 40, loss = 0.21286612\n","Iteration 41, loss = 0.21241940\n","Iteration 42, loss = 0.21201224\n","Iteration 43, loss = 0.21157905\n","Iteration 44, loss = 0.21119905\n","Iteration 45, loss = 0.21080576\n","Iteration 46, loss = 0.21041723\n","Iteration 47, loss = 0.21005308\n","Iteration 48, loss = 0.20974688\n","Iteration 49, loss = 0.20931284\n","Iteration 50, loss = 0.20895551\n","Iteration 51, loss = 0.20860804\n","Iteration 52, loss = 0.20833993\n","Iteration 53, loss = 0.20797252\n","Iteration 54, loss = 0.20764623\n","Iteration 55, loss = 0.20733814\n","Iteration 56, loss = 0.20707006\n","Iteration 57, loss = 0.20673995\n","Iteration 58, loss = 0.20646960\n","Iteration 59, loss = 0.20615536\n","Iteration 60, loss = 0.20586821\n","Iteration 61, loss = 0.20560220\n","Iteration 62, loss = 0.20536340\n","Iteration 63, loss = 0.20505297\n","Iteration 64, loss = 0.20481944\n","Iteration 65, loss = 0.20453464\n","Iteration 66, loss = 0.20424265\n","Iteration 67, loss = 0.20399816\n","Iteration 68, loss = 0.20374043\n","Iteration 69, loss = 0.20352439\n","Iteration 70, loss = 0.20323657\n","Iteration 71, loss = 0.20302130\n","Iteration 72, loss = 0.20277589\n","Iteration 73, loss = 0.20254463\n","Iteration 74, loss = 0.20230242\n","Iteration 75, loss = 0.20206116\n","Iteration 76, loss = 0.20181425\n","Iteration 77, loss = 0.20158570\n","Iteration 78, loss = 0.20140760\n","Iteration 79, loss = 0.20108968\n","Iteration 80, loss = 0.20090561\n","Iteration 81, loss = 0.20072645\n","Iteration 82, loss = 0.20042855\n","Iteration 83, loss = 0.20023978\n","Iteration 84, loss = 0.19999331\n","Iteration 85, loss = 0.19979116\n","Iteration 86, loss = 0.19958823\n","Iteration 87, loss = 0.19940843\n","Iteration 88, loss = 0.19918712\n","Iteration 89, loss = 0.19896617\n","Iteration 90, loss = 0.19877282\n","Iteration 91, loss = 0.19852666\n","Iteration 92, loss = 0.19835258\n","Iteration 93, loss = 0.19815278\n","Iteration 94, loss = 0.19796663\n","Iteration 95, loss = 0.19775327\n","Iteration 96, loss = 0.19757973\n","Iteration 97, loss = 0.19739100\n","Iteration 98, loss = 0.19720947\n","Iteration 99, loss = 0.19702233\n","Iteration 100, loss = 0.19684487\n","Iteration 101, loss = 0.19663565\n","Iteration 102, loss = 0.19645998\n","Iteration 103, loss = 0.19627664\n","Iteration 104, loss = 0.19611036\n","Iteration 105, loss = 0.19591115\n","Iteration 106, loss = 0.19576218\n","Iteration 107, loss = 0.19555758\n","Iteration 108, loss = 0.19537082\n","Iteration 109, loss = 0.19519112\n","Iteration 110, loss = 0.19500773\n","Iteration 111, loss = 0.19485564\n","Iteration 112, loss = 0.19466856\n","Iteration 113, loss = 0.19448781\n","Iteration 114, loss = 0.19430845\n","Iteration 115, loss = 0.19417502\n","Iteration 116, loss = 0.19400164\n","Iteration 117, loss = 0.19388356\n","Iteration 118, loss = 0.19364510\n","Iteration 119, loss = 0.19347438\n","Iteration 120, loss = 0.19330335\n","Iteration 121, loss = 0.19315738\n","Iteration 122, loss = 0.19298943\n","Iteration 123, loss = 0.19279942\n","Iteration 124, loss = 0.19264238\n","Iteration 125, loss = 0.19249044\n","Iteration 126, loss = 0.19232651\n","Iteration 127, loss = 0.19214141\n","Iteration 128, loss = 0.19198289\n","Iteration 129, loss = 0.19180827\n","Iteration 130, loss = 0.19164508\n","Iteration 131, loss = 0.19149990\n","Iteration 132, loss = 0.19133623\n","Iteration 133, loss = 0.19123012\n","Iteration 134, loss = 0.19103337\n","Iteration 135, loss = 0.19087876\n","Iteration 136, loss = 0.19071042\n","Iteration 137, loss = 0.19058073\n","Iteration 138, loss = 0.19042481\n","Iteration 139, loss = 0.19025788\n","Iteration 140, loss = 0.19009510\n","Iteration 141, loss = 0.18995467\n","Iteration 142, loss = 0.18976198\n","Iteration 143, loss = 0.18961986\n","Iteration 144, loss = 0.18947453\n","Iteration 145, loss = 0.18934628\n","Iteration 146, loss = 0.18920731\n","Iteration 147, loss = 0.18903177\n","Iteration 148, loss = 0.18884459\n","Iteration 149, loss = 0.18870615\n","Iteration 150, loss = 0.18854853\n","Iteration 1, loss = 0.81758672\n","Iteration 2, loss = 0.61728110\n","Iteration 3, loss = 0.51358694\n","Iteration 4, loss = 0.43912376\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 5, loss = 0.38361743\n","Iteration 6, loss = 0.34420776\n","Iteration 7, loss = 0.31637517\n","Iteration 8, loss = 0.29654944\n","Iteration 9, loss = 0.28230028\n","Iteration 10, loss = 0.27182442\n","Iteration 11, loss = 0.26383329\n","Iteration 12, loss = 0.25755153\n","Iteration 13, loss = 0.25265123\n","Iteration 14, loss = 0.24863660\n","Iteration 15, loss = 0.24538371\n","Iteration 16, loss = 0.24261444\n","Iteration 17, loss = 0.24021953\n","Iteration 18, loss = 0.23812049\n","Iteration 19, loss = 0.23632382\n","Iteration 20, loss = 0.23472026\n","Iteration 21, loss = 0.23324707\n","Iteration 22, loss = 0.23192948\n","Iteration 23, loss = 0.23075399\n","Iteration 24, loss = 0.22966966\n","Iteration 25, loss = 0.22863163\n","Iteration 26, loss = 0.22772140\n","Iteration 27, loss = 0.22687752\n","Iteration 28, loss = 0.22611102\n","Iteration 29, loss = 0.22537824\n","Iteration 30, loss = 0.22469185\n","Iteration 31, loss = 0.22404076\n","Iteration 32, loss = 0.22345676\n","Iteration 33, loss = 0.22282937\n","Iteration 34, loss = 0.22229596\n","Iteration 35, loss = 0.22175545\n","Iteration 36, loss = 0.22122846\n","Iteration 37, loss = 0.22073768\n","Iteration 38, loss = 0.22027673\n","Iteration 39, loss = 0.21981906\n","Iteration 40, loss = 0.21938257\n","Iteration 41, loss = 0.21897064\n","Iteration 42, loss = 0.21854152\n","Iteration 43, loss = 0.21817081\n","Iteration 44, loss = 0.21780199\n","Iteration 45, loss = 0.21742167\n","Iteration 46, loss = 0.21705886\n","Iteration 47, loss = 0.21673333\n","Iteration 48, loss = 0.21633912\n","Iteration 49, loss = 0.21600813\n","Iteration 50, loss = 0.21568453\n","Iteration 51, loss = 0.21535841\n","Iteration 52, loss = 0.21503541\n","Iteration 53, loss = 0.21474147\n","Iteration 54, loss = 0.21444191\n","Iteration 55, loss = 0.21417387\n","Iteration 56, loss = 0.21384768\n","Iteration 57, loss = 0.21355246\n","Iteration 58, loss = 0.21326641\n","Iteration 59, loss = 0.21298248\n","Iteration 60, loss = 0.21272383\n","Iteration 61, loss = 0.21243734\n","Iteration 62, loss = 0.21216802\n","Iteration 63, loss = 0.21192233\n","Iteration 64, loss = 0.21162633\n","Iteration 65, loss = 0.21138924\n","Iteration 66, loss = 0.21113061\n","Iteration 67, loss = 0.21085527\n","Iteration 68, loss = 0.21061969\n","Iteration 69, loss = 0.21036498\n","Iteration 70, loss = 0.21013358\n","Iteration 71, loss = 0.20991628\n","Iteration 72, loss = 0.20967264\n","Iteration 73, loss = 0.20944294\n","Iteration 74, loss = 0.20926028\n","Iteration 75, loss = 0.20897573\n","Iteration 76, loss = 0.20877374\n","Iteration 77, loss = 0.20854508\n","Iteration 78, loss = 0.20831917\n","Iteration 79, loss = 0.20814534\n","Iteration 80, loss = 0.20790119\n","Iteration 81, loss = 0.20768571\n","Iteration 82, loss = 0.20747739\n","Iteration 83, loss = 0.20728337\n","Iteration 84, loss = 0.20704609\n","Iteration 85, loss = 0.20685596\n","Iteration 86, loss = 0.20666698\n","Iteration 87, loss = 0.20646485\n","Iteration 88, loss = 0.20626024\n","Iteration 89, loss = 0.20606496\n","Iteration 90, loss = 0.20586739\n","Iteration 91, loss = 0.20564847\n","Iteration 92, loss = 0.20545815\n","Iteration 93, loss = 0.20529642\n","Iteration 94, loss = 0.20509072\n","Iteration 95, loss = 0.20492352\n","Iteration 96, loss = 0.20470736\n","Iteration 97, loss = 0.20450651\n","Iteration 98, loss = 0.20433185\n","Iteration 99, loss = 0.20414123\n","Iteration 100, loss = 0.20402379\n","Iteration 101, loss = 0.20379426\n","Iteration 102, loss = 0.20366185\n","Iteration 103, loss = 0.20344955\n","Iteration 104, loss = 0.20326550\n","Iteration 105, loss = 0.20306188\n","Iteration 106, loss = 0.20289769\n","Iteration 107, loss = 0.20274409\n","Iteration 108, loss = 0.20256167\n","Iteration 109, loss = 0.20239414\n","Iteration 110, loss = 0.20223425\n","Iteration 111, loss = 0.20208867\n","Iteration 112, loss = 0.20188568\n","Iteration 113, loss = 0.20173024\n","Iteration 114, loss = 0.20156800\n","Iteration 115, loss = 0.20139936\n","Iteration 116, loss = 0.20125571\n","Iteration 117, loss = 0.20109367\n","Iteration 118, loss = 0.20095736\n","Iteration 119, loss = 0.20080768\n","Iteration 120, loss = 0.20063071\n","Iteration 121, loss = 0.20050039\n","Iteration 122, loss = 0.20035407\n","Iteration 123, loss = 0.20019567\n","Iteration 124, loss = 0.20005905\n","Iteration 125, loss = 0.19988630\n","Iteration 126, loss = 0.19975107\n","Iteration 127, loss = 0.19959241\n","Iteration 128, loss = 0.19944161\n","Iteration 129, loss = 0.19932380\n","Iteration 130, loss = 0.19915115\n","Iteration 131, loss = 0.19899032\n","Iteration 132, loss = 0.19890770\n","Iteration 133, loss = 0.19871004\n","Iteration 134, loss = 0.19859340\n","Iteration 135, loss = 0.19854478\n","Iteration 136, loss = 0.19827964\n","Iteration 137, loss = 0.19814431\n","Iteration 138, loss = 0.19800687\n","Iteration 139, loss = 0.19789537\n","Iteration 140, loss = 0.19773994\n","Iteration 141, loss = 0.19760208\n","Iteration 142, loss = 0.19746646\n","Iteration 143, loss = 0.19732755\n","Iteration 144, loss = 0.19717772\n","Iteration 145, loss = 0.19703670\n","Iteration 146, loss = 0.19689456\n","Iteration 147, loss = 0.19677367\n","Iteration 148, loss = 0.19661293\n","Iteration 149, loss = 0.19648610\n","Iteration 150, loss = 0.19637430\n","Iteration 1, loss = 0.63261604\n","Iteration 2, loss = 0.53057854\n","Iteration 3, loss = 0.45873519\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.40387614\n","Iteration 5, loss = 0.36176157\n","Iteration 6, loss = 0.33019911\n","Iteration 7, loss = 0.30701683\n","Iteration 8, loss = 0.28955355\n","Iteration 9, loss = 0.27655751\n","Iteration 10, loss = 0.26662453\n","Iteration 11, loss = 0.25911979\n","Iteration 12, loss = 0.25315911\n","Iteration 13, loss = 0.24836741\n","Iteration 14, loss = 0.24441032\n","Iteration 15, loss = 0.24115540\n","Iteration 16, loss = 0.23836440\n","Iteration 17, loss = 0.23593456\n","Iteration 18, loss = 0.23379824\n","Iteration 19, loss = 0.23192393\n","Iteration 20, loss = 0.23025859\n","Iteration 21, loss = 0.22874593\n","Iteration 22, loss = 0.22739603\n","Iteration 23, loss = 0.22615601\n","Iteration 24, loss = 0.22501529\n","Iteration 25, loss = 0.22399795\n","Iteration 26, loss = 0.22304572\n","Iteration 27, loss = 0.22216667\n","Iteration 28, loss = 0.22134606\n","Iteration 29, loss = 0.22055816\n","Iteration 30, loss = 0.21985224\n","Iteration 31, loss = 0.21916842\n","Iteration 32, loss = 0.21855755\n","Iteration 33, loss = 0.21792583\n","Iteration 34, loss = 0.21733842\n","Iteration 35, loss = 0.21678951\n","Iteration 36, loss = 0.21627564\n","Iteration 37, loss = 0.21578575\n","Iteration 38, loss = 0.21531119\n","Iteration 39, loss = 0.21482857\n","Iteration 40, loss = 0.21441865\n","Iteration 41, loss = 0.21399370\n","Iteration 42, loss = 0.21357574\n","Iteration 43, loss = 0.21320011\n","Iteration 44, loss = 0.21280257\n","Iteration 45, loss = 0.21246247\n","Iteration 46, loss = 0.21211277\n","Iteration 47, loss = 0.21175568\n","Iteration 48, loss = 0.21144872\n","Iteration 49, loss = 0.21111047\n","Iteration 50, loss = 0.21083313\n","Iteration 51, loss = 0.21053653\n","Iteration 52, loss = 0.21025212\n","Iteration 53, loss = 0.20996439\n","Iteration 54, loss = 0.20970468\n","Iteration 55, loss = 0.20944162\n","Iteration 56, loss = 0.20915184\n","Iteration 57, loss = 0.20890444\n","Iteration 58, loss = 0.20866178\n","Iteration 59, loss = 0.20839695\n","Iteration 60, loss = 0.20816701\n","Iteration 61, loss = 0.20793182\n","Iteration 62, loss = 0.20766927\n","Iteration 63, loss = 0.20744955\n","Iteration 64, loss = 0.20721834\n","Iteration 65, loss = 0.20698902\n","Iteration 66, loss = 0.20677831\n","Iteration 67, loss = 0.20656234\n","Iteration 68, loss = 0.20633027\n","Iteration 69, loss = 0.20610382\n","Iteration 70, loss = 0.20593162\n","Iteration 71, loss = 0.20567604\n","Iteration 72, loss = 0.20547421\n","Iteration 73, loss = 0.20527407\n","Iteration 74, loss = 0.20507692\n","Iteration 75, loss = 0.20490265\n","Iteration 76, loss = 0.20468798\n","Iteration 77, loss = 0.20448065\n","Iteration 78, loss = 0.20431919\n","Iteration 79, loss = 0.20413051\n","Iteration 80, loss = 0.20393427\n","Iteration 81, loss = 0.20377086\n","Iteration 82, loss = 0.20359002\n","Iteration 83, loss = 0.20339225\n","Iteration 84, loss = 0.20325500\n","Iteration 85, loss = 0.20305657\n","Iteration 86, loss = 0.20289185\n","Iteration 87, loss = 0.20270087\n","Iteration 88, loss = 0.20253008\n","Iteration 89, loss = 0.20238160\n","Iteration 90, loss = 0.20220794\n","Iteration 91, loss = 0.20201839\n","Iteration 92, loss = 0.20187194\n","Iteration 93, loss = 0.20171721\n","Iteration 94, loss = 0.20152730\n","Iteration 95, loss = 0.20135931\n","Iteration 96, loss = 0.20120508\n","Iteration 97, loss = 0.20104924\n","Iteration 98, loss = 0.20089577\n","Iteration 99, loss = 0.20072115\n","Iteration 100, loss = 0.20056497\n","Iteration 101, loss = 0.20039347\n","Iteration 102, loss = 0.20027341\n","Iteration 103, loss = 0.20009011\n","Iteration 104, loss = 0.19993885\n","Iteration 105, loss = 0.19979124\n","Iteration 106, loss = 0.19965528\n","Iteration 107, loss = 0.19951064\n","Iteration 108, loss = 0.19936640\n","Iteration 109, loss = 0.19919774\n","Iteration 110, loss = 0.19905958\n","Iteration 111, loss = 0.19891974\n","Iteration 112, loss = 0.19876934\n","Iteration 113, loss = 0.19862209\n","Iteration 114, loss = 0.19847984\n","Iteration 115, loss = 0.19832497\n","Iteration 116, loss = 0.19819338\n","Iteration 117, loss = 0.19806082\n","Iteration 118, loss = 0.19791299\n","Iteration 119, loss = 0.19777789\n","Iteration 120, loss = 0.19761550\n","Iteration 121, loss = 0.19747732\n","Iteration 122, loss = 0.19730834\n","Iteration 123, loss = 0.19720038\n","Iteration 124, loss = 0.19701760\n","Iteration 125, loss = 0.19687423\n","Iteration 126, loss = 0.19676063\n","Iteration 127, loss = 0.19660624\n","Iteration 128, loss = 0.19643359\n","Iteration 129, loss = 0.19624691\n","Iteration 130, loss = 0.19614990\n","Iteration 131, loss = 0.19598730\n","Iteration 132, loss = 0.19583977\n","Iteration 133, loss = 0.19574005\n","Iteration 134, loss = 0.19555192\n","Iteration 135, loss = 0.19542220\n","Iteration 136, loss = 0.19526052\n","Iteration 137, loss = 0.19513108\n","Iteration 138, loss = 0.19498697\n","Iteration 139, loss = 0.19482962\n","Iteration 140, loss = 0.19468287\n","Iteration 141, loss = 0.19456207\n","Iteration 142, loss = 0.19439151\n","Iteration 143, loss = 0.19425816\n","Iteration 144, loss = 0.19412466\n","Iteration 145, loss = 0.19402355\n","Iteration 146, loss = 0.19386916\n","Iteration 147, loss = 0.19373650\n","Iteration 148, loss = 0.19357597\n","Iteration 149, loss = 0.19344081\n","Iteration 150, loss = 0.19333460\n","Iteration 1, loss = 0.56755654\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.30641629\n","Iteration 3, loss = 0.24408886\n","Iteration 4, loss = 0.22548740\n","Iteration 5, loss = 0.21728391\n","Iteration 6, loss = 0.21237205\n","Iteration 7, loss = 0.20809918\n","Iteration 8, loss = 0.20529347\n","Iteration 9, loss = 0.20244358\n","Iteration 10, loss = 0.19999430\n","Iteration 11, loss = 0.19800673\n","Iteration 12, loss = 0.19619798\n","Iteration 13, loss = 0.19469335\n","Iteration 14, loss = 0.19321216\n","Iteration 15, loss = 0.19142783\n","Iteration 16, loss = 0.19017431\n","Iteration 17, loss = 0.18823796\n","Iteration 18, loss = 0.18720782\n","Iteration 19, loss = 0.18537344\n","Iteration 20, loss = 0.18396013\n","Iteration 21, loss = 0.18251984\n","Iteration 22, loss = 0.18097374\n","Iteration 23, loss = 0.17932889\n","Iteration 24, loss = 0.17768013\n","Iteration 25, loss = 0.17628507\n","Iteration 26, loss = 0.17500340\n","Iteration 27, loss = 0.17359182\n","Iteration 28, loss = 0.17240981\n","Iteration 29, loss = 0.17146821\n","Iteration 30, loss = 0.17003276\n","Iteration 31, loss = 0.16835542\n","Iteration 32, loss = 0.16708297\n","Iteration 33, loss = 0.16568029\n","Iteration 34, loss = 0.16409280\n","Iteration 35, loss = 0.16315552\n","Iteration 36, loss = 0.16197170\n","Iteration 37, loss = 0.16103603\n","Iteration 38, loss = 0.15953472\n","Iteration 39, loss = 0.15798145\n","Iteration 40, loss = 0.15626683\n","Iteration 41, loss = 0.15542901\n","Iteration 42, loss = 0.15385160\n","Iteration 43, loss = 0.15248325\n","Iteration 44, loss = 0.15108790\n","Iteration 45, loss = 0.15040024\n","Iteration 46, loss = 0.14927769\n","Iteration 47, loss = 0.14777785\n","Iteration 48, loss = 0.14628788\n","Iteration 49, loss = 0.14553147\n","Iteration 50, loss = 0.14413283\n","Iteration 51, loss = 0.14238653\n","Iteration 52, loss = 0.14077642\n","Iteration 53, loss = 0.13955359\n","Iteration 54, loss = 0.13895176\n","Iteration 55, loss = 0.13710043\n","Iteration 56, loss = 0.13608529\n","Iteration 57, loss = 0.13436505\n","Iteration 58, loss = 0.13289815\n","Iteration 59, loss = 0.13175784\n","Iteration 60, loss = 0.13062016\n","Iteration 61, loss = 0.12898561\n","Iteration 62, loss = 0.12947434\n","Iteration 63, loss = 0.12599736\n","Iteration 64, loss = 0.12520486\n","Iteration 65, loss = 0.12472428\n","Iteration 66, loss = 0.12279346\n","Iteration 67, loss = 0.12236226\n","Iteration 68, loss = 0.12076777\n","Iteration 69, loss = 0.11992587\n","Iteration 70, loss = 0.11908848\n","Iteration 71, loss = 0.11704475\n","Iteration 72, loss = 0.11682482\n","Iteration 73, loss = 0.11548750\n","Iteration 74, loss = 0.11463928\n","Iteration 75, loss = 0.11280099\n","Iteration 76, loss = 0.11191138\n","Iteration 77, loss = 0.11092345\n","Iteration 78, loss = 0.11065514\n","Iteration 79, loss = 0.10857736\n","Iteration 80, loss = 0.10746666\n","Iteration 81, loss = 0.10635096\n","Iteration 82, loss = 0.10548171\n","Iteration 83, loss = 0.10451107\n","Iteration 84, loss = 0.10424415\n","Iteration 85, loss = 0.10312966\n","Iteration 86, loss = 0.10137739\n","Iteration 87, loss = 0.10027226\n","Iteration 88, loss = 0.09953156\n","Iteration 89, loss = 0.09845622\n","Iteration 90, loss = 0.09752046\n","Iteration 91, loss = 0.09671710\n","Iteration 92, loss = 0.09582645\n","Iteration 93, loss = 0.09570504\n","Iteration 94, loss = 0.09454666\n","Iteration 95, loss = 0.09294274\n","Iteration 96, loss = 0.09245487\n","Iteration 97, loss = 0.09109099\n","Iteration 98, loss = 0.09056030\n","Iteration 99, loss = 0.09009226\n","Iteration 100, loss = 0.08872565\n","Iteration 101, loss = 0.08814315\n","Iteration 102, loss = 0.08753479\n","Iteration 103, loss = 0.08674713\n","Iteration 104, loss = 0.08554634\n","Iteration 105, loss = 0.08511817\n","Iteration 106, loss = 0.08488391\n","Iteration 107, loss = 0.08438295\n","Iteration 108, loss = 0.08259999\n","Iteration 109, loss = 0.08207436\n","Iteration 110, loss = 0.08190581\n","Iteration 111, loss = 0.08083381\n","Iteration 112, loss = 0.07985655\n","Iteration 113, loss = 0.07910488\n","Iteration 114, loss = 0.07884424\n","Iteration 115, loss = 0.07815607\n","Iteration 116, loss = 0.07743232\n","Iteration 117, loss = 0.07643466\n","Iteration 118, loss = 0.07654649\n","Iteration 119, loss = 0.07567472\n","Iteration 120, loss = 0.07538379\n","Iteration 121, loss = 0.07449449\n","Iteration 122, loss = 0.07429455\n","Iteration 123, loss = 0.07284814\n","Iteration 124, loss = 0.07233340\n","Iteration 125, loss = 0.07224617\n","Iteration 126, loss = 0.07101704\n","Iteration 127, loss = 0.07094243\n","Iteration 128, loss = 0.06967021\n","Iteration 129, loss = 0.06950606\n","Iteration 130, loss = 0.06856834\n","Iteration 131, loss = 0.06770310\n","Iteration 132, loss = 0.06783580\n","Iteration 133, loss = 0.06705121\n","Iteration 134, loss = 0.06678698\n","Iteration 135, loss = 0.06581220\n","Iteration 136, loss = 0.06636300\n","Iteration 137, loss = 0.06532881\n","Iteration 138, loss = 0.06436759\n","Iteration 139, loss = 0.06477829\n","Iteration 140, loss = 0.06396604\n","Iteration 141, loss = 0.06350978\n","Iteration 142, loss = 0.06259246\n","Iteration 143, loss = 0.06187445\n","Iteration 144, loss = 0.06148065\n","Iteration 145, loss = 0.06046847\n","Iteration 146, loss = 0.06046078\n","Iteration 147, loss = 0.05970299\n","Iteration 148, loss = 0.05978760\n","Iteration 149, loss = 0.05907553\n","Iteration 150, loss = 0.05871820\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.61732203\n","Iteration 2, loss = 0.31886115\n","Iteration 3, loss = 0.25239057\n","Iteration 4, loss = 0.23055331\n","Iteration 5, loss = 0.22016408\n","Iteration 6, loss = 0.21413204\n","Iteration 7, loss = 0.21002372\n","Iteration 8, loss = 0.20678263\n","Iteration 9, loss = 0.20450558\n","Iteration 10, loss = 0.20207531\n","Iteration 11, loss = 0.20009536\n","Iteration 12, loss = 0.19834184\n","Iteration 13, loss = 0.19657684\n","Iteration 14, loss = 0.19511220\n","Iteration 15, loss = 0.19368764\n","Iteration 16, loss = 0.19202148\n","Iteration 17, loss = 0.19077707\n","Iteration 18, loss = 0.18963610\n","Iteration 19, loss = 0.18823876\n","Iteration 20, loss = 0.18684246\n","Iteration 21, loss = 0.18545480\n","Iteration 22, loss = 0.18455019\n","Iteration 23, loss = 0.18340824\n","Iteration 24, loss = 0.18139072\n","Iteration 25, loss = 0.18049962\n","Iteration 26, loss = 0.17911283\n","Iteration 27, loss = 0.17705072\n","Iteration 28, loss = 0.17654413\n","Iteration 29, loss = 0.17457461\n","Iteration 30, loss = 0.17327166\n","Iteration 31, loss = 0.17177838\n","Iteration 32, loss = 0.17065618\n","Iteration 33, loss = 0.16887209\n","Iteration 34, loss = 0.16739395\n","Iteration 35, loss = 0.16598474\n","Iteration 36, loss = 0.16505796\n","Iteration 37, loss = 0.16380226\n","Iteration 38, loss = 0.16221191\n","Iteration 39, loss = 0.16073197\n","Iteration 40, loss = 0.15961242\n","Iteration 41, loss = 0.15795829\n","Iteration 42, loss = 0.15646963\n","Iteration 43, loss = 0.15530424\n","Iteration 44, loss = 0.15343330\n","Iteration 45, loss = 0.15208400\n","Iteration 46, loss = 0.15089593\n","Iteration 47, loss = 0.14925803\n","Iteration 48, loss = 0.14866934\n","Iteration 49, loss = 0.14710446\n","Iteration 50, loss = 0.14565393\n","Iteration 51, loss = 0.14415295\n","Iteration 52, loss = 0.14271669\n","Iteration 53, loss = 0.14160526\n","Iteration 54, loss = 0.13994922\n","Iteration 55, loss = 0.13938699\n","Iteration 56, loss = 0.13803083\n","Iteration 57, loss = 0.13665924\n","Iteration 58, loss = 0.13555501\n","Iteration 59, loss = 0.13381079\n","Iteration 60, loss = 0.13250141\n","Iteration 61, loss = 0.13202462\n","Iteration 62, loss = 0.13049492\n","Iteration 63, loss = 0.12949880\n","Iteration 64, loss = 0.12783733\n","Iteration 65, loss = 0.12683709\n","Iteration 66, loss = 0.12585027\n","Iteration 67, loss = 0.12449538\n","Iteration 68, loss = 0.12327015\n","Iteration 69, loss = 0.12264819\n","Iteration 70, loss = 0.12168011\n","Iteration 71, loss = 0.12023310\n","Iteration 72, loss = 0.11939412\n","Iteration 73, loss = 0.11823288\n","Iteration 74, loss = 0.11698650\n","Iteration 75, loss = 0.11597378\n","Iteration 76, loss = 0.11549336\n","Iteration 77, loss = 0.11420271\n","Iteration 78, loss = 0.11274556\n","Iteration 79, loss = 0.11181113\n","Iteration 80, loss = 0.11126106\n","Iteration 81, loss = 0.10981374\n","Iteration 82, loss = 0.10851665\n","Iteration 83, loss = 0.10755319\n","Iteration 84, loss = 0.10623799\n","Iteration 85, loss = 0.10636330\n","Iteration 86, loss = 0.10548773\n","Iteration 87, loss = 0.10349902\n","Iteration 88, loss = 0.10269585\n","Iteration 89, loss = 0.10231124\n","Iteration 90, loss = 0.10218210\n","Iteration 91, loss = 0.10051659\n","Iteration 92, loss = 0.09960987\n","Iteration 93, loss = 0.09821898\n","Iteration 94, loss = 0.09796167\n","Iteration 95, loss = 0.09652419\n","Iteration 96, loss = 0.09612655\n","Iteration 97, loss = 0.09537321\n","Iteration 98, loss = 0.09476650\n","Iteration 99, loss = 0.09399346\n","Iteration 100, loss = 0.09271575\n","Iteration 101, loss = 0.09327791\n","Iteration 102, loss = 0.09190716\n","Iteration 103, loss = 0.09058214\n","Iteration 104, loss = 0.08928005\n","Iteration 105, loss = 0.08948797\n","Iteration 106, loss = 0.08795103\n","Iteration 107, loss = 0.08734380\n","Iteration 108, loss = 0.08671816\n","Iteration 109, loss = 0.08528341\n","Iteration 110, loss = 0.08498840\n","Iteration 111, loss = 0.08431044\n","Iteration 112, loss = 0.08438845\n","Iteration 113, loss = 0.08294025\n","Iteration 114, loss = 0.08300431\n","Iteration 115, loss = 0.08321414\n","Iteration 116, loss = 0.08090553\n","Iteration 117, loss = 0.08027628\n","Iteration 118, loss = 0.07982478\n","Iteration 119, loss = 0.07965412\n","Iteration 120, loss = 0.07876354\n","Iteration 121, loss = 0.07744573\n","Iteration 122, loss = 0.07775915\n","Iteration 123, loss = 0.07715332\n","Iteration 124, loss = 0.07614951\n","Iteration 125, loss = 0.07660832\n","Iteration 126, loss = 0.07515184\n","Iteration 127, loss = 0.07413646\n","Iteration 128, loss = 0.07385936\n","Iteration 129, loss = 0.07336992\n","Iteration 130, loss = 0.07281462\n","Iteration 131, loss = 0.07194552\n","Iteration 132, loss = 0.07118248\n","Iteration 133, loss = 0.07052833\n","Iteration 134, loss = 0.06960088\n","Iteration 135, loss = 0.06976257\n","Iteration 136, loss = 0.07020592\n","Iteration 137, loss = 0.06827982\n","Iteration 138, loss = 0.06800360\n","Iteration 139, loss = 0.06758067\n","Iteration 140, loss = 0.06774954\n","Iteration 141, loss = 0.06656802\n","Iteration 142, loss = 0.06590529\n","Iteration 143, loss = 0.06562143\n","Iteration 144, loss = 0.06437119\n","Iteration 145, loss = 0.06473149\n","Iteration 146, loss = 0.06322415\n","Iteration 147, loss = 0.06295827\n","Iteration 148, loss = 0.06317759\n","Iteration 149, loss = 0.06273125\n","Iteration 150, loss = 0.06187003\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.56412197\n","Iteration 2, loss = 0.27363094\n","Iteration 3, loss = 0.22360447\n","Iteration 4, loss = 0.21334464\n","Iteration 5, loss = 0.20837395\n","Iteration 6, loss = 0.20456772\n","Iteration 7, loss = 0.20139273\n","Iteration 8, loss = 0.19875103\n","Iteration 9, loss = 0.19623945\n","Iteration 10, loss = 0.19421572\n","Iteration 11, loss = 0.19210600\n","Iteration 12, loss = 0.19028136\n","Iteration 13, loss = 0.18827241\n","Iteration 14, loss = 0.18634630\n","Iteration 15, loss = 0.18480150\n","Iteration 16, loss = 0.18352549\n","Iteration 17, loss = 0.18146205\n","Iteration 18, loss = 0.18036523\n","Iteration 19, loss = 0.17841862\n","Iteration 20, loss = 0.17671301\n","Iteration 21, loss = 0.17562978\n","Iteration 22, loss = 0.17407391\n","Iteration 23, loss = 0.17242013\n","Iteration 24, loss = 0.17053504\n","Iteration 25, loss = 0.16929891\n","Iteration 26, loss = 0.16786678\n","Iteration 27, loss = 0.16592760\n","Iteration 28, loss = 0.16410563\n","Iteration 29, loss = 0.16248938\n","Iteration 30, loss = 0.16140771\n","Iteration 31, loss = 0.16006175\n","Iteration 32, loss = 0.15866858\n","Iteration 33, loss = 0.15757442\n","Iteration 34, loss = 0.15652542\n","Iteration 35, loss = 0.15471823\n","Iteration 36, loss = 0.15304648\n","Iteration 37, loss = 0.15183740\n","Iteration 38, loss = 0.15020581\n","Iteration 39, loss = 0.14875612\n","Iteration 40, loss = 0.14742088\n","Iteration 41, loss = 0.14646719\n","Iteration 42, loss = 0.14550648\n","Iteration 43, loss = 0.14374185\n","Iteration 44, loss = 0.14265115\n","Iteration 45, loss = 0.14170039\n","Iteration 46, loss = 0.14100531\n","Iteration 47, loss = 0.13851632\n","Iteration 48, loss = 0.13790073\n","Iteration 49, loss = 0.13631366\n","Iteration 50, loss = 0.13535819\n","Iteration 51, loss = 0.13443278\n","Iteration 52, loss = 0.13356506\n","Iteration 53, loss = 0.13203492\n","Iteration 54, loss = 0.13003192\n","Iteration 55, loss = 0.12937257\n","Iteration 56, loss = 0.12814140\n","Iteration 57, loss = 0.12703679\n","Iteration 58, loss = 0.12561193\n","Iteration 59, loss = 0.12485452\n","Iteration 60, loss = 0.12363769\n","Iteration 61, loss = 0.12279565\n","Iteration 62, loss = 0.12173523\n","Iteration 63, loss = 0.12083904\n","Iteration 64, loss = 0.11871526\n","Iteration 65, loss = 0.11871062\n","Iteration 66, loss = 0.11883314\n","Iteration 67, loss = 0.11625388\n","Iteration 68, loss = 0.11545700\n","Iteration 69, loss = 0.11417399\n","Iteration 70, loss = 0.11315268\n","Iteration 71, loss = 0.11210795\n","Iteration 72, loss = 0.11117028\n","Iteration 73, loss = 0.11010717\n","Iteration 74, loss = 0.10932522\n","Iteration 75, loss = 0.10842540\n","Iteration 76, loss = 0.10677488\n","Iteration 77, loss = 0.10624744\n","Iteration 78, loss = 0.10428284\n","Iteration 79, loss = 0.10449908\n","Iteration 80, loss = 0.10262005\n","Iteration 81, loss = 0.10223607\n","Iteration 82, loss = 0.10154979\n","Iteration 83, loss = 0.09940439\n","Iteration 84, loss = 0.09872356\n","Iteration 85, loss = 0.09842614\n","Iteration 86, loss = 0.09742433\n","Iteration 87, loss = 0.09566657\n","Iteration 88, loss = 0.09571723\n","Iteration 89, loss = 0.09494237\n","Iteration 90, loss = 0.09315860\n","Iteration 91, loss = 0.09310409\n","Iteration 92, loss = 0.09233217\n","Iteration 93, loss = 0.08990169\n","Iteration 94, loss = 0.09005041\n","Iteration 95, loss = 0.08965557\n","Iteration 96, loss = 0.08816285\n","Iteration 97, loss = 0.08743170\n","Iteration 98, loss = 0.08646920\n","Iteration 99, loss = 0.08494948\n","Iteration 100, loss = 0.08556627\n","Iteration 101, loss = 0.08359071\n","Iteration 102, loss = 0.08319063\n","Iteration 103, loss = 0.08197739\n","Iteration 104, loss = 0.08122337\n","Iteration 105, loss = 0.08030809\n","Iteration 106, loss = 0.08050056\n","Iteration 107, loss = 0.07908670\n","Iteration 108, loss = 0.07901808\n","Iteration 109, loss = 0.07785370\n","Iteration 110, loss = 0.07702852\n","Iteration 111, loss = 0.07696760\n","Iteration 112, loss = 0.07573835\n","Iteration 113, loss = 0.07438778\n","Iteration 114, loss = 0.07320393\n","Iteration 115, loss = 0.07224423\n","Iteration 116, loss = 0.07264962\n","Iteration 117, loss = 0.07172988\n","Iteration 118, loss = 0.07062657\n","Iteration 119, loss = 0.07072039\n","Iteration 120, loss = 0.06948705\n","Iteration 121, loss = 0.06926350\n","Iteration 122, loss = 0.07019562\n","Iteration 123, loss = 0.06880021\n","Iteration 124, loss = 0.06706122\n","Iteration 125, loss = 0.06665316\n","Iteration 126, loss = 0.06716626\n","Iteration 127, loss = 0.06546849\n","Iteration 128, loss = 0.06476118\n","Iteration 129, loss = 0.06400431\n","Iteration 130, loss = 0.06498043\n","Iteration 131, loss = 0.06324808\n","Iteration 132, loss = 0.06313850\n","Iteration 133, loss = 0.06186761\n","Iteration 134, loss = 0.06139889\n","Iteration 135, loss = 0.06080988\n","Iteration 136, loss = 0.06032771\n","Iteration 137, loss = 0.05924201\n","Iteration 138, loss = 0.05906161\n","Iteration 139, loss = 0.05868256\n","Iteration 140, loss = 0.05835770\n","Iteration 141, loss = 0.05733935\n","Iteration 142, loss = 0.05671959\n","Iteration 143, loss = 0.05641051\n","Iteration 144, loss = 0.05522785\n","Iteration 145, loss = 0.05644788\n","Iteration 146, loss = 0.05532693\n","Iteration 147, loss = 0.05451768\n","Iteration 148, loss = 0.05370019\n","Iteration 149, loss = 0.05285011\n","Iteration 150, loss = 0.05213913\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.51582423\n","Iteration 2, loss = 0.27844006\n","Iteration 3, loss = 0.23535490\n","Iteration 4, loss = 0.22237726\n","Iteration 5, loss = 0.21590199\n","Iteration 6, loss = 0.21158319\n","Iteration 7, loss = 0.20856838\n","Iteration 8, loss = 0.20562156\n","Iteration 9, loss = 0.20297563\n","Iteration 10, loss = 0.20048279\n","Iteration 11, loss = 0.19856002\n","Iteration 12, loss = 0.19604759\n","Iteration 13, loss = 0.19418505\n","Iteration 14, loss = 0.19220593\n","Iteration 15, loss = 0.18989380\n","Iteration 16, loss = 0.18800553\n","Iteration 17, loss = 0.18627210\n","Iteration 18, loss = 0.18450926\n","Iteration 19, loss = 0.18303563\n","Iteration 20, loss = 0.18103731\n","Iteration 21, loss = 0.18033320\n","Iteration 22, loss = 0.17834823\n","Iteration 23, loss = 0.17663522\n","Iteration 24, loss = 0.17534603\n","Iteration 25, loss = 0.17442186\n","Iteration 26, loss = 0.17263092\n","Iteration 27, loss = 0.17084628\n","Iteration 28, loss = 0.16957377\n","Iteration 29, loss = 0.16819810\n","Iteration 30, loss = 0.16660735\n","Iteration 31, loss = 0.16588137\n","Iteration 32, loss = 0.16437182\n","Iteration 33, loss = 0.16315657\n","Iteration 34, loss = 0.16152095\n","Iteration 35, loss = 0.16029171\n","Iteration 36, loss = 0.15928117\n","Iteration 37, loss = 0.15778560\n","Iteration 38, loss = 0.15686293\n","Iteration 39, loss = 0.15527589\n","Iteration 40, loss = 0.15434123\n","Iteration 41, loss = 0.15365916\n","Iteration 42, loss = 0.15282327\n","Iteration 43, loss = 0.15075872\n","Iteration 44, loss = 0.15000873\n","Iteration 45, loss = 0.14895446\n","Iteration 46, loss = 0.14744564\n","Iteration 47, loss = 0.14581183\n","Iteration 48, loss = 0.14526212\n","Iteration 49, loss = 0.14497428\n","Iteration 50, loss = 0.14301387\n","Iteration 51, loss = 0.14217272\n","Iteration 52, loss = 0.14114747\n","Iteration 53, loss = 0.14019462\n","Iteration 54, loss = 0.13823962\n","Iteration 55, loss = 0.13740366\n","Iteration 56, loss = 0.13769769\n","Iteration 57, loss = 0.13508618\n","Iteration 58, loss = 0.13408928\n","Iteration 59, loss = 0.13290555\n","Iteration 60, loss = 0.13218294\n","Iteration 61, loss = 0.13154231\n","Iteration 62, loss = 0.12959678\n","Iteration 63, loss = 0.12930892\n","Iteration 64, loss = 0.12818876\n","Iteration 65, loss = 0.12686672\n","Iteration 66, loss = 0.12605253\n","Iteration 67, loss = 0.12531491\n","Iteration 68, loss = 0.12393995\n","Iteration 69, loss = 0.12271699\n","Iteration 70, loss = 0.12195659\n","Iteration 71, loss = 0.12021027\n","Iteration 72, loss = 0.12082279\n","Iteration 73, loss = 0.11983295\n","Iteration 74, loss = 0.11824920\n","Iteration 75, loss = 0.11673315\n","Iteration 76, loss = 0.11570831\n","Iteration 77, loss = 0.11712546\n","Iteration 78, loss = 0.11472125\n","Iteration 79, loss = 0.11380445\n","Iteration 80, loss = 0.11217029\n","Iteration 81, loss = 0.11299609\n","Iteration 82, loss = 0.11099377\n","Iteration 83, loss = 0.11008501\n","Iteration 84, loss = 0.10912768\n","Iteration 85, loss = 0.10893943\n","Iteration 86, loss = 0.10793673\n","Iteration 87, loss = 0.10677825\n","Iteration 88, loss = 0.10711302\n","Iteration 89, loss = 0.10661364\n","Iteration 90, loss = 0.10437710\n","Iteration 91, loss = 0.10396365\n","Iteration 92, loss = 0.10271002\n","Iteration 93, loss = 0.10251188\n","Iteration 94, loss = 0.10227920\n","Iteration 95, loss = 0.10248114\n","Iteration 96, loss = 0.10096115\n","Iteration 97, loss = 0.09953879\n","Iteration 98, loss = 0.09929791\n","Iteration 99, loss = 0.09821016\n","Iteration 100, loss = 0.09740997\n","Iteration 101, loss = 0.09757146\n","Iteration 102, loss = 0.09643207\n","Iteration 103, loss = 0.09580778\n","Iteration 104, loss = 0.09528137\n","Iteration 105, loss = 0.09578693\n","Iteration 106, loss = 0.09325386\n","Iteration 107, loss = 0.09279409\n","Iteration 108, loss = 0.09276759\n","Iteration 109, loss = 0.09207446\n","Iteration 110, loss = 0.09127102\n","Iteration 111, loss = 0.09027719\n","Iteration 112, loss = 0.09018284\n","Iteration 113, loss = 0.08970966\n","Iteration 114, loss = 0.08894389\n","Iteration 115, loss = 0.08766846\n","Iteration 116, loss = 0.08726157\n","Iteration 117, loss = 0.08621799\n","Iteration 118, loss = 0.08630967\n","Iteration 119, loss = 0.08517247\n","Iteration 120, loss = 0.08465418\n","Iteration 121, loss = 0.08410329\n","Iteration 122, loss = 0.08445425\n","Iteration 123, loss = 0.08250429\n","Iteration 124, loss = 0.08209181\n","Iteration 125, loss = 0.08233991\n","Iteration 126, loss = 0.08108536\n","Iteration 127, loss = 0.08093439\n","Iteration 128, loss = 0.08025066\n","Iteration 129, loss = 0.07893333\n","Iteration 130, loss = 0.07874165\n","Iteration 131, loss = 0.07853172\n","Iteration 132, loss = 0.07847674\n","Iteration 133, loss = 0.07747753\n","Iteration 134, loss = 0.07638093\n","Iteration 135, loss = 0.07677496\n","Iteration 136, loss = 0.07547409\n","Iteration 137, loss = 0.07550546\n","Iteration 138, loss = 0.07433639\n","Iteration 139, loss = 0.07387872\n","Iteration 140, loss = 0.07442953\n","Iteration 141, loss = 0.07409998\n","Iteration 142, loss = 0.07234397\n","Iteration 143, loss = 0.07222182\n","Iteration 144, loss = 0.07116315\n","Iteration 145, loss = 0.07056361\n","Iteration 146, loss = 0.07033331\n","Iteration 147, loss = 0.06980723\n","Iteration 148, loss = 0.06879927\n","Iteration 149, loss = 0.06937894\n","Iteration 150, loss = 0.06886588\n","Iteration 1, loss = 0.50808323\n","Iteration 2, loss = 0.28342220\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.23710021\n","Iteration 4, loss = 0.22135373\n","Iteration 5, loss = 0.21344412\n","Iteration 6, loss = 0.20817476\n","Iteration 7, loss = 0.20409104\n","Iteration 8, loss = 0.20065728\n","Iteration 9, loss = 0.19825086\n","Iteration 10, loss = 0.19542783\n","Iteration 11, loss = 0.19312909\n","Iteration 12, loss = 0.19111981\n","Iteration 13, loss = 0.18890197\n","Iteration 14, loss = 0.18729202\n","Iteration 15, loss = 0.18547640\n","Iteration 16, loss = 0.18360596\n","Iteration 17, loss = 0.18205937\n","Iteration 18, loss = 0.18069635\n","Iteration 19, loss = 0.17902756\n","Iteration 20, loss = 0.17721090\n","Iteration 21, loss = 0.17577326\n","Iteration 22, loss = 0.17377124\n","Iteration 23, loss = 0.17348814\n","Iteration 24, loss = 0.17104618\n","Iteration 25, loss = 0.17017818\n","Iteration 26, loss = 0.16817304\n","Iteration 27, loss = 0.16708536\n","Iteration 28, loss = 0.16522048\n","Iteration 29, loss = 0.16330599\n","Iteration 30, loss = 0.16247273\n","Iteration 31, loss = 0.16059983\n","Iteration 32, loss = 0.15921344\n","Iteration 33, loss = 0.15755470\n","Iteration 34, loss = 0.15629677\n","Iteration 35, loss = 0.15417986\n","Iteration 36, loss = 0.15307656\n","Iteration 37, loss = 0.15113367\n","Iteration 38, loss = 0.14963514\n","Iteration 39, loss = 0.14837441\n","Iteration 40, loss = 0.14670987\n","Iteration 41, loss = 0.14545251\n","Iteration 42, loss = 0.14429138\n","Iteration 43, loss = 0.14268241\n","Iteration 44, loss = 0.14129299\n","Iteration 45, loss = 0.13953868\n","Iteration 46, loss = 0.13751463\n","Iteration 47, loss = 0.13627493\n","Iteration 48, loss = 0.13580684\n","Iteration 49, loss = 0.13420083\n","Iteration 50, loss = 0.13321474\n","Iteration 51, loss = 0.13024419\n","Iteration 52, loss = 0.12916494\n","Iteration 53, loss = 0.12770211\n","Iteration 54, loss = 0.12597679\n","Iteration 55, loss = 0.12577977\n","Iteration 56, loss = 0.12449532\n","Iteration 57, loss = 0.12262190\n","Iteration 58, loss = 0.12080869\n","Iteration 59, loss = 0.11976987\n","Iteration 60, loss = 0.11916174\n","Iteration 61, loss = 0.11681231\n","Iteration 62, loss = 0.11582693\n","Iteration 63, loss = 0.11414454\n","Iteration 64, loss = 0.11301683\n","Iteration 65, loss = 0.11218128\n","Iteration 66, loss = 0.11097849\n","Iteration 67, loss = 0.10959816\n","Iteration 68, loss = 0.10766550\n","Iteration 69, loss = 0.10692703\n","Iteration 70, loss = 0.10580668\n","Iteration 71, loss = 0.10475156\n","Iteration 72, loss = 0.10418853\n","Iteration 73, loss = 0.10237021\n","Iteration 74, loss = 0.10129229\n","Iteration 75, loss = 0.10033711\n","Iteration 76, loss = 0.09914888\n","Iteration 77, loss = 0.09835173\n","Iteration 78, loss = 0.09657822\n","Iteration 79, loss = 0.09656269\n","Iteration 80, loss = 0.09536625\n","Iteration 81, loss = 0.09487041\n","Iteration 82, loss = 0.09317948\n","Iteration 83, loss = 0.09324936\n","Iteration 84, loss = 0.09165167\n","Iteration 85, loss = 0.09047702\n","Iteration 86, loss = 0.08990877\n","Iteration 87, loss = 0.08851174\n","Iteration 88, loss = 0.08816761\n","Iteration 89, loss = 0.08674865\n","Iteration 90, loss = 0.08659296\n","Iteration 91, loss = 0.08512524\n","Iteration 92, loss = 0.08430364\n","Iteration 93, loss = 0.08288766\n","Iteration 94, loss = 0.08228347\n","Iteration 95, loss = 0.08213155\n","Iteration 96, loss = 0.08190483\n","Iteration 97, loss = 0.08050320\n","Iteration 98, loss = 0.08075689\n","Iteration 99, loss = 0.07836167\n","Iteration 100, loss = 0.07941238\n","Iteration 101, loss = 0.07715759\n","Iteration 102, loss = 0.07585021\n","Iteration 103, loss = 0.07513683\n","Iteration 104, loss = 0.07463297\n","Iteration 105, loss = 0.07447215\n","Iteration 106, loss = 0.07384493\n","Iteration 107, loss = 0.07287266\n","Iteration 108, loss = 0.07226467\n","Iteration 109, loss = 0.07173750\n","Iteration 110, loss = 0.07117684\n","Iteration 111, loss = 0.06959751\n","Iteration 112, loss = 0.06936953\n","Iteration 113, loss = 0.06796442\n","Iteration 114, loss = 0.06777581\n","Iteration 115, loss = 0.06719336\n","Iteration 116, loss = 0.06586465\n","Iteration 117, loss = 0.06571400\n","Iteration 118, loss = 0.06671132\n","Iteration 119, loss = 0.06401787\n","Iteration 120, loss = 0.06488935\n","Iteration 121, loss = 0.06420739\n","Iteration 122, loss = 0.06264007\n","Iteration 123, loss = 0.06255482\n","Iteration 124, loss = 0.06279756\n","Iteration 125, loss = 0.06146248\n","Iteration 126, loss = 0.06146705\n","Iteration 127, loss = 0.06055503\n","Iteration 128, loss = 0.05911714\n","Iteration 129, loss = 0.05947458\n","Iteration 130, loss = 0.05781495\n","Iteration 131, loss = 0.05899306\n","Iteration 132, loss = 0.05678302\n","Iteration 133, loss = 0.05719641\n","Iteration 134, loss = 0.05652184\n","Iteration 135, loss = 0.05527847\n","Iteration 136, loss = 0.05510473\n","Iteration 137, loss = 0.05445851\n","Iteration 138, loss = 0.05418390\n","Iteration 139, loss = 0.05386908\n","Iteration 140, loss = 0.05263183\n","Iteration 141, loss = 0.05236999\n","Iteration 142, loss = 0.05202146\n","Iteration 143, loss = 0.05189444\n","Iteration 144, loss = 0.05069256\n","Iteration 145, loss = 0.04967489\n","Iteration 146, loss = 0.05057427\n","Iteration 147, loss = 0.04956036\n","Iteration 148, loss = 0.04935961\n","Iteration 149, loss = 0.04913675\n","Iteration 150, loss = 0.04831150\n","Iteration 1, loss = 0.61749265\n","Iteration 2, loss = 0.49749693\n","Iteration 3, loss = 0.43725672\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.39742125\n","Iteration 5, loss = 0.36630817\n","Iteration 6, loss = 0.34065733\n","Iteration 7, loss = 0.31935753\n","Iteration 8, loss = 0.30186321\n","Iteration 9, loss = 0.28788670\n","Iteration 10, loss = 0.27668923\n","Iteration 11, loss = 0.26786540\n","Iteration 12, loss = 0.26077691\n","Iteration 13, loss = 0.25503158\n","Iteration 14, loss = 0.25027260\n","Iteration 15, loss = 0.24631359\n","Iteration 16, loss = 0.24298193\n","Iteration 17, loss = 0.24023531\n","Iteration 18, loss = 0.23768618\n","Iteration 19, loss = 0.23555358\n","Iteration 20, loss = 0.23362244\n","Iteration 21, loss = 0.23192677\n","Iteration 22, loss = 0.23036797\n","Iteration 23, loss = 0.22897686\n","Iteration 24, loss = 0.22772105\n","Iteration 25, loss = 0.22657970\n","Iteration 26, loss = 0.22548722\n","Iteration 27, loss = 0.22452737\n","Iteration 28, loss = 0.22357739\n","Iteration 29, loss = 0.22273978\n","Iteration 30, loss = 0.22195074\n","Iteration 31, loss = 0.22117554\n","Iteration 32, loss = 0.22047431\n","Iteration 33, loss = 0.21975814\n","Iteration 34, loss = 0.21914016\n","Iteration 35, loss = 0.21847917\n","Iteration 36, loss = 0.21791378\n","Iteration 37, loss = 0.21729807\n","Iteration 38, loss = 0.21675579\n","Iteration 39, loss = 0.21625057\n","Iteration 40, loss = 0.21572941\n","Iteration 41, loss = 0.21523869\n","Iteration 42, loss = 0.21480996\n","Iteration 43, loss = 0.21427557\n","Iteration 44, loss = 0.21384000\n","Iteration 45, loss = 0.21340065\n","Iteration 46, loss = 0.21302969\n","Iteration 47, loss = 0.21261050\n","Iteration 48, loss = 0.21221635\n","Iteration 49, loss = 0.21182046\n","Iteration 50, loss = 0.21140761\n","Iteration 51, loss = 0.21106500\n","Iteration 52, loss = 0.21070584\n","Iteration 53, loss = 0.21039303\n","Iteration 54, loss = 0.20997048\n","Iteration 55, loss = 0.20962299\n","Iteration 56, loss = 0.20930996\n","Iteration 57, loss = 0.20892998\n","Iteration 58, loss = 0.20863736\n","Iteration 59, loss = 0.20827663\n","Iteration 60, loss = 0.20797029\n","Iteration 61, loss = 0.20768655\n","Iteration 62, loss = 0.20737333\n","Iteration 63, loss = 0.20707949\n","Iteration 64, loss = 0.20674042\n","Iteration 65, loss = 0.20643215\n","Iteration 66, loss = 0.20619198\n","Iteration 67, loss = 0.20588694\n","Iteration 68, loss = 0.20558742\n","Iteration 69, loss = 0.20534255\n","Iteration 70, loss = 0.20507716\n","Iteration 71, loss = 0.20478827\n","Iteration 72, loss = 0.20452132\n","Iteration 73, loss = 0.20426744\n","Iteration 74, loss = 0.20401984\n","Iteration 75, loss = 0.20378719\n","Iteration 76, loss = 0.20351002\n","Iteration 77, loss = 0.20323290\n","Iteration 78, loss = 0.20298787\n","Iteration 79, loss = 0.20273843\n","Iteration 80, loss = 0.20251638\n","Iteration 81, loss = 0.20227204\n","Iteration 82, loss = 0.20203496\n","Iteration 83, loss = 0.20179378\n","Iteration 84, loss = 0.20157772\n","Iteration 85, loss = 0.20139643\n","Iteration 86, loss = 0.20105182\n","Iteration 87, loss = 0.20088407\n","Iteration 88, loss = 0.20062823\n","Iteration 89, loss = 0.20039327\n","Iteration 90, loss = 0.20020818\n","Iteration 91, loss = 0.19997405\n","Iteration 92, loss = 0.19975979\n","Iteration 93, loss = 0.19957812\n","Iteration 94, loss = 0.19934477\n","Iteration 95, loss = 0.19913294\n","Iteration 96, loss = 0.19888518\n","Iteration 97, loss = 0.19870448\n","Iteration 98, loss = 0.19849013\n","Iteration 99, loss = 0.19828378\n","Iteration 100, loss = 0.19805629\n","Iteration 101, loss = 0.19783681\n","Iteration 102, loss = 0.19767018\n","Iteration 103, loss = 0.19749296\n","Iteration 104, loss = 0.19728604\n","Iteration 105, loss = 0.19704481\n","Iteration 106, loss = 0.19685034\n","Iteration 107, loss = 0.19668607\n","Iteration 108, loss = 0.19647008\n","Iteration 109, loss = 0.19627053\n","Iteration 110, loss = 0.19606457\n","Iteration 111, loss = 0.19586615\n","Iteration 112, loss = 0.19569471\n","Iteration 113, loss = 0.19548439\n","Iteration 114, loss = 0.19530891\n","Iteration 115, loss = 0.19518478\n","Iteration 116, loss = 0.19492891\n","Iteration 117, loss = 0.19475971\n","Iteration 118, loss = 0.19459996\n","Iteration 119, loss = 0.19442359\n","Iteration 120, loss = 0.19421263\n","Iteration 121, loss = 0.19400053\n","Iteration 122, loss = 0.19383261\n","Iteration 123, loss = 0.19365964\n","Iteration 124, loss = 0.19343536\n","Iteration 125, loss = 0.19329517\n","Iteration 126, loss = 0.19308092\n","Iteration 127, loss = 0.19285360\n","Iteration 128, loss = 0.19271765\n","Iteration 129, loss = 0.19251661\n","Iteration 130, loss = 0.19230194\n","Iteration 131, loss = 0.19213638\n","Iteration 132, loss = 0.19193999\n","Iteration 133, loss = 0.19170838\n","Iteration 134, loss = 0.19161736\n","Iteration 135, loss = 0.19137960\n","Iteration 136, loss = 0.19122669\n","Iteration 137, loss = 0.19110208\n","Iteration 138, loss = 0.19082174\n","Iteration 139, loss = 0.19062996\n","Iteration 140, loss = 0.19049325\n","Iteration 141, loss = 0.19027990\n","Iteration 142, loss = 0.19015717\n","Iteration 143, loss = 0.18994300\n","Iteration 144, loss = 0.18978675\n","Iteration 145, loss = 0.18957458\n","Iteration 146, loss = 0.18942607\n","Iteration 147, loss = 0.18924276\n","Iteration 148, loss = 0.18912162\n","Iteration 149, loss = 0.18889616\n","Iteration 150, loss = 0.18872507\n","Iteration 1, loss = 0.74875473\n","Iteration 2, loss = 0.59923515\n","Iteration 3, loss = 0.49104219\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.41354247\n","Iteration 5, loss = 0.35905832\n","Iteration 6, loss = 0.32197614\n","Iteration 7, loss = 0.29694317\n","Iteration 8, loss = 0.27972332\n","Iteration 9, loss = 0.26755209\n","Iteration 10, loss = 0.25854194\n","Iteration 11, loss = 0.25171965\n","Iteration 12, loss = 0.24645808\n","Iteration 13, loss = 0.24216753\n","Iteration 14, loss = 0.23873679\n","Iteration 15, loss = 0.23580855\n","Iteration 16, loss = 0.23335793\n","Iteration 17, loss = 0.23121299\n","Iteration 18, loss = 0.22934355\n","Iteration 19, loss = 0.22771456\n","Iteration 20, loss = 0.22624715\n","Iteration 21, loss = 0.22497688\n","Iteration 22, loss = 0.22385291\n","Iteration 23, loss = 0.22284086\n","Iteration 24, loss = 0.22190580\n","Iteration 25, loss = 0.22102929\n","Iteration 26, loss = 0.22020015\n","Iteration 27, loss = 0.21950794\n","Iteration 28, loss = 0.21877675\n","Iteration 29, loss = 0.21816059\n","Iteration 30, loss = 0.21754592\n","Iteration 31, loss = 0.21694974\n","Iteration 32, loss = 0.21643709\n","Iteration 33, loss = 0.21593652\n","Iteration 34, loss = 0.21544117\n","Iteration 35, loss = 0.21502256\n","Iteration 36, loss = 0.21458756\n","Iteration 37, loss = 0.21412261\n","Iteration 38, loss = 0.21367669\n","Iteration 39, loss = 0.21327355\n","Iteration 40, loss = 0.21289741\n","Iteration 41, loss = 0.21250062\n","Iteration 42, loss = 0.21210855\n","Iteration 43, loss = 0.21175900\n","Iteration 44, loss = 0.21140161\n","Iteration 45, loss = 0.21103649\n","Iteration 46, loss = 0.21070772\n","Iteration 47, loss = 0.21038225\n","Iteration 48, loss = 0.21003161\n","Iteration 49, loss = 0.20969201\n","Iteration 50, loss = 0.20945126\n","Iteration 51, loss = 0.20905671\n","Iteration 52, loss = 0.20876463\n","Iteration 53, loss = 0.20850607\n","Iteration 54, loss = 0.20818321\n","Iteration 55, loss = 0.20790395\n","Iteration 56, loss = 0.20754248\n","Iteration 57, loss = 0.20731868\n","Iteration 58, loss = 0.20707670\n","Iteration 59, loss = 0.20680432\n","Iteration 60, loss = 0.20647707\n","Iteration 61, loss = 0.20623044\n","Iteration 62, loss = 0.20595030\n","Iteration 63, loss = 0.20569138\n","Iteration 64, loss = 0.20549116\n","Iteration 65, loss = 0.20523882\n","Iteration 66, loss = 0.20491435\n","Iteration 67, loss = 0.20470352\n","Iteration 68, loss = 0.20440803\n","Iteration 69, loss = 0.20420360\n","Iteration 70, loss = 0.20394213\n","Iteration 71, loss = 0.20370648\n","Iteration 72, loss = 0.20347945\n","Iteration 73, loss = 0.20328479\n","Iteration 74, loss = 0.20297458\n","Iteration 75, loss = 0.20278927\n","Iteration 76, loss = 0.20254382\n","Iteration 77, loss = 0.20232735\n","Iteration 78, loss = 0.20207957\n","Iteration 79, loss = 0.20186079\n","Iteration 80, loss = 0.20165048\n","Iteration 81, loss = 0.20140864\n","Iteration 82, loss = 0.20117985\n","Iteration 83, loss = 0.20094926\n","Iteration 84, loss = 0.20074254\n","Iteration 85, loss = 0.20054709\n","Iteration 86, loss = 0.20033629\n","Iteration 87, loss = 0.20013143\n","Iteration 88, loss = 0.19988091\n","Iteration 89, loss = 0.19966075\n","Iteration 90, loss = 0.19950120\n","Iteration 91, loss = 0.19924893\n","Iteration 92, loss = 0.19905822\n","Iteration 93, loss = 0.19884842\n","Iteration 94, loss = 0.19864930\n","Iteration 95, loss = 0.19846308\n","Iteration 96, loss = 0.19826111\n","Iteration 97, loss = 0.19804950\n","Iteration 98, loss = 0.19785048\n","Iteration 99, loss = 0.19766826\n","Iteration 100, loss = 0.19746311\n","Iteration 101, loss = 0.19727414\n","Iteration 102, loss = 0.19708800\n","Iteration 103, loss = 0.19688432\n","Iteration 104, loss = 0.19667904\n","Iteration 105, loss = 0.19649525\n","Iteration 106, loss = 0.19630715\n","Iteration 107, loss = 0.19613755\n","Iteration 108, loss = 0.19588497\n","Iteration 109, loss = 0.19567107\n","Iteration 110, loss = 0.19547334\n","Iteration 111, loss = 0.19532813\n","Iteration 112, loss = 0.19510792\n","Iteration 113, loss = 0.19488236\n","Iteration 114, loss = 0.19469838\n","Iteration 115, loss = 0.19455023\n","Iteration 116, loss = 0.19432987\n","Iteration 117, loss = 0.19417769\n","Iteration 118, loss = 0.19391987\n","Iteration 119, loss = 0.19378247\n","Iteration 120, loss = 0.19358383\n","Iteration 121, loss = 0.19337093\n","Iteration 122, loss = 0.19320596\n","Iteration 123, loss = 0.19301748\n","Iteration 124, loss = 0.19286777\n","Iteration 125, loss = 0.19265422\n","Iteration 126, loss = 0.19242855\n","Iteration 127, loss = 0.19230535\n","Iteration 128, loss = 0.19209459\n","Iteration 129, loss = 0.19187838\n","Iteration 130, loss = 0.19170799\n","Iteration 131, loss = 0.19151743\n","Iteration 132, loss = 0.19134860\n","Iteration 133, loss = 0.19116161\n","Iteration 134, loss = 0.19101005\n","Iteration 135, loss = 0.19077770\n","Iteration 136, loss = 0.19058518\n","Iteration 137, loss = 0.19041057\n","Iteration 138, loss = 0.19024244\n","Iteration 139, loss = 0.19007501\n","Iteration 140, loss = 0.18988544\n","Iteration 141, loss = 0.18975578\n","Iteration 142, loss = 0.18953151\n","Iteration 143, loss = 0.18939301\n","Iteration 144, loss = 0.18921216\n","Iteration 145, loss = 0.18905365\n","Iteration 146, loss = 0.18889407\n","Iteration 147, loss = 0.18865619\n","Iteration 148, loss = 0.18849643\n","Iteration 149, loss = 0.18831874\n","Iteration 150, loss = 0.18815078\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.74382464\n","Iteration 2, loss = 0.47158545\n","Iteration 3, loss = 0.37036942\n","Iteration 4, loss = 0.32032126\n","Iteration 5, loss = 0.29191176\n","Iteration 6, loss = 0.27445243\n","Iteration 7, loss = 0.26269553\n","Iteration 8, loss = 0.25457486\n","Iteration 9, loss = 0.24838822\n","Iteration 10, loss = 0.24365598\n","Iteration 11, loss = 0.23987154\n","Iteration 12, loss = 0.23665107\n","Iteration 13, loss = 0.23394570\n","Iteration 14, loss = 0.23164653\n","Iteration 15, loss = 0.22968407\n","Iteration 16, loss = 0.22795811\n","Iteration 17, loss = 0.22637484\n","Iteration 18, loss = 0.22499306\n","Iteration 19, loss = 0.22373497\n","Iteration 20, loss = 0.22256584\n","Iteration 21, loss = 0.22150667\n","Iteration 22, loss = 0.22046795\n","Iteration 23, loss = 0.21955770\n","Iteration 24, loss = 0.21877889\n","Iteration 25, loss = 0.21790417\n","Iteration 26, loss = 0.21711239\n","Iteration 27, loss = 0.21640175\n","Iteration 28, loss = 0.21570908\n","Iteration 29, loss = 0.21502276\n","Iteration 30, loss = 0.21443944\n","Iteration 31, loss = 0.21382795\n","Iteration 32, loss = 0.21325292\n","Iteration 33, loss = 0.21269151\n","Iteration 34, loss = 0.21216338\n","Iteration 35, loss = 0.21168717\n","Iteration 36, loss = 0.21119365\n","Iteration 37, loss = 0.21071629\n","Iteration 38, loss = 0.21022079\n","Iteration 39, loss = 0.20976249\n","Iteration 40, loss = 0.20933288\n","Iteration 41, loss = 0.20891525\n","Iteration 42, loss = 0.20848795\n","Iteration 43, loss = 0.20811655\n","Iteration 44, loss = 0.20771416\n","Iteration 45, loss = 0.20732256\n","Iteration 46, loss = 0.20695115\n","Iteration 47, loss = 0.20661473\n","Iteration 48, loss = 0.20622660\n","Iteration 49, loss = 0.20583642\n","Iteration 50, loss = 0.20548365\n","Iteration 51, loss = 0.20513205\n","Iteration 52, loss = 0.20481192\n","Iteration 53, loss = 0.20451700\n","Iteration 54, loss = 0.20419315\n","Iteration 55, loss = 0.20386310\n","Iteration 56, loss = 0.20358684\n","Iteration 57, loss = 0.20324713\n","Iteration 58, loss = 0.20297986\n","Iteration 59, loss = 0.20274342\n","Iteration 60, loss = 0.20240065\n","Iteration 61, loss = 0.20217907\n","Iteration 62, loss = 0.20187231\n","Iteration 63, loss = 0.20154626\n","Iteration 64, loss = 0.20136208\n","Iteration 65, loss = 0.20100223\n","Iteration 66, loss = 0.20075510\n","Iteration 67, loss = 0.20050894\n","Iteration 68, loss = 0.20023462\n","Iteration 69, loss = 0.19998861\n","Iteration 70, loss = 0.19975730\n","Iteration 71, loss = 0.19948381\n","Iteration 72, loss = 0.19928323\n","Iteration 73, loss = 0.19902146\n","Iteration 74, loss = 0.19872216\n","Iteration 75, loss = 0.19852380\n","Iteration 76, loss = 0.19822830\n","Iteration 77, loss = 0.19802677\n","Iteration 78, loss = 0.19780709\n","Iteration 79, loss = 0.19757387\n","Iteration 80, loss = 0.19734867\n","Iteration 81, loss = 0.19710308\n","Iteration 82, loss = 0.19694300\n","Iteration 83, loss = 0.19671100\n","Iteration 84, loss = 0.19644981\n","Iteration 85, loss = 0.19626316\n","Iteration 86, loss = 0.19607337\n","Iteration 87, loss = 0.19583086\n","Iteration 88, loss = 0.19561992\n","Iteration 89, loss = 0.19542250\n","Iteration 90, loss = 0.19525720\n","Iteration 91, loss = 0.19507367\n","Iteration 92, loss = 0.19481551\n","Iteration 93, loss = 0.19462869\n","Iteration 94, loss = 0.19437216\n","Iteration 95, loss = 0.19418368\n","Iteration 96, loss = 0.19404949\n","Iteration 97, loss = 0.19389031\n","Iteration 98, loss = 0.19357483\n","Iteration 99, loss = 0.19341352\n","Iteration 100, loss = 0.19323700\n","Iteration 101, loss = 0.19304007\n","Iteration 102, loss = 0.19281343\n","Iteration 103, loss = 0.19262092\n","Iteration 104, loss = 0.19248309\n","Iteration 105, loss = 0.19221199\n","Iteration 106, loss = 0.19204977\n","Iteration 107, loss = 0.19186968\n","Iteration 108, loss = 0.19171593\n","Iteration 109, loss = 0.19149375\n","Iteration 110, loss = 0.19129057\n","Iteration 111, loss = 0.19111689\n","Iteration 112, loss = 0.19097833\n","Iteration 113, loss = 0.19075083\n","Iteration 114, loss = 0.19056047\n","Iteration 115, loss = 0.19033413\n","Iteration 116, loss = 0.19016918\n","Iteration 117, loss = 0.19001112\n","Iteration 118, loss = 0.18982330\n","Iteration 119, loss = 0.18962873\n","Iteration 120, loss = 0.18944389\n","Iteration 121, loss = 0.18923362\n","Iteration 122, loss = 0.18907690\n","Iteration 123, loss = 0.18885844\n","Iteration 124, loss = 0.18873583\n","Iteration 125, loss = 0.18853747\n","Iteration 126, loss = 0.18832921\n","Iteration 127, loss = 0.18814391\n","Iteration 128, loss = 0.18798019\n","Iteration 129, loss = 0.18784237\n","Iteration 130, loss = 0.18768878\n","Iteration 131, loss = 0.18747069\n","Iteration 132, loss = 0.18731570\n","Iteration 133, loss = 0.18712830\n","Iteration 134, loss = 0.18696552\n","Iteration 135, loss = 0.18679970\n","Iteration 136, loss = 0.18662150\n","Iteration 137, loss = 0.18642599\n","Iteration 138, loss = 0.18627138\n","Iteration 139, loss = 0.18611911\n","Iteration 140, loss = 0.18594682\n","Iteration 141, loss = 0.18576123\n","Iteration 142, loss = 0.18560907\n","Iteration 143, loss = 0.18548255\n","Iteration 144, loss = 0.18528336\n","Iteration 145, loss = 0.18515801\n","Iteration 146, loss = 0.18496778\n","Iteration 147, loss = 0.18478841\n","Iteration 148, loss = 0.18463828\n","Iteration 149, loss = 0.18448239\n","Iteration 150, loss = 0.18430350\n","Iteration 1, loss = 0.65543788\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.52205602\n","Iteration 3, loss = 0.45314531\n","Iteration 4, loss = 0.40502391\n","Iteration 5, loss = 0.36741322\n","Iteration 6, loss = 0.33757557\n","Iteration 7, loss = 0.31382254\n","Iteration 8, loss = 0.29504564\n","Iteration 9, loss = 0.28052352\n","Iteration 10, loss = 0.26932469\n","Iteration 11, loss = 0.26057037\n","Iteration 12, loss = 0.25372556\n","Iteration 13, loss = 0.24829870\n","Iteration 14, loss = 0.24395581\n","Iteration 15, loss = 0.24035723\n","Iteration 16, loss = 0.23732393\n","Iteration 17, loss = 0.23482824\n","Iteration 18, loss = 0.23261337\n","Iteration 19, loss = 0.23071450\n","Iteration 20, loss = 0.22911514\n","Iteration 21, loss = 0.22755867\n","Iteration 22, loss = 0.22627069\n","Iteration 23, loss = 0.22506288\n","Iteration 24, loss = 0.22393382\n","Iteration 25, loss = 0.22289841\n","Iteration 26, loss = 0.22195568\n","Iteration 27, loss = 0.22113839\n","Iteration 28, loss = 0.22032030\n","Iteration 29, loss = 0.21950906\n","Iteration 30, loss = 0.21879695\n","Iteration 31, loss = 0.21808777\n","Iteration 32, loss = 0.21747195\n","Iteration 33, loss = 0.21680988\n","Iteration 34, loss = 0.21630312\n","Iteration 35, loss = 0.21565552\n","Iteration 36, loss = 0.21509014\n","Iteration 37, loss = 0.21458948\n","Iteration 38, loss = 0.21400326\n","Iteration 39, loss = 0.21353343\n","Iteration 40, loss = 0.21300294\n","Iteration 41, loss = 0.21257398\n","Iteration 42, loss = 0.21213163\n","Iteration 43, loss = 0.21174964\n","Iteration 44, loss = 0.21122402\n","Iteration 45, loss = 0.21080142\n","Iteration 46, loss = 0.21035114\n","Iteration 47, loss = 0.20995180\n","Iteration 48, loss = 0.20958433\n","Iteration 49, loss = 0.20918267\n","Iteration 50, loss = 0.20883180\n","Iteration 51, loss = 0.20843828\n","Iteration 52, loss = 0.20806992\n","Iteration 53, loss = 0.20768689\n","Iteration 54, loss = 0.20738905\n","Iteration 55, loss = 0.20705849\n","Iteration 56, loss = 0.20665945\n","Iteration 57, loss = 0.20634311\n","Iteration 58, loss = 0.20604745\n","Iteration 59, loss = 0.20567340\n","Iteration 60, loss = 0.20535279\n","Iteration 61, loss = 0.20504833\n","Iteration 62, loss = 0.20469396\n","Iteration 63, loss = 0.20440704\n","Iteration 64, loss = 0.20411424\n","Iteration 65, loss = 0.20378821\n","Iteration 66, loss = 0.20348723\n","Iteration 67, loss = 0.20318677\n","Iteration 68, loss = 0.20287303\n","Iteration 69, loss = 0.20260899\n","Iteration 70, loss = 0.20230639\n","Iteration 71, loss = 0.20200718\n","Iteration 72, loss = 0.20180088\n","Iteration 73, loss = 0.20142290\n","Iteration 74, loss = 0.20122776\n","Iteration 75, loss = 0.20093996\n","Iteration 76, loss = 0.20071103\n","Iteration 77, loss = 0.20039359\n","Iteration 78, loss = 0.20010113\n","Iteration 79, loss = 0.19987981\n","Iteration 80, loss = 0.19966099\n","Iteration 81, loss = 0.19933508\n","Iteration 82, loss = 0.19908553\n","Iteration 83, loss = 0.19879905\n","Iteration 84, loss = 0.19859043\n","Iteration 85, loss = 0.19835459\n","Iteration 86, loss = 0.19804928\n","Iteration 87, loss = 0.19781439\n","Iteration 88, loss = 0.19759472\n","Iteration 89, loss = 0.19736252\n","Iteration 90, loss = 0.19709196\n","Iteration 91, loss = 0.19689955\n","Iteration 92, loss = 0.19660939\n","Iteration 93, loss = 0.19644359\n","Iteration 94, loss = 0.19615291\n","Iteration 95, loss = 0.19591240\n","Iteration 96, loss = 0.19569469\n","Iteration 97, loss = 0.19549232\n","Iteration 98, loss = 0.19528809\n","Iteration 99, loss = 0.19503366\n","Iteration 100, loss = 0.19480489\n","Iteration 101, loss = 0.19461235\n","Iteration 102, loss = 0.19435107\n","Iteration 103, loss = 0.19414884\n","Iteration 104, loss = 0.19397172\n","Iteration 105, loss = 0.19371945\n","Iteration 106, loss = 0.19349477\n","Iteration 107, loss = 0.19326653\n","Iteration 108, loss = 0.19309005\n","Iteration 109, loss = 0.19284111\n","Iteration 110, loss = 0.19263301\n","Iteration 111, loss = 0.19248055\n","Iteration 112, loss = 0.19227505\n","Iteration 113, loss = 0.19205538\n","Iteration 114, loss = 0.19182496\n","Iteration 115, loss = 0.19161396\n","Iteration 116, loss = 0.19143123\n","Iteration 117, loss = 0.19130860\n","Iteration 118, loss = 0.19101232\n","Iteration 119, loss = 0.19081356\n","Iteration 120, loss = 0.19062898\n","Iteration 121, loss = 0.19043320\n","Iteration 122, loss = 0.19022887\n","Iteration 123, loss = 0.19004972\n","Iteration 124, loss = 0.18985633\n","Iteration 125, loss = 0.18963349\n","Iteration 126, loss = 0.18944933\n","Iteration 127, loss = 0.18930592\n","Iteration 128, loss = 0.18900992\n","Iteration 129, loss = 0.18891248\n","Iteration 130, loss = 0.18868648\n","Iteration 131, loss = 0.18850279\n","Iteration 132, loss = 0.18835744\n","Iteration 133, loss = 0.18801703\n","Iteration 134, loss = 0.18784456\n","Iteration 135, loss = 0.18765810\n","Iteration 136, loss = 0.18748244\n","Iteration 137, loss = 0.18728215\n","Iteration 138, loss = 0.18713219\n","Iteration 139, loss = 0.18696227\n","Iteration 140, loss = 0.18672677\n","Iteration 141, loss = 0.18655176\n","Iteration 142, loss = 0.18636573\n","Iteration 143, loss = 0.18615632\n","Iteration 144, loss = 0.18597343\n","Iteration 145, loss = 0.18582079\n","Iteration 146, loss = 0.18560683\n","Iteration 147, loss = 0.18539382\n","Iteration 148, loss = 0.18521642\n","Iteration 149, loss = 0.18510580\n","Iteration 150, loss = 0.18484379\n","Iteration 1, loss = 0.59886470\n","Iteration 2, loss = 0.46486473\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.38933749\n","Iteration 4, loss = 0.34317298\n","Iteration 5, loss = 0.31330677\n","Iteration 6, loss = 0.29255553\n","Iteration 7, loss = 0.27736724\n","Iteration 8, loss = 0.26601522\n","Iteration 9, loss = 0.25730869\n","Iteration 10, loss = 0.25047718\n","Iteration 11, loss = 0.24497839\n","Iteration 12, loss = 0.24060920\n","Iteration 13, loss = 0.23698541\n","Iteration 14, loss = 0.23404615\n","Iteration 15, loss = 0.23146785\n","Iteration 16, loss = 0.22925679\n","Iteration 17, loss = 0.22732101\n","Iteration 18, loss = 0.22563889\n","Iteration 19, loss = 0.22402789\n","Iteration 20, loss = 0.22265853\n","Iteration 21, loss = 0.22137120\n","Iteration 22, loss = 0.22026937\n","Iteration 23, loss = 0.21920441\n","Iteration 24, loss = 0.21819502\n","Iteration 25, loss = 0.21728130\n","Iteration 26, loss = 0.21643984\n","Iteration 27, loss = 0.21568165\n","Iteration 28, loss = 0.21491959\n","Iteration 29, loss = 0.21425386\n","Iteration 30, loss = 0.21360729\n","Iteration 31, loss = 0.21294331\n","Iteration 32, loss = 0.21232964\n","Iteration 33, loss = 0.21178290\n","Iteration 34, loss = 0.21129241\n","Iteration 35, loss = 0.21072309\n","Iteration 36, loss = 0.21027200\n","Iteration 37, loss = 0.20975712\n","Iteration 38, loss = 0.20929763\n","Iteration 39, loss = 0.20886220\n","Iteration 40, loss = 0.20843399\n","Iteration 41, loss = 0.20807110\n","Iteration 42, loss = 0.20764324\n","Iteration 43, loss = 0.20725458\n","Iteration 44, loss = 0.20688550\n","Iteration 45, loss = 0.20650912\n","Iteration 46, loss = 0.20615954\n","Iteration 47, loss = 0.20582069\n","Iteration 48, loss = 0.20547236\n","Iteration 49, loss = 0.20513713\n","Iteration 50, loss = 0.20486920\n","Iteration 51, loss = 0.20453985\n","Iteration 52, loss = 0.20414864\n","Iteration 53, loss = 0.20386927\n","Iteration 54, loss = 0.20355552\n","Iteration 55, loss = 0.20325755\n","Iteration 56, loss = 0.20303781\n","Iteration 57, loss = 0.20270338\n","Iteration 58, loss = 0.20240926\n","Iteration 59, loss = 0.20220693\n","Iteration 60, loss = 0.20186245\n","Iteration 61, loss = 0.20159860\n","Iteration 62, loss = 0.20138081\n","Iteration 63, loss = 0.20112524\n","Iteration 64, loss = 0.20082191\n","Iteration 65, loss = 0.20066565\n","Iteration 66, loss = 0.20028602\n","Iteration 67, loss = 0.20006716\n","Iteration 68, loss = 0.19980429\n","Iteration 69, loss = 0.19953445\n","Iteration 70, loss = 0.19928087\n","Iteration 71, loss = 0.19907046\n","Iteration 72, loss = 0.19884637\n","Iteration 73, loss = 0.19862324\n","Iteration 74, loss = 0.19835821\n","Iteration 75, loss = 0.19811358\n","Iteration 76, loss = 0.19794079\n","Iteration 77, loss = 0.19771328\n","Iteration 78, loss = 0.19746088\n","Iteration 79, loss = 0.19728785\n","Iteration 80, loss = 0.19697927\n","Iteration 81, loss = 0.19679462\n","Iteration 82, loss = 0.19656541\n","Iteration 83, loss = 0.19632665\n","Iteration 84, loss = 0.19613127\n","Iteration 85, loss = 0.19588981\n","Iteration 86, loss = 0.19572061\n","Iteration 87, loss = 0.19550014\n","Iteration 88, loss = 0.19526713\n","Iteration 89, loss = 0.19510081\n","Iteration 90, loss = 0.19481677\n","Iteration 91, loss = 0.19463177\n","Iteration 92, loss = 0.19446816\n","Iteration 93, loss = 0.19421154\n","Iteration 94, loss = 0.19403131\n","Iteration 95, loss = 0.19378214\n","Iteration 96, loss = 0.19359181\n","Iteration 97, loss = 0.19339294\n","Iteration 98, loss = 0.19317617\n","Iteration 99, loss = 0.19295863\n","Iteration 100, loss = 0.19275213\n","Iteration 101, loss = 0.19258288\n","Iteration 102, loss = 0.19236819\n","Iteration 103, loss = 0.19216841\n","Iteration 104, loss = 0.19194886\n","Iteration 105, loss = 0.19174924\n","Iteration 106, loss = 0.19153163\n","Iteration 107, loss = 0.19139683\n","Iteration 108, loss = 0.19121019\n","Iteration 109, loss = 0.19097777\n","Iteration 110, loss = 0.19080276\n","Iteration 111, loss = 0.19062644\n","Iteration 112, loss = 0.19047258\n","Iteration 113, loss = 0.19024233\n","Iteration 114, loss = 0.19005538\n","Iteration 115, loss = 0.18986807\n","Iteration 116, loss = 0.18969370\n","Iteration 117, loss = 0.18952115\n","Iteration 118, loss = 0.18936003\n","Iteration 119, loss = 0.18914435\n","Iteration 120, loss = 0.18892766\n","Iteration 121, loss = 0.18877287\n","Iteration 122, loss = 0.18861422\n","Iteration 123, loss = 0.18845205\n","Iteration 124, loss = 0.18824704\n","Iteration 125, loss = 0.18807272\n","Iteration 126, loss = 0.18790819\n","Iteration 127, loss = 0.18771749\n","Iteration 128, loss = 0.18755764\n","Iteration 129, loss = 0.18739432\n","Iteration 130, loss = 0.18724296\n","Iteration 131, loss = 0.18704172\n","Iteration 132, loss = 0.18686552\n","Iteration 133, loss = 0.18674759\n","Iteration 134, loss = 0.18651624\n","Iteration 135, loss = 0.18641430\n","Iteration 136, loss = 0.18623371\n","Iteration 137, loss = 0.18606534\n","Iteration 138, loss = 0.18590209\n","Iteration 139, loss = 0.18575130\n","Iteration 140, loss = 0.18557260\n","Iteration 141, loss = 0.18540254\n","Iteration 142, loss = 0.18531483\n","Iteration 143, loss = 0.18506317\n","Iteration 144, loss = 0.18490911\n","Iteration 145, loss = 0.18482233\n","Iteration 146, loss = 0.18458058\n","Iteration 147, loss = 0.18447290\n","Iteration 148, loss = 0.18427895\n","Iteration 149, loss = 0.18407069\n","Iteration 150, loss = 0.18394399\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.34360163\n","Iteration 2, loss = 0.22829351\n","Iteration 3, loss = 0.21630360\n","Iteration 4, loss = 0.20959788\n","Iteration 5, loss = 0.20453403\n","Iteration 6, loss = 0.20077213\n","Iteration 7, loss = 0.19584353\n","Iteration 8, loss = 0.19262826\n","Iteration 9, loss = 0.18987254\n","Iteration 10, loss = 0.18618046\n","Iteration 11, loss = 0.18313426\n","Iteration 12, loss = 0.18006661\n","Iteration 13, loss = 0.17770246\n","Iteration 14, loss = 0.17450931\n","Iteration 15, loss = 0.17140376\n","Iteration 16, loss = 0.16872606\n","Iteration 17, loss = 0.16638038\n","Iteration 18, loss = 0.16416600\n","Iteration 19, loss = 0.16020850\n","Iteration 20, loss = 0.15738740\n","Iteration 21, loss = 0.15418341\n","Iteration 22, loss = 0.15237552\n","Iteration 23, loss = 0.14880046\n","Iteration 24, loss = 0.14584397\n","Iteration 25, loss = 0.14260117\n","Iteration 26, loss = 0.14060987\n","Iteration 27, loss = 0.13647525\n","Iteration 28, loss = 0.13491924\n","Iteration 29, loss = 0.13089774\n","Iteration 30, loss = 0.12770162\n","Iteration 31, loss = 0.12457223\n","Iteration 32, loss = 0.12170919\n","Iteration 33, loss = 0.11953278\n","Iteration 34, loss = 0.11630685\n","Iteration 35, loss = 0.11452713\n","Iteration 36, loss = 0.11087956\n","Iteration 37, loss = 0.10714778\n","Iteration 38, loss = 0.10578482\n","Iteration 39, loss = 0.10425331\n","Iteration 40, loss = 0.09897104\n","Iteration 41, loss = 0.09792889\n","Iteration 42, loss = 0.09490833\n","Iteration 43, loss = 0.09173050\n","Iteration 44, loss = 0.08962020\n","Iteration 45, loss = 0.08753393\n","Iteration 46, loss = 0.08660213\n","Iteration 47, loss = 0.08326118\n","Iteration 48, loss = 0.08066104\n","Iteration 49, loss = 0.07764352\n","Iteration 50, loss = 0.07628173\n","Iteration 51, loss = 0.07347524\n","Iteration 52, loss = 0.07060790\n","Iteration 53, loss = 0.06924090\n","Iteration 54, loss = 0.06711066\n","Iteration 55, loss = 0.06562860\n","Iteration 56, loss = 0.06159902\n","Iteration 57, loss = 0.06101104\n","Iteration 58, loss = 0.05906730\n","Iteration 59, loss = 0.05703168\n","Iteration 60, loss = 0.05519473\n","Iteration 61, loss = 0.05262802\n","Iteration 62, loss = 0.05061693\n","Iteration 63, loss = 0.04985802\n","Iteration 64, loss = 0.04800293\n","Iteration 65, loss = 0.04560891\n","Iteration 66, loss = 0.04289069\n","Iteration 67, loss = 0.04375855\n","Iteration 68, loss = 0.04104190\n","Iteration 69, loss = 0.04133046\n","Iteration 70, loss = 0.03812516\n","Iteration 71, loss = 0.03701075\n","Iteration 72, loss = 0.03780894\n","Iteration 73, loss = 0.03431646\n","Iteration 74, loss = 0.03445587\n","Iteration 75, loss = 0.03229533\n","Iteration 76, loss = 0.03061341\n","Iteration 77, loss = 0.02976556\n","Iteration 78, loss = 0.02889408\n","Iteration 79, loss = 0.02856113\n","Iteration 80, loss = 0.02649469\n","Iteration 81, loss = 0.02678205\n","Iteration 82, loss = 0.02640662\n","Iteration 83, loss = 0.02443233\n","Iteration 84, loss = 0.02255410\n","Iteration 85, loss = 0.02208029\n","Iteration 86, loss = 0.02219778\n","Iteration 87, loss = 0.02194857\n","Iteration 88, loss = 0.02087752\n","Iteration 89, loss = 0.02106098\n","Iteration 90, loss = 0.01952085\n","Iteration 91, loss = 0.01876810\n","Iteration 92, loss = 0.01754476\n","Iteration 93, loss = 0.01686682\n","Iteration 94, loss = 0.01692179\n","Iteration 95, loss = 0.01640580\n","Iteration 96, loss = 0.01546681\n","Iteration 97, loss = 0.01421050\n","Iteration 98, loss = 0.01382048\n","Iteration 99, loss = 0.01307979\n","Iteration 100, loss = 0.01285319\n","Iteration 101, loss = 0.01248296\n","Iteration 102, loss = 0.01188677\n","Iteration 103, loss = 0.01175457\n","Iteration 104, loss = 0.01118923\n","Iteration 105, loss = 0.01110491\n","Iteration 106, loss = 0.01056591\n","Iteration 107, loss = 0.00998915\n","Iteration 108, loss = 0.01002251\n","Iteration 109, loss = 0.00948691\n","Iteration 110, loss = 0.00906228\n","Iteration 111, loss = 0.00873287\n","Iteration 112, loss = 0.00867589\n","Iteration 113, loss = 0.00814527\n","Iteration 114, loss = 0.00801677\n","Iteration 115, loss = 0.00784954\n","Iteration 116, loss = 0.00774010\n","Iteration 117, loss = 0.00774318\n","Iteration 118, loss = 0.00691476\n","Iteration 119, loss = 0.00669704\n","Iteration 120, loss = 0.00655635\n","Iteration 121, loss = 0.00619773\n","Iteration 122, loss = 0.00598995\n","Iteration 123, loss = 0.00573537\n","Iteration 124, loss = 0.00603950\n","Iteration 125, loss = 0.00605535\n","Iteration 126, loss = 0.00547769\n","Iteration 127, loss = 0.00552739\n","Iteration 128, loss = 0.00506671\n","Iteration 129, loss = 0.00490727\n","Iteration 130, loss = 0.00475611\n","Iteration 131, loss = 0.00467334\n","Iteration 132, loss = 0.00460561\n","Iteration 133, loss = 0.00431326\n","Iteration 134, loss = 0.00433404\n","Iteration 135, loss = 0.00417550\n","Iteration 136, loss = 0.00401255\n","Iteration 137, loss = 0.00390685\n","Iteration 138, loss = 0.00378295\n","Iteration 139, loss = 0.00363646\n","Iteration 140, loss = 0.00353782\n","Iteration 141, loss = 0.00343991\n","Iteration 142, loss = 0.00343642\n","Iteration 143, loss = 0.00339570\n","Iteration 144, loss = 0.00337479\n","Iteration 145, loss = 0.00310619\n","Iteration 146, loss = 0.00297840\n","Iteration 147, loss = 0.00293130\n","Iteration 148, loss = 0.00285381\n","Iteration 149, loss = 0.00282823\n","Iteration 150, loss = 0.00269112\n","Iteration 1, loss = 0.35841520\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.22926602\n","Iteration 3, loss = 0.21694668\n","Iteration 4, loss = 0.21005526\n","Iteration 5, loss = 0.20552108\n","Iteration 6, loss = 0.20170020\n","Iteration 7, loss = 0.19756417\n","Iteration 8, loss = 0.19387757\n","Iteration 9, loss = 0.19062708\n","Iteration 10, loss = 0.18836171\n","Iteration 11, loss = 0.18413168\n","Iteration 12, loss = 0.18145020\n","Iteration 13, loss = 0.17807360\n","Iteration 14, loss = 0.17572512\n","Iteration 15, loss = 0.17227535\n","Iteration 16, loss = 0.16988341\n","Iteration 17, loss = 0.16643918\n","Iteration 18, loss = 0.16375647\n","Iteration 19, loss = 0.16016415\n","Iteration 20, loss = 0.15648165\n","Iteration 21, loss = 0.15393472\n","Iteration 22, loss = 0.15114482\n","Iteration 23, loss = 0.14781191\n","Iteration 24, loss = 0.14489064\n","Iteration 25, loss = 0.14121817\n","Iteration 26, loss = 0.13864472\n","Iteration 27, loss = 0.13500297\n","Iteration 28, loss = 0.13359997\n","Iteration 29, loss = 0.13024549\n","Iteration 30, loss = 0.12553475\n","Iteration 31, loss = 0.12275911\n","Iteration 32, loss = 0.12005046\n","Iteration 33, loss = 0.11836739\n","Iteration 34, loss = 0.11461381\n","Iteration 35, loss = 0.11276094\n","Iteration 36, loss = 0.10825876\n","Iteration 37, loss = 0.10558398\n","Iteration 38, loss = 0.10192693\n","Iteration 39, loss = 0.09968332\n","Iteration 40, loss = 0.09982533\n","Iteration 41, loss = 0.09467396\n","Iteration 42, loss = 0.09189841\n","Iteration 43, loss = 0.09106866\n","Iteration 44, loss = 0.08652426\n","Iteration 45, loss = 0.08359768\n","Iteration 46, loss = 0.08253670\n","Iteration 47, loss = 0.08072733\n","Iteration 48, loss = 0.07720046\n","Iteration 49, loss = 0.07587606\n","Iteration 50, loss = 0.07372770\n","Iteration 51, loss = 0.07040690\n","Iteration 52, loss = 0.07000884\n","Iteration 53, loss = 0.06666729\n","Iteration 54, loss = 0.06484440\n","Iteration 55, loss = 0.06313397\n","Iteration 56, loss = 0.06213210\n","Iteration 57, loss = 0.05986270\n","Iteration 58, loss = 0.05876189\n","Iteration 59, loss = 0.05659615\n","Iteration 60, loss = 0.05368627\n","Iteration 61, loss = 0.05220913\n","Iteration 62, loss = 0.05230599\n","Iteration 63, loss = 0.04824463\n","Iteration 64, loss = 0.04612695\n","Iteration 65, loss = 0.04528520\n","Iteration 66, loss = 0.04343906\n","Iteration 67, loss = 0.04216164\n","Iteration 68, loss = 0.04116718\n","Iteration 69, loss = 0.03855827\n","Iteration 70, loss = 0.03802030\n","Iteration 71, loss = 0.03659444\n","Iteration 72, loss = 0.03565495\n","Iteration 73, loss = 0.03377901\n","Iteration 74, loss = 0.03210101\n","Iteration 75, loss = 0.03129802\n","Iteration 76, loss = 0.03008461\n","Iteration 77, loss = 0.02876573\n","Iteration 78, loss = 0.02807964\n","Iteration 79, loss = 0.02700993\n","Iteration 80, loss = 0.02511743\n","Iteration 81, loss = 0.02472994\n","Iteration 82, loss = 0.02429979\n","Iteration 83, loss = 0.02468140\n","Iteration 84, loss = 0.02364163\n","Iteration 85, loss = 0.02181842\n","Iteration 86, loss = 0.02204454\n","Iteration 87, loss = 0.02044987\n","Iteration 88, loss = 0.01877815\n","Iteration 89, loss = 0.01857216\n","Iteration 90, loss = 0.01835864\n","Iteration 91, loss = 0.01718002\n","Iteration 92, loss = 0.01693515\n","Iteration 93, loss = 0.01592323\n","Iteration 94, loss = 0.01565452\n","Iteration 95, loss = 0.01509693\n","Iteration 96, loss = 0.01374415\n","Iteration 97, loss = 0.01352674\n","Iteration 98, loss = 0.01325768\n","Iteration 99, loss = 0.01267127\n","Iteration 100, loss = 0.01184395\n","Iteration 101, loss = 0.01143761\n","Iteration 102, loss = 0.01110741\n","Iteration 103, loss = 0.01112031\n","Iteration 104, loss = 0.01057043\n","Iteration 105, loss = 0.00986610\n","Iteration 106, loss = 0.00947343\n","Iteration 107, loss = 0.00906513\n","Iteration 108, loss = 0.00969045\n","Iteration 109, loss = 0.00879454\n","Iteration 110, loss = 0.00857282\n","Iteration 111, loss = 0.00819926\n","Iteration 112, loss = 0.00788241\n","Iteration 113, loss = 0.00776507\n","Iteration 114, loss = 0.00734477\n","Iteration 115, loss = 0.00692557\n","Iteration 116, loss = 0.00676594\n","Iteration 117, loss = 0.00672349\n","Iteration 118, loss = 0.00668214\n","Iteration 119, loss = 0.00630417\n","Iteration 120, loss = 0.00588237\n","Iteration 121, loss = 0.00581826\n","Iteration 122, loss = 0.00580218\n","Iteration 123, loss = 0.00563968\n","Iteration 124, loss = 0.00533559\n","Iteration 125, loss = 0.00508308\n","Iteration 126, loss = 0.00499292\n","Iteration 127, loss = 0.00487471\n","Iteration 128, loss = 0.00483623\n","Iteration 129, loss = 0.00445672\n","Iteration 130, loss = 0.00432440\n","Iteration 131, loss = 0.00414903\n","Iteration 132, loss = 0.00413963\n","Iteration 133, loss = 0.00394470\n","Iteration 134, loss = 0.00387067\n","Iteration 135, loss = 0.00379526\n","Iteration 136, loss = 0.00362427\n","Iteration 137, loss = 0.00350366\n","Iteration 138, loss = 0.00353171\n","Iteration 139, loss = 0.00346777\n","Iteration 140, loss = 0.00340533\n","Iteration 141, loss = 0.00328714\n","Iteration 142, loss = 0.00307766\n","Iteration 143, loss = 0.00298324\n","Iteration 144, loss = 0.00292735\n","Iteration 145, loss = 0.00282265\n","Iteration 146, loss = 0.00278072\n","Iteration 147, loss = 0.00278653\n","Iteration 148, loss = 0.00276991\n","Iteration 149, loss = 0.00263140\n","Iteration 150, loss = 0.00252270\n","Iteration 1, loss = 0.42862505\n","Iteration 2, loss = 0.23983815\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.21743450\n","Iteration 4, loss = 0.20846392\n","Iteration 5, loss = 0.20259795\n","Iteration 6, loss = 0.19749270\n","Iteration 7, loss = 0.19335442\n","Iteration 8, loss = 0.18981894\n","Iteration 9, loss = 0.18610688\n","Iteration 10, loss = 0.18234533\n","Iteration 11, loss = 0.17926838\n","Iteration 12, loss = 0.17564532\n","Iteration 13, loss = 0.17259739\n","Iteration 14, loss = 0.16923296\n","Iteration 15, loss = 0.16632949\n","Iteration 16, loss = 0.16385645\n","Iteration 17, loss = 0.16070208\n","Iteration 18, loss = 0.15724795\n","Iteration 19, loss = 0.15491098\n","Iteration 20, loss = 0.15107400\n","Iteration 21, loss = 0.14812025\n","Iteration 22, loss = 0.14533053\n","Iteration 23, loss = 0.14153115\n","Iteration 24, loss = 0.13923055\n","Iteration 25, loss = 0.13569486\n","Iteration 26, loss = 0.13328826\n","Iteration 27, loss = 0.12993859\n","Iteration 28, loss = 0.12702990\n","Iteration 29, loss = 0.12439393\n","Iteration 30, loss = 0.12149195\n","Iteration 31, loss = 0.11763174\n","Iteration 32, loss = 0.11437325\n","Iteration 33, loss = 0.11181197\n","Iteration 34, loss = 0.11053698\n","Iteration 35, loss = 0.10538492\n","Iteration 36, loss = 0.10286434\n","Iteration 37, loss = 0.10045352\n","Iteration 38, loss = 0.09729091\n","Iteration 39, loss = 0.09607544\n","Iteration 40, loss = 0.09248858\n","Iteration 41, loss = 0.08905174\n","Iteration 42, loss = 0.08668580\n","Iteration 43, loss = 0.08456608\n","Iteration 44, loss = 0.08134000\n","Iteration 45, loss = 0.07936595\n","Iteration 46, loss = 0.07710467\n","Iteration 47, loss = 0.07415640\n","Iteration 48, loss = 0.07178058\n","Iteration 49, loss = 0.07150637\n","Iteration 50, loss = 0.06782025\n","Iteration 51, loss = 0.06770109\n","Iteration 52, loss = 0.06348658\n","Iteration 53, loss = 0.06118010\n","Iteration 54, loss = 0.05907285\n","Iteration 55, loss = 0.05603000\n","Iteration 56, loss = 0.05367823\n","Iteration 57, loss = 0.05288004\n","Iteration 58, loss = 0.05133593\n","Iteration 59, loss = 0.04934784\n","Iteration 60, loss = 0.04895540\n","Iteration 61, loss = 0.04602679\n","Iteration 62, loss = 0.04547737\n","Iteration 63, loss = 0.04285623\n","Iteration 64, loss = 0.04258731\n","Iteration 65, loss = 0.04023735\n","Iteration 66, loss = 0.03763412\n","Iteration 67, loss = 0.03762477\n","Iteration 68, loss = 0.03572286\n","Iteration 69, loss = 0.03423053\n","Iteration 70, loss = 0.03389838\n","Iteration 71, loss = 0.03160431\n","Iteration 72, loss = 0.02999032\n","Iteration 73, loss = 0.02867340\n","Iteration 74, loss = 0.02796292\n","Iteration 75, loss = 0.02718376\n","Iteration 76, loss = 0.02588178\n","Iteration 77, loss = 0.02493992\n","Iteration 78, loss = 0.02470670\n","Iteration 79, loss = 0.02471129\n","Iteration 80, loss = 0.02260597\n","Iteration 81, loss = 0.02137577\n","Iteration 82, loss = 0.02024145\n","Iteration 83, loss = 0.01981887\n","Iteration 84, loss = 0.01895814\n","Iteration 85, loss = 0.01902675\n","Iteration 86, loss = 0.01773677\n","Iteration 87, loss = 0.01795960\n","Iteration 88, loss = 0.01625110\n","Iteration 89, loss = 0.01548321\n","Iteration 90, loss = 0.01491090\n","Iteration 91, loss = 0.01439844\n","Iteration 92, loss = 0.01405786\n","Iteration 93, loss = 0.01464686\n","Iteration 94, loss = 0.01350965\n","Iteration 95, loss = 0.01228141\n","Iteration 96, loss = 0.01192996\n","Iteration 97, loss = 0.01122725\n","Iteration 98, loss = 0.01070481\n","Iteration 99, loss = 0.01057873\n","Iteration 100, loss = 0.00977137\n","Iteration 101, loss = 0.00955405\n","Iteration 102, loss = 0.00919629\n","Iteration 103, loss = 0.00896044\n","Iteration 104, loss = 0.00873931\n","Iteration 105, loss = 0.00894977\n","Iteration 106, loss = 0.00838398\n","Iteration 107, loss = 0.00762784\n","Iteration 108, loss = 0.00729458\n","Iteration 109, loss = 0.00733075\n","Iteration 110, loss = 0.00705724\n","Iteration 111, loss = 0.00692017\n","Iteration 112, loss = 0.00638320\n","Iteration 113, loss = 0.00614693\n","Iteration 114, loss = 0.00600824\n","Iteration 115, loss = 0.00575068\n","Iteration 116, loss = 0.00569095\n","Iteration 117, loss = 0.00542097\n","Iteration 118, loss = 0.00521341\n","Iteration 119, loss = 0.00508465\n","Iteration 120, loss = 0.00499880\n","Iteration 121, loss = 0.00469611\n","Iteration 122, loss = 0.00452984\n","Iteration 123, loss = 0.00432400\n","Iteration 124, loss = 0.00427764\n","Iteration 125, loss = 0.00414555\n","Iteration 126, loss = 0.00406217\n","Iteration 127, loss = 0.00383768\n","Iteration 128, loss = 0.00367113\n","Iteration 129, loss = 0.00358183\n","Iteration 130, loss = 0.00356877\n","Iteration 131, loss = 0.00343800\n","Iteration 132, loss = 0.00341277\n","Iteration 133, loss = 0.00321547\n","Iteration 134, loss = 0.00313474\n","Iteration 135, loss = 0.00322134\n","Iteration 136, loss = 0.00312280\n","Iteration 137, loss = 0.00292324\n","Iteration 138, loss = 0.00281037\n","Iteration 139, loss = 0.00273835\n","Iteration 140, loss = 0.00267372\n","Iteration 141, loss = 0.00259917\n","Iteration 142, loss = 0.00251199\n","Iteration 143, loss = 0.00247807\n","Iteration 144, loss = 0.00239033\n","Iteration 145, loss = 0.00231477\n","Iteration 146, loss = 0.00225438\n","Iteration 147, loss = 0.00224800\n","Iteration 148, loss = 0.00214781\n","Iteration 149, loss = 0.00208985\n","Iteration 150, loss = 0.00201522\n","Iteration 1, loss = 0.50052640\n","Iteration 2, loss = 0.24816542\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.22114285\n","Iteration 4, loss = 0.21138390\n","Iteration 5, loss = 0.20575294\n","Iteration 6, loss = 0.20062943\n","Iteration 7, loss = 0.19701350\n","Iteration 8, loss = 0.19342151\n","Iteration 9, loss = 0.18950604\n","Iteration 10, loss = 0.18639280\n","Iteration 11, loss = 0.18364819\n","Iteration 12, loss = 0.18076845\n","Iteration 13, loss = 0.17743394\n","Iteration 14, loss = 0.17371591\n","Iteration 15, loss = 0.17088791\n","Iteration 16, loss = 0.16766401\n","Iteration 17, loss = 0.16491542\n","Iteration 18, loss = 0.16153629\n","Iteration 19, loss = 0.15948158\n","Iteration 20, loss = 0.15512325\n","Iteration 21, loss = 0.15318611\n","Iteration 22, loss = 0.15020600\n","Iteration 23, loss = 0.14674669\n","Iteration 24, loss = 0.14472097\n","Iteration 25, loss = 0.14040750\n","Iteration 26, loss = 0.13821140\n","Iteration 27, loss = 0.13555387\n","Iteration 28, loss = 0.13328855\n","Iteration 29, loss = 0.12963522\n","Iteration 30, loss = 0.12764109\n","Iteration 31, loss = 0.12321035\n","Iteration 32, loss = 0.12181722\n","Iteration 33, loss = 0.11776827\n","Iteration 34, loss = 0.11635889\n","Iteration 35, loss = 0.11326575\n","Iteration 36, loss = 0.10998981\n","Iteration 37, loss = 0.10824510\n","Iteration 38, loss = 0.10435036\n","Iteration 39, loss = 0.10234919\n","Iteration 40, loss = 0.10086620\n","Iteration 41, loss = 0.09929327\n","Iteration 42, loss = 0.09570416\n","Iteration 43, loss = 0.09214559\n","Iteration 44, loss = 0.09041252\n","Iteration 45, loss = 0.08897724\n","Iteration 46, loss = 0.08548072\n","Iteration 47, loss = 0.08424539\n","Iteration 48, loss = 0.08150440\n","Iteration 49, loss = 0.07936779\n","Iteration 50, loss = 0.07770363\n","Iteration 51, loss = 0.07558180\n","Iteration 52, loss = 0.07243589\n","Iteration 53, loss = 0.07084584\n","Iteration 54, loss = 0.06909608\n","Iteration 55, loss = 0.06770906\n","Iteration 56, loss = 0.06551578\n","Iteration 57, loss = 0.06354602\n","Iteration 58, loss = 0.06168460\n","Iteration 59, loss = 0.05952273\n","Iteration 60, loss = 0.05698847\n","Iteration 61, loss = 0.05547220\n","Iteration 62, loss = 0.05427197\n","Iteration 63, loss = 0.05309298\n","Iteration 64, loss = 0.05150202\n","Iteration 65, loss = 0.04946879\n","Iteration 66, loss = 0.04864966\n","Iteration 67, loss = 0.04791196\n","Iteration 68, loss = 0.04469513\n","Iteration 69, loss = 0.04202502\n","Iteration 70, loss = 0.04222564\n","Iteration 71, loss = 0.04108615\n","Iteration 72, loss = 0.04021793\n","Iteration 73, loss = 0.03919121\n","Iteration 74, loss = 0.03696146\n","Iteration 75, loss = 0.03559729\n","Iteration 76, loss = 0.03556517\n","Iteration 77, loss = 0.03374323\n","Iteration 78, loss = 0.03334193\n","Iteration 79, loss = 0.03057431\n","Iteration 80, loss = 0.03121902\n","Iteration 81, loss = 0.02846539\n","Iteration 82, loss = 0.02755366\n","Iteration 83, loss = 0.02786506\n","Iteration 84, loss = 0.02603543\n","Iteration 85, loss = 0.02508195\n","Iteration 86, loss = 0.02379891\n","Iteration 87, loss = 0.02346421\n","Iteration 88, loss = 0.02179571\n","Iteration 89, loss = 0.02153252\n","Iteration 90, loss = 0.02061581\n","Iteration 91, loss = 0.02051875\n","Iteration 92, loss = 0.01967071\n","Iteration 93, loss = 0.01867704\n","Iteration 94, loss = 0.01771289\n","Iteration 95, loss = 0.01711327\n","Iteration 96, loss = 0.01704008\n","Iteration 97, loss = 0.01612511\n","Iteration 98, loss = 0.01531533\n","Iteration 99, loss = 0.01496745\n","Iteration 100, loss = 0.01546896\n","Iteration 101, loss = 0.01451660\n","Iteration 102, loss = 0.01380231\n","Iteration 103, loss = 0.01374226\n","Iteration 104, loss = 0.01256628\n","Iteration 105, loss = 0.01213196\n","Iteration 106, loss = 0.01232860\n","Iteration 107, loss = 0.01132757\n","Iteration 108, loss = 0.01117123\n","Iteration 109, loss = 0.01082554\n","Iteration 110, loss = 0.01034829\n","Iteration 111, loss = 0.00948757\n","Iteration 112, loss = 0.00916712\n","Iteration 113, loss = 0.00907028\n","Iteration 114, loss = 0.00914414\n","Iteration 115, loss = 0.00856326\n","Iteration 116, loss = 0.00795479\n","Iteration 117, loss = 0.00788732\n","Iteration 118, loss = 0.00773943\n","Iteration 119, loss = 0.00726180\n","Iteration 120, loss = 0.00694153\n","Iteration 121, loss = 0.00692309\n","Iteration 122, loss = 0.00645831\n","Iteration 123, loss = 0.00632763\n","Iteration 124, loss = 0.00643753\n","Iteration 125, loss = 0.00618034\n","Iteration 126, loss = 0.00624649\n","Iteration 127, loss = 0.00574729\n","Iteration 128, loss = 0.00552477\n","Iteration 129, loss = 0.00557236\n","Iteration 130, loss = 0.00545018\n","Iteration 131, loss = 0.00510584\n","Iteration 132, loss = 0.00478268\n","Iteration 133, loss = 0.00460674\n","Iteration 134, loss = 0.00455675\n","Iteration 135, loss = 0.00433556\n","Iteration 136, loss = 0.00423127\n","Iteration 137, loss = 0.00412598\n","Iteration 138, loss = 0.00420708\n","Iteration 139, loss = 0.00403603\n","Iteration 140, loss = 0.00388628\n","Iteration 141, loss = 0.00376207\n","Iteration 142, loss = 0.00363794\n","Iteration 143, loss = 0.00351451\n","Iteration 144, loss = 0.00338982\n","Iteration 145, loss = 0.00337057\n","Iteration 146, loss = 0.00326083\n","Iteration 147, loss = 0.00321930\n","Iteration 148, loss = 0.00306166\n","Iteration 149, loss = 0.00295415\n","Iteration 150, loss = 0.00297464\n","Iteration 1, loss = 0.46162353\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.24315442\n","Iteration 3, loss = 0.21730917\n","Iteration 4, loss = 0.20812377\n","Iteration 5, loss = 0.20203340\n","Iteration 6, loss = 0.19713718\n","Iteration 7, loss = 0.19354296\n","Iteration 8, loss = 0.18904115\n","Iteration 9, loss = 0.18550315\n","Iteration 10, loss = 0.18218293\n","Iteration 11, loss = 0.17960376\n","Iteration 12, loss = 0.17663958\n","Iteration 13, loss = 0.17373647\n","Iteration 14, loss = 0.17158856\n","Iteration 15, loss = 0.16890583\n","Iteration 16, loss = 0.16633189\n","Iteration 17, loss = 0.16362486\n","Iteration 18, loss = 0.16047637\n","Iteration 19, loss = 0.15706789\n","Iteration 20, loss = 0.15567228\n","Iteration 21, loss = 0.15357650\n","Iteration 22, loss = 0.14959922\n","Iteration 23, loss = 0.14764616\n","Iteration 24, loss = 0.14460756\n","Iteration 25, loss = 0.14165908\n","Iteration 26, loss = 0.14010239\n","Iteration 27, loss = 0.13721046\n","Iteration 28, loss = 0.13441229\n","Iteration 29, loss = 0.13126003\n","Iteration 30, loss = 0.12835247\n","Iteration 31, loss = 0.12594868\n","Iteration 32, loss = 0.12309145\n","Iteration 33, loss = 0.12108076\n","Iteration 34, loss = 0.11807345\n","Iteration 35, loss = 0.11643792\n","Iteration 36, loss = 0.11330218\n","Iteration 37, loss = 0.11036877\n","Iteration 38, loss = 0.11080735\n","Iteration 39, loss = 0.10560011\n","Iteration 40, loss = 0.10567773\n","Iteration 41, loss = 0.10196690\n","Iteration 42, loss = 0.09953756\n","Iteration 43, loss = 0.09630197\n","Iteration 44, loss = 0.09368097\n","Iteration 45, loss = 0.09320043\n","Iteration 46, loss = 0.09073857\n","Iteration 47, loss = 0.08745151\n","Iteration 48, loss = 0.08506868\n","Iteration 49, loss = 0.08447842\n","Iteration 50, loss = 0.08088398\n","Iteration 51, loss = 0.07794221\n","Iteration 52, loss = 0.07735440\n","Iteration 53, loss = 0.07389361\n","Iteration 54, loss = 0.07471380\n","Iteration 55, loss = 0.07095123\n","Iteration 56, loss = 0.06891563\n","Iteration 57, loss = 0.06597451\n","Iteration 58, loss = 0.06529666\n","Iteration 59, loss = 0.06340521\n","Iteration 60, loss = 0.06252376\n","Iteration 61, loss = 0.06067460\n","Iteration 62, loss = 0.05810900\n","Iteration 63, loss = 0.05510518\n","Iteration 64, loss = 0.05415294\n","Iteration 65, loss = 0.05192820\n","Iteration 66, loss = 0.05257269\n","Iteration 67, loss = 0.05153206\n","Iteration 68, loss = 0.04829550\n","Iteration 69, loss = 0.04633478\n","Iteration 70, loss = 0.04636101\n","Iteration 71, loss = 0.04422937\n","Iteration 72, loss = 0.04245882\n","Iteration 73, loss = 0.04085771\n","Iteration 74, loss = 0.04006303\n","Iteration 75, loss = 0.03874421\n","Iteration 76, loss = 0.03744735\n","Iteration 77, loss = 0.03738781\n","Iteration 78, loss = 0.03608508\n","Iteration 79, loss = 0.03404003\n","Iteration 80, loss = 0.03336212\n","Iteration 81, loss = 0.03135735\n","Iteration 82, loss = 0.03101141\n","Iteration 83, loss = 0.02897882\n","Iteration 84, loss = 0.02807967\n","Iteration 85, loss = 0.02786345\n","Iteration 86, loss = 0.02620476\n","Iteration 87, loss = 0.02580453\n","Iteration 88, loss = 0.02385379\n","Iteration 89, loss = 0.02412919\n","Iteration 90, loss = 0.02343769\n","Iteration 91, loss = 0.02243160\n","Iteration 92, loss = 0.02226494\n","Iteration 93, loss = 0.02103568\n","Iteration 94, loss = 0.01957565\n","Iteration 95, loss = 0.01903804\n","Iteration 96, loss = 0.01840267\n","Iteration 97, loss = 0.01764873\n","Iteration 98, loss = 0.01670888\n","Iteration 99, loss = 0.01628712\n","Iteration 100, loss = 0.01558650\n","Iteration 101, loss = 0.01503412\n","Iteration 102, loss = 0.01509603\n","Iteration 103, loss = 0.01417722\n","Iteration 104, loss = 0.01337188\n","Iteration 105, loss = 0.01349504\n","Iteration 106, loss = 0.01272915\n","Iteration 107, loss = 0.01235623\n","Iteration 108, loss = 0.01191864\n","Iteration 109, loss = 0.01146759\n","Iteration 110, loss = 0.01188140\n","Iteration 111, loss = 0.01093301\n","Iteration 112, loss = 0.01053401\n","Iteration 113, loss = 0.00981607\n","Iteration 114, loss = 0.00947139\n","Iteration 115, loss = 0.00960543\n","Iteration 116, loss = 0.00901905\n","Iteration 117, loss = 0.00863882\n","Iteration 118, loss = 0.00843093\n","Iteration 119, loss = 0.00808083\n","Iteration 120, loss = 0.00796942\n","Iteration 121, loss = 0.00738735\n","Iteration 122, loss = 0.00734466\n","Iteration 123, loss = 0.00718936\n","Iteration 124, loss = 0.00679762\n","Iteration 125, loss = 0.00660785\n","Iteration 126, loss = 0.00666626\n","Iteration 127, loss = 0.00637449\n","Iteration 128, loss = 0.00614725\n","Iteration 129, loss = 0.00593083\n","Iteration 130, loss = 0.00564577\n","Iteration 131, loss = 0.00547148\n","Iteration 132, loss = 0.00540525\n","Iteration 133, loss = 0.00510171\n","Iteration 134, loss = 0.00495108\n","Iteration 135, loss = 0.00495678\n","Iteration 136, loss = 0.00475830\n","Iteration 137, loss = 0.00459219\n","Iteration 138, loss = 0.00452282\n","Iteration 139, loss = 0.00441651\n","Iteration 140, loss = 0.00419630\n","Iteration 141, loss = 0.00405613\n","Iteration 142, loss = 0.00416864\n","Iteration 143, loss = 0.00394140\n","Iteration 144, loss = 0.00378669\n","Iteration 145, loss = 0.00362855\n","Iteration 146, loss = 0.00351577\n","Iteration 147, loss = 0.00337092\n","Iteration 148, loss = 0.00328840\n","Iteration 149, loss = 0.00320264\n","Iteration 150, loss = 0.00318550\n","Iteration 1, loss = 0.68523453\n","Iteration 2, loss = 0.57755191"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration 3, loss = 0.51301593\n","Iteration 4, loss = 0.46790326\n","Iteration 5, loss = 0.43063284\n","Iteration 6, loss = 0.39777239\n","Iteration 7, loss = 0.36890832\n","Iteration 8, loss = 0.34439518\n","Iteration 9, loss = 0.32401186\n","Iteration 10, loss = 0.30772580\n","Iteration 11, loss = 0.29470550\n","Iteration 12, loss = 0.28436014\n","Iteration 13, loss = 0.27605217\n","Iteration 14, loss = 0.26930425\n","Iteration 15, loss = 0.26365028\n","Iteration 16, loss = 0.25895993\n","Iteration 17, loss = 0.25494997\n","Iteration 18, loss = 0.25144304\n","Iteration 19, loss = 0.24845395\n","Iteration 20, loss = 0.24586225\n","Iteration 21, loss = 0.24354967\n","Iteration 22, loss = 0.24149863\n","Iteration 23, loss = 0.23967264\n","Iteration 24, loss = 0.23798272\n","Iteration 25, loss = 0.23646739\n","Iteration 26, loss = 0.23513477\n","Iteration 27, loss = 0.23388249\n","Iteration 28, loss = 0.23280174\n","Iteration 29, loss = 0.23175111\n","Iteration 30, loss = 0.23083687\n","Iteration 31, loss = 0.23001152\n","Iteration 32, loss = 0.22920216\n","Iteration 33, loss = 0.22841393\n","Iteration 34, loss = 0.22766833\n","Iteration 35, loss = 0.22709615\n","Iteration 36, loss = 0.22639695\n","Iteration 37, loss = 0.22579495\n","Iteration 38, loss = 0.22520089\n","Iteration 39, loss = 0.22467222\n","Iteration 40, loss = 0.22413987\n","Iteration 41, loss = 0.22360838\n","Iteration 42, loss = 0.22314703\n","Iteration 43, loss = 0.22267141\n","Iteration 44, loss = 0.22224181\n","Iteration 45, loss = 0.22180100\n","Iteration 46, loss = 0.22136678\n","Iteration 47, loss = 0.22095951\n","Iteration 48, loss = 0.22060611\n","Iteration 49, loss = 0.22017058\n","Iteration 50, loss = 0.21983750\n","Iteration 51, loss = 0.21944237\n","Iteration 52, loss = 0.21910776\n","Iteration 53, loss = 0.21876342\n","Iteration 54, loss = 0.21845651\n","Iteration 55, loss = 0.21809589\n","Iteration 56, loss = 0.21781124\n","Iteration 57, loss = 0.21748837\n","Iteration 58, loss = 0.21722384\n","Iteration 59, loss = 0.21692250\n","Iteration 60, loss = 0.21664903\n","Iteration 61, loss = 0.21639965\n","Iteration 62, loss = 0.21606753\n","Iteration 63, loss = 0.21581977\n","Iteration 64, loss = 0.21555442\n","Iteration 65, loss = 0.21530925\n","Iteration 66, loss = 0.21503780\n","Iteration 67, loss = 0.21476248\n","Iteration 68, loss = 0.21454136\n","Iteration 69, loss = 0.21426293\n","Iteration 70, loss = 0.21404666\n","Iteration 71, loss = 0.21378661\n","Iteration 72, loss = 0.21355994\n","Iteration 73, loss = 0.21329656\n","Iteration 74, loss = 0.21307513\n","Iteration 75, loss = 0.21284253\n","Iteration 76, loss = 0.21262485\n","Iteration 77, loss = 0.21241883\n","Iteration 78, loss = 0.21218221\n","Iteration 79, loss = 0.21195242\n","Iteration 80, loss = 0.21174582\n","Iteration 81, loss = 0.21158621\n","Iteration 82, loss = 0.21131973\n","Iteration 83, loss = 0.21111481\n","Iteration 84, loss = 0.21096088\n","Iteration 85, loss = 0.21073217\n","Iteration 86, loss = 0.21053221\n","Iteration 87, loss = 0.21035119\n","Iteration 88, loss = 0.21018025\n","Iteration 89, loss = 0.21004514\n","Iteration 90, loss = 0.20976540\n","Iteration 91, loss = 0.20959044\n","Iteration 92, loss = 0.20944017\n","Iteration 93, loss = 0.20923408\n","Iteration 94, loss = 0.20906079\n","Iteration 95, loss = 0.20887521\n","Iteration 96, loss = 0.20870147\n","Iteration 97, loss = 0.20852999\n","Iteration 98, loss = 0.20837389\n","Iteration 99, loss = 0.20821021\n","Iteration 100, loss = 0.20802199\n","Iteration 101, loss = 0.20784674\n","Iteration 102, loss = 0.20765002\n","Iteration 103, loss = 0.20751427\n","Iteration 104, loss = 0.20735441\n","Iteration 105, loss = 0.20717450\n","Iteration 106, loss = 0.20700823\n","Iteration 107, loss = 0.20682717\n","Iteration 108, loss = 0.20669159\n","Iteration 109, loss = 0.20655582\n","Iteration 110, loss = 0.20638133\n","Iteration 111, loss = 0.20619554\n","Iteration 112, loss = 0.20606518\n","Iteration 113, loss = 0.20589307\n","Iteration 114, loss = 0.20573245\n","Iteration 115, loss = 0.20558791\n","Iteration 116, loss = 0.20545636\n","Iteration 117, loss = 0.20531689\n","Iteration 118, loss = 0.20513751\n","Iteration 119, loss = 0.20501145\n","Iteration 120, loss = 0.20489000\n","Iteration 121, loss = 0.20470002\n","Iteration 122, loss = 0.20452555\n","Iteration 123, loss = 0.20437245\n","Iteration 124, loss = 0.20425771\n","Iteration 125, loss = 0.20414643\n","Iteration 126, loss = 0.20397128\n","Iteration 127, loss = 0.20381327\n","Iteration 128, loss = 0.20367367\n","Iteration 129, loss = 0.20358206\n","Iteration 130, loss = 0.20342305\n","Iteration 131, loss = 0.20328949\n","Iteration 132, loss = 0.20316320\n","Iteration 133, loss = 0.20299019\n","Iteration 134, loss = 0.20285151\n","Iteration 135, loss = 0.20272801\n","Iteration 136, loss = 0.20261322\n","Iteration 137, loss = 0.20246400\n","Iteration 138, loss = 0.20230673\n","Iteration 139, loss = 0.20218405\n","Iteration 140, loss = 0.20204841\n","Iteration 141, loss = 0.20195452\n","Iteration 142, loss = 0.20179461\n","Iteration 143, loss = 0.20166721\n","Iteration 144, loss = 0.20152337\n","Iteration 145, loss = 0.20139628\n","Iteration 146, loss = 0.20130112\n","Iteration 147, loss = 0.20115112\n","Iteration 148, loss = 0.20102606\n","Iteration 149, loss = 0.20088122\n","Iteration 150, loss = 0.20075369\n","Iteration 1, loss = 0.81149390\n","Iteration 2, loss = 0.53978221\n","Iteration 3, loss = 0.43799151\n","Iteration 4, loss = 0.38180615"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration 5, loss = 0.34468411\n","Iteration 6, loss = 0.31852751\n","Iteration 7, loss = 0.30012671\n","Iteration 8, loss = 0.28663633\n","Iteration 9, loss = 0.27631142\n","Iteration 10, loss = 0.26833673\n","Iteration 11, loss = 0.26211522\n","Iteration 12, loss = 0.25697851\n","Iteration 13, loss = 0.25271131\n","Iteration 14, loss = 0.24915323\n","Iteration 15, loss = 0.24616290\n","Iteration 16, loss = 0.24337982\n","Iteration 17, loss = 0.24102922\n","Iteration 18, loss = 0.23892255\n","Iteration 19, loss = 0.23706636\n","Iteration 20, loss = 0.23535976\n","Iteration 21, loss = 0.23386916\n","Iteration 22, loss = 0.23246785\n","Iteration 23, loss = 0.23118839\n","Iteration 24, loss = 0.23001598\n","Iteration 25, loss = 0.22893581\n","Iteration 26, loss = 0.22798224\n","Iteration 27, loss = 0.22699189\n","Iteration 28, loss = 0.22614453\n","Iteration 29, loss = 0.22531935\n","Iteration 30, loss = 0.22454448\n","Iteration 31, loss = 0.22378439\n","Iteration 32, loss = 0.22310692\n","Iteration 33, loss = 0.22241052\n","Iteration 34, loss = 0.22178110\n","Iteration 35, loss = 0.22118104\n","Iteration 36, loss = 0.22059544\n","Iteration 37, loss = 0.22005852\n","Iteration 38, loss = 0.21947962\n","Iteration 39, loss = 0.21898055\n","Iteration 40, loss = 0.21846337\n","Iteration 41, loss = 0.21803576\n","Iteration 42, loss = 0.21755757\n","Iteration 43, loss = 0.21709377\n","Iteration 44, loss = 0.21672616\n","Iteration 45, loss = 0.21627991\n","Iteration 46, loss = 0.21585781\n","Iteration 47, loss = 0.21552635\n","Iteration 48, loss = 0.21509298\n","Iteration 49, loss = 0.21474049\n","Iteration 50, loss = 0.21439772\n","Iteration 51, loss = 0.21404702\n","Iteration 52, loss = 0.21371516\n","Iteration 53, loss = 0.21341419\n","Iteration 54, loss = 0.21315268\n","Iteration 55, loss = 0.21278309\n","Iteration 56, loss = 0.21249980\n","Iteration 57, loss = 0.21222880\n","Iteration 58, loss = 0.21190944\n","Iteration 59, loss = 0.21165365\n","Iteration 60, loss = 0.21139337\n","Iteration 61, loss = 0.21106418\n","Iteration 62, loss = 0.21082236\n","Iteration 63, loss = 0.21058412\n","Iteration 64, loss = 0.21037128\n","Iteration 65, loss = 0.21005104\n","Iteration 66, loss = 0.20977800\n","Iteration 67, loss = 0.20954058\n","Iteration 68, loss = 0.20931243\n","Iteration 69, loss = 0.20910976\n","Iteration 70, loss = 0.20883478\n","Iteration 71, loss = 0.20859376\n","Iteration 72, loss = 0.20837103\n","Iteration 73, loss = 0.20813126\n","Iteration 74, loss = 0.20788781\n","Iteration 75, loss = 0.20767549\n","Iteration 76, loss = 0.20744649\n","Iteration 77, loss = 0.20721682\n","Iteration 78, loss = 0.20697970\n","Iteration 79, loss = 0.20677121\n","Iteration 80, loss = 0.20655902\n","Iteration 81, loss = 0.20632875\n","Iteration 82, loss = 0.20611844\n","Iteration 83, loss = 0.20591614\n","Iteration 84, loss = 0.20569378\n","Iteration 85, loss = 0.20551050\n","Iteration 86, loss = 0.20531616\n","Iteration 87, loss = 0.20513695\n","Iteration 88, loss = 0.20493168\n","Iteration 89, loss = 0.20472714\n","Iteration 90, loss = 0.20449942\n","Iteration 91, loss = 0.20429916\n","Iteration 92, loss = 0.20411152\n","Iteration 93, loss = 0.20392519\n","Iteration 94, loss = 0.20375122\n","Iteration 95, loss = 0.20353165\n","Iteration 96, loss = 0.20332771\n","Iteration 97, loss = 0.20315581\n","Iteration 98, loss = 0.20302164\n","Iteration 99, loss = 0.20281585\n","Iteration 100, loss = 0.20261360\n","Iteration 101, loss = 0.20248258\n","Iteration 102, loss = 0.20223973\n","Iteration 103, loss = 0.20209503\n","Iteration 104, loss = 0.20192018\n","Iteration 105, loss = 0.20173624\n","Iteration 106, loss = 0.20160139\n","Iteration 107, loss = 0.20139396\n","Iteration 108, loss = 0.20123789\n","Iteration 109, loss = 0.20112680\n","Iteration 110, loss = 0.20089058\n","Iteration 111, loss = 0.20079697\n","Iteration 112, loss = 0.20057229\n","Iteration 113, loss = 0.20041639\n","Iteration 114, loss = 0.20024487\n","Iteration 115, loss = 0.20008055\n","Iteration 116, loss = 0.19992348\n","Iteration 117, loss = 0.19977461\n","Iteration 118, loss = 0.19959633\n","Iteration 119, loss = 0.19944750\n","Iteration 120, loss = 0.19928378\n","Iteration 121, loss = 0.19912090\n","Iteration 122, loss = 0.19897805\n","Iteration 123, loss = 0.19881938\n","Iteration 124, loss = 0.19868437\n","Iteration 125, loss = 0.19857506\n","Iteration 126, loss = 0.19839261\n","Iteration 127, loss = 0.19824075\n","Iteration 128, loss = 0.19809806\n","Iteration 129, loss = 0.19796152\n","Iteration 130, loss = 0.19784231\n","Iteration 131, loss = 0.19770281\n","Iteration 132, loss = 0.19755539\n","Iteration 133, loss = 0.19742600\n","Iteration 134, loss = 0.19731581\n","Iteration 135, loss = 0.19717272\n","Iteration 136, loss = 0.19702900\n","Iteration 137, loss = 0.19694508\n","Iteration 138, loss = 0.19677458\n","Iteration 139, loss = 0.19669052\n","Iteration 140, loss = 0.19651573\n","Iteration 141, loss = 0.19647462\n","Iteration 142, loss = 0.19626480\n","Iteration 143, loss = 0.19613902\n","Iteration 144, loss = 0.19606184\n","Iteration 145, loss = 0.19591496\n","Iteration 146, loss = 0.19581978\n","Iteration 147, loss = 0.19569648\n","Iteration 148, loss = 0.19555940\n","Iteration 149, loss = 0.19544837\n","Iteration 150, loss = 0.19533581\n","Iteration 1, loss = 0.80516026\n","Iteration 2, loss = 0.64670551\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.57313249\n","Iteration 4, loss = 0.51966016\n","Iteration 5, loss = 0.47273150\n","Iteration 6, loss = 0.42773963\n","Iteration 7, loss = 0.38706502\n","Iteration 8, loss = 0.35359521\n","Iteration 9, loss = 0.32721439\n","Iteration 10, loss = 0.30712672\n","Iteration 11, loss = 0.29142217\n","Iteration 12, loss = 0.27942355\n","Iteration 13, loss = 0.26998540\n","Iteration 14, loss = 0.26238807\n","Iteration 15, loss = 0.25624859\n","Iteration 16, loss = 0.25116762\n","Iteration 17, loss = 0.24679693\n","Iteration 18, loss = 0.24304851\n","Iteration 19, loss = 0.23979690\n","Iteration 20, loss = 0.23692506\n","Iteration 21, loss = 0.23443838\n","Iteration 22, loss = 0.23216617\n","Iteration 23, loss = 0.23029296\n","Iteration 24, loss = 0.22845076\n","Iteration 25, loss = 0.22686916\n","Iteration 26, loss = 0.22543972\n","Iteration 27, loss = 0.22413905\n","Iteration 28, loss = 0.22288747\n","Iteration 29, loss = 0.22180385\n","Iteration 30, loss = 0.22073211\n","Iteration 31, loss = 0.21981762\n","Iteration 32, loss = 0.21892310\n","Iteration 33, loss = 0.21811319\n","Iteration 34, loss = 0.21732555\n","Iteration 35, loss = 0.21662766\n","Iteration 36, loss = 0.21596768\n","Iteration 37, loss = 0.21534184\n","Iteration 38, loss = 0.21475384\n","Iteration 39, loss = 0.21424639\n","Iteration 40, loss = 0.21372220\n","Iteration 41, loss = 0.21321238\n","Iteration 42, loss = 0.21271097\n","Iteration 43, loss = 0.21230673\n","Iteration 44, loss = 0.21184265\n","Iteration 45, loss = 0.21142872\n","Iteration 46, loss = 0.21103459\n","Iteration 47, loss = 0.21064746\n","Iteration 48, loss = 0.21027729\n","Iteration 49, loss = 0.20991349\n","Iteration 50, loss = 0.20954561\n","Iteration 51, loss = 0.20922419\n","Iteration 52, loss = 0.20888140\n","Iteration 53, loss = 0.20854817\n","Iteration 54, loss = 0.20826666\n","Iteration 55, loss = 0.20795108\n","Iteration 56, loss = 0.20764257\n","Iteration 57, loss = 0.20738172\n","Iteration 58, loss = 0.20709135\n","Iteration 59, loss = 0.20681763\n","Iteration 60, loss = 0.20651187\n","Iteration 61, loss = 0.20622535\n","Iteration 62, loss = 0.20597432\n","Iteration 63, loss = 0.20573070\n","Iteration 64, loss = 0.20544128\n","Iteration 65, loss = 0.20520594\n","Iteration 66, loss = 0.20498831\n","Iteration 67, loss = 0.20467367\n","Iteration 68, loss = 0.20442570\n","Iteration 69, loss = 0.20422646\n","Iteration 70, loss = 0.20393413\n","Iteration 71, loss = 0.20370318\n","Iteration 72, loss = 0.20348060\n","Iteration 73, loss = 0.20325702\n","Iteration 74, loss = 0.20306365\n","Iteration 75, loss = 0.20279793\n","Iteration 76, loss = 0.20258834\n","Iteration 77, loss = 0.20236710\n","Iteration 78, loss = 0.20218174\n","Iteration 79, loss = 0.20191791\n","Iteration 80, loss = 0.20170523\n","Iteration 81, loss = 0.20148489\n","Iteration 82, loss = 0.20130296\n","Iteration 83, loss = 0.20110387\n","Iteration 84, loss = 0.20093122\n","Iteration 85, loss = 0.20069360\n","Iteration 86, loss = 0.20046849\n","Iteration 87, loss = 0.20026359\n","Iteration 88, loss = 0.20012479\n","Iteration 89, loss = 0.19986942\n","Iteration 90, loss = 0.19970622\n","Iteration 91, loss = 0.19948332\n","Iteration 92, loss = 0.19930963\n","Iteration 93, loss = 0.19913466\n","Iteration 94, loss = 0.19892052\n","Iteration 95, loss = 0.19873222\n","Iteration 96, loss = 0.19856007\n","Iteration 97, loss = 0.19835450\n","Iteration 98, loss = 0.19817580\n","Iteration 99, loss = 0.19798915\n","Iteration 100, loss = 0.19780179\n","Iteration 101, loss = 0.19770927\n","Iteration 102, loss = 0.19748216\n","Iteration 103, loss = 0.19729426\n","Iteration 104, loss = 0.19712161\n","Iteration 105, loss = 0.19694369\n","Iteration 106, loss = 0.19680068\n","Iteration 107, loss = 0.19662059\n","Iteration 108, loss = 0.19643278\n","Iteration 109, loss = 0.19628398\n","Iteration 110, loss = 0.19615638\n","Iteration 111, loss = 0.19593399\n","Iteration 112, loss = 0.19577187\n","Iteration 113, loss = 0.19566802\n","Iteration 114, loss = 0.19545419\n","Iteration 115, loss = 0.19534509\n","Iteration 116, loss = 0.19515330\n","Iteration 117, loss = 0.19501513\n","Iteration 118, loss = 0.19484718\n","Iteration 119, loss = 0.19469873\n","Iteration 120, loss = 0.19455070\n","Iteration 121, loss = 0.19437911\n","Iteration 122, loss = 0.19425375\n","Iteration 123, loss = 0.19408480\n","Iteration 124, loss = 0.19391186\n","Iteration 125, loss = 0.19381138\n","Iteration 126, loss = 0.19370244\n","Iteration 127, loss = 0.19350683\n","Iteration 128, loss = 0.19337675\n","Iteration 129, loss = 0.19322091\n","Iteration 130, loss = 0.19306973\n","Iteration 131, loss = 0.19293074\n","Iteration 132, loss = 0.19282202\n","Iteration 133, loss = 0.19266157\n","Iteration 134, loss = 0.19253760\n","Iteration 135, loss = 0.19235875\n","Iteration 136, loss = 0.19222913\n","Iteration 137, loss = 0.19212294\n","Iteration 138, loss = 0.19192684\n","Iteration 139, loss = 0.19181233\n","Iteration 140, loss = 0.19167005\n","Iteration 141, loss = 0.19153018\n","Iteration 142, loss = 0.19137601\n","Iteration 143, loss = 0.19123930\n","Iteration 144, loss = 0.19113641\n","Iteration 145, loss = 0.19099629\n","Iteration 146, loss = 0.19084408\n","Iteration 147, loss = 0.19073318\n","Iteration 148, loss = 0.19059750\n","Iteration 149, loss = 0.19041816\n","Iteration 150, loss = 0.19027860\n","Iteration 1, loss = 0.61294293\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.55544432\n","Iteration 3, loss = 0.49981472\n","Iteration 4, loss = 0.44618558\n","Iteration 5, loss = 0.39828821\n","Iteration 6, loss = 0.35872106\n","Iteration 7, loss = 0.32845459\n","Iteration 8, loss = 0.30583644\n","Iteration 9, loss = 0.28914098\n","Iteration 10, loss = 0.27677086\n","Iteration 11, loss = 0.26730167\n","Iteration 12, loss = 0.25993728\n","Iteration 13, loss = 0.25419163\n","Iteration 14, loss = 0.24946750\n","Iteration 15, loss = 0.24567123\n","Iteration 16, loss = 0.24243020\n","Iteration 17, loss = 0.23965338\n","Iteration 18, loss = 0.23728447\n","Iteration 19, loss = 0.23514200\n","Iteration 20, loss = 0.23320451\n","Iteration 21, loss = 0.23150359\n","Iteration 22, loss = 0.22996175\n","Iteration 23, loss = 0.22854603\n","Iteration 24, loss = 0.22729490\n","Iteration 25, loss = 0.22617185\n","Iteration 26, loss = 0.22509387\n","Iteration 27, loss = 0.22407325\n","Iteration 28, loss = 0.22316507\n","Iteration 29, loss = 0.22228624\n","Iteration 30, loss = 0.22147317\n","Iteration 31, loss = 0.22072148\n","Iteration 32, loss = 0.21998762\n","Iteration 33, loss = 0.21928274\n","Iteration 34, loss = 0.21863119\n","Iteration 35, loss = 0.21799309\n","Iteration 36, loss = 0.21736323\n","Iteration 37, loss = 0.21676885\n","Iteration 38, loss = 0.21621829\n","Iteration 39, loss = 0.21567604\n","Iteration 40, loss = 0.21515422\n","Iteration 41, loss = 0.21463354\n","Iteration 42, loss = 0.21415937\n","Iteration 43, loss = 0.21367438\n","Iteration 44, loss = 0.21327235\n","Iteration 45, loss = 0.21276371\n","Iteration 46, loss = 0.21236359\n","Iteration 47, loss = 0.21191170\n","Iteration 48, loss = 0.21153014\n","Iteration 49, loss = 0.21113516\n","Iteration 50, loss = 0.21076028\n","Iteration 51, loss = 0.21038672\n","Iteration 52, loss = 0.21003110\n","Iteration 53, loss = 0.20967822\n","Iteration 54, loss = 0.20931636\n","Iteration 55, loss = 0.20899280\n","Iteration 56, loss = 0.20868336\n","Iteration 57, loss = 0.20839234\n","Iteration 58, loss = 0.20806991\n","Iteration 59, loss = 0.20777627\n","Iteration 60, loss = 0.20749777\n","Iteration 61, loss = 0.20720419\n","Iteration 62, loss = 0.20696962\n","Iteration 63, loss = 0.20668416\n","Iteration 64, loss = 0.20642571\n","Iteration 65, loss = 0.20616722\n","Iteration 66, loss = 0.20590254\n","Iteration 67, loss = 0.20565160\n","Iteration 68, loss = 0.20540212\n","Iteration 69, loss = 0.20515521\n","Iteration 70, loss = 0.20490669\n","Iteration 71, loss = 0.20468845\n","Iteration 72, loss = 0.20446004\n","Iteration 73, loss = 0.20422933\n","Iteration 74, loss = 0.20399696\n","Iteration 75, loss = 0.20375980\n","Iteration 76, loss = 0.20353952\n","Iteration 77, loss = 0.20333899\n","Iteration 78, loss = 0.20315539\n","Iteration 79, loss = 0.20290742\n","Iteration 80, loss = 0.20267413\n","Iteration 81, loss = 0.20248891\n","Iteration 82, loss = 0.20229596\n","Iteration 83, loss = 0.20206573\n","Iteration 84, loss = 0.20184755\n","Iteration 85, loss = 0.20164329\n","Iteration 86, loss = 0.20145453\n","Iteration 87, loss = 0.20125022\n","Iteration 88, loss = 0.20105635\n","Iteration 89, loss = 0.20085475\n","Iteration 90, loss = 0.20066983\n","Iteration 91, loss = 0.20048002\n","Iteration 92, loss = 0.20027735\n","Iteration 93, loss = 0.20011259\n","Iteration 94, loss = 0.19991508\n","Iteration 95, loss = 0.19976702\n","Iteration 96, loss = 0.19954372\n","Iteration 97, loss = 0.19939831\n","Iteration 98, loss = 0.19923491\n","Iteration 99, loss = 0.19903716\n","Iteration 100, loss = 0.19887583\n","Iteration 101, loss = 0.19870544\n","Iteration 102, loss = 0.19854716\n","Iteration 103, loss = 0.19840733\n","Iteration 104, loss = 0.19822430\n","Iteration 105, loss = 0.19806199\n","Iteration 106, loss = 0.19789687\n","Iteration 107, loss = 0.19776304\n","Iteration 108, loss = 0.19760221\n","Iteration 109, loss = 0.19744400\n","Iteration 110, loss = 0.19729597\n","Iteration 111, loss = 0.19713502\n","Iteration 112, loss = 0.19701587\n","Iteration 113, loss = 0.19685116\n","Iteration 114, loss = 0.19667474\n","Iteration 115, loss = 0.19656514\n","Iteration 116, loss = 0.19642198\n","Iteration 117, loss = 0.19624669\n","Iteration 118, loss = 0.19612589\n","Iteration 119, loss = 0.19597378\n","Iteration 120, loss = 0.19586510\n","Iteration 121, loss = 0.19570564\n","Iteration 122, loss = 0.19557217\n","Iteration 123, loss = 0.19540952\n","Iteration 124, loss = 0.19533121\n","Iteration 125, loss = 0.19519185\n","Iteration 126, loss = 0.19504387\n","Iteration 127, loss = 0.19490449\n","Iteration 128, loss = 0.19477641\n","Iteration 129, loss = 0.19462180\n","Iteration 130, loss = 0.19449687\n","Iteration 131, loss = 0.19438429\n","Iteration 132, loss = 0.19424012\n","Iteration 133, loss = 0.19413937\n","Iteration 134, loss = 0.19397563\n","Iteration 135, loss = 0.19383796\n","Iteration 136, loss = 0.19374526\n","Iteration 137, loss = 0.19360962\n","Iteration 138, loss = 0.19345234\n","Iteration 139, loss = 0.19332976\n","Iteration 140, loss = 0.19323250\n","Iteration 141, loss = 0.19315079\n","Iteration 142, loss = 0.19298705\n","Iteration 143, loss = 0.19289249\n","Iteration 144, loss = 0.19276106\n","Iteration 145, loss = 0.19265595\n","Iteration 146, loss = 0.19249994\n","Iteration 147, loss = 0.19241400\n","Iteration 148, loss = 0.19230878\n","Iteration 149, loss = 0.19217515\n","Iteration 150, loss = 0.19205734\n","Iteration 1, loss = 0.87302430\n","Iteration 2, loss = 0.50464123\n","Iteration 3, loss = 0.39912260\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.34968958\n","Iteration 5, loss = 0.31788653\n","Iteration 6, loss = 0.29656783\n","Iteration 7, loss = 0.28165129\n","Iteration 8, loss = 0.27084629\n","Iteration 9, loss = 0.26260543\n","Iteration 10, loss = 0.25628531\n","Iteration 11, loss = 0.25120258\n","Iteration 12, loss = 0.24708834\n","Iteration 13, loss = 0.24362288\n","Iteration 14, loss = 0.24075362\n","Iteration 15, loss = 0.23822545\n","Iteration 16, loss = 0.23609227\n","Iteration 17, loss = 0.23414903\n","Iteration 18, loss = 0.23243243\n","Iteration 19, loss = 0.23094417\n","Iteration 20, loss = 0.22958850\n","Iteration 21, loss = 0.22840428\n","Iteration 22, loss = 0.22726403\n","Iteration 23, loss = 0.22622900\n","Iteration 24, loss = 0.22526820\n","Iteration 25, loss = 0.22450788\n","Iteration 26, loss = 0.22367608\n","Iteration 27, loss = 0.22290100\n","Iteration 28, loss = 0.22217542\n","Iteration 29, loss = 0.22154762\n","Iteration 30, loss = 0.22092991\n","Iteration 31, loss = 0.22037310\n","Iteration 32, loss = 0.21977899\n","Iteration 33, loss = 0.21924428\n","Iteration 34, loss = 0.21875948\n","Iteration 35, loss = 0.21827462\n","Iteration 36, loss = 0.21774193\n","Iteration 37, loss = 0.21727649\n","Iteration 38, loss = 0.21688208\n","Iteration 39, loss = 0.21645671\n","Iteration 40, loss = 0.21603622\n","Iteration 41, loss = 0.21559783\n","Iteration 42, loss = 0.21524807\n","Iteration 43, loss = 0.21486129\n","Iteration 44, loss = 0.21447007\n","Iteration 45, loss = 0.21414626\n","Iteration 46, loss = 0.21378382\n","Iteration 47, loss = 0.21343142\n","Iteration 48, loss = 0.21310573\n","Iteration 49, loss = 0.21273022\n","Iteration 50, loss = 0.21245419\n","Iteration 51, loss = 0.21213732\n","Iteration 52, loss = 0.21182777\n","Iteration 53, loss = 0.21150530\n","Iteration 54, loss = 0.21123576\n","Iteration 55, loss = 0.21095134\n","Iteration 56, loss = 0.21068375\n","Iteration 57, loss = 0.21041048\n","Iteration 58, loss = 0.21014871\n","Iteration 59, loss = 0.20988788\n","Iteration 60, loss = 0.20963188\n","Iteration 61, loss = 0.20942355\n","Iteration 62, loss = 0.20912682\n","Iteration 63, loss = 0.20888325\n","Iteration 64, loss = 0.20864968\n","Iteration 65, loss = 0.20838538\n","Iteration 66, loss = 0.20825323\n","Iteration 67, loss = 0.20795402\n","Iteration 68, loss = 0.20769644\n","Iteration 69, loss = 0.20747866\n","Iteration 70, loss = 0.20730507\n","Iteration 71, loss = 0.20705005\n","Iteration 72, loss = 0.20687640\n","Iteration 73, loss = 0.20663188\n","Iteration 74, loss = 0.20642693\n","Iteration 75, loss = 0.20625535\n","Iteration 76, loss = 0.20602156\n","Iteration 77, loss = 0.20584054\n","Iteration 78, loss = 0.20563546\n","Iteration 79, loss = 0.20542799\n","Iteration 80, loss = 0.20523305\n","Iteration 81, loss = 0.20503805\n","Iteration 82, loss = 0.20483141\n","Iteration 83, loss = 0.20465726\n","Iteration 84, loss = 0.20443809\n","Iteration 85, loss = 0.20430321\n","Iteration 86, loss = 0.20410049\n","Iteration 87, loss = 0.20392937\n","Iteration 88, loss = 0.20373569\n","Iteration 89, loss = 0.20356607\n","Iteration 90, loss = 0.20342474\n","Iteration 91, loss = 0.20322137\n","Iteration 92, loss = 0.20309684\n","Iteration 93, loss = 0.20289885\n","Iteration 94, loss = 0.20273862\n","Iteration 95, loss = 0.20258664\n","Iteration 96, loss = 0.20242794\n","Iteration 97, loss = 0.20227207\n","Iteration 98, loss = 0.20211053\n","Iteration 99, loss = 0.20199575\n","Iteration 100, loss = 0.20182983\n","Iteration 101, loss = 0.20167157\n","Iteration 102, loss = 0.20150340\n","Iteration 103, loss = 0.20136187\n","Iteration 104, loss = 0.20125336\n","Iteration 105, loss = 0.20109056\n","Iteration 106, loss = 0.20095602\n","Iteration 107, loss = 0.20082076\n","Iteration 108, loss = 0.20066235\n","Iteration 109, loss = 0.20053521\n","Iteration 110, loss = 0.20039286\n","Iteration 111, loss = 0.20027643\n","Iteration 112, loss = 0.20010516\n","Iteration 113, loss = 0.19999210\n","Iteration 114, loss = 0.19986124\n","Iteration 115, loss = 0.19969792\n","Iteration 116, loss = 0.19958279\n","Iteration 117, loss = 0.19945324\n","Iteration 118, loss = 0.19931406\n","Iteration 119, loss = 0.19920606\n","Iteration 120, loss = 0.19908720\n","Iteration 121, loss = 0.19895097\n","Iteration 122, loss = 0.19879574\n","Iteration 123, loss = 0.19867126\n","Iteration 124, loss = 0.19862334\n","Iteration 125, loss = 0.19840649\n","Iteration 126, loss = 0.19829139\n","Iteration 127, loss = 0.19815800\n","Iteration 128, loss = 0.19801628\n","Iteration 129, loss = 0.19788774\n","Iteration 130, loss = 0.19777051\n","Iteration 131, loss = 0.19767323\n","Iteration 132, loss = 0.19751512\n","Iteration 133, loss = 0.19741058\n","Iteration 134, loss = 0.19729535\n","Iteration 135, loss = 0.19725585\n","Iteration 136, loss = 0.19706078\n","Iteration 137, loss = 0.19693424\n","Iteration 138, loss = 0.19681044\n","Iteration 139, loss = 0.19669614\n","Iteration 140, loss = 0.19660300\n","Iteration 141, loss = 0.19649602\n","Iteration 142, loss = 0.19636262\n","Iteration 143, loss = 0.19625912\n","Iteration 144, loss = 0.19613273\n","Iteration 145, loss = 0.19600511\n","Iteration 146, loss = 0.19587118\n","Iteration 147, loss = 0.19577177\n","Iteration 148, loss = 0.19567198\n","Iteration 149, loss = 0.19551508\n","Iteration 150, loss = 0.19546885\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.44014689\n","Iteration 2, loss = 0.26275608\n","Iteration 3, loss = 0.23504308\n","Iteration 4, loss = 0.22391425\n","Iteration 5, loss = 0.21712181\n","Iteration 6, loss = 0.21208640\n","Iteration 7, loss = 0.20834435\n","Iteration 8, loss = 0.20491131\n","Iteration 9, loss = 0.20295258\n","Iteration 10, loss = 0.20061745\n","Iteration 11, loss = 0.19736071\n","Iteration 12, loss = 0.19561599\n","Iteration 13, loss = 0.19378447\n","Iteration 14, loss = 0.19127942\n","Iteration 15, loss = 0.18934780\n","Iteration 16, loss = 0.18751715\n","Iteration 17, loss = 0.18564863\n","Iteration 18, loss = 0.18376678\n","Iteration 19, loss = 0.18266889\n","Iteration 20, loss = 0.18052751\n","Iteration 21, loss = 0.17907236\n","Iteration 22, loss = 0.17756966\n","Iteration 23, loss = 0.17629147\n","Iteration 24, loss = 0.17514808\n","Iteration 25, loss = 0.17373563\n","Iteration 26, loss = 0.17170972\n","Iteration 27, loss = 0.16990712\n","Iteration 28, loss = 0.16825519\n","Iteration 29, loss = 0.16744272\n","Iteration 30, loss = 0.16589494\n","Iteration 31, loss = 0.16385580\n","Iteration 32, loss = 0.16299549\n","Iteration 33, loss = 0.16084071\n","Iteration 34, loss = 0.15933119\n","Iteration 35, loss = 0.15852169\n","Iteration 36, loss = 0.15749283\n","Iteration 37, loss = 0.15582404\n","Iteration 38, loss = 0.15386855\n","Iteration 39, loss = 0.15245794\n","Iteration 40, loss = 0.15060685\n","Iteration 41, loss = 0.15027399\n","Iteration 42, loss = 0.14870435\n","Iteration 43, loss = 0.14631172\n","Iteration 44, loss = 0.14621883\n","Iteration 45, loss = 0.14410361\n","Iteration 46, loss = 0.14207665\n","Iteration 47, loss = 0.14086031\n","Iteration 48, loss = 0.13974130\n","Iteration 49, loss = 0.13842797\n","Iteration 50, loss = 0.13652072\n","Iteration 51, loss = 0.13550790\n","Iteration 52, loss = 0.13400477\n","Iteration 53, loss = 0.13280154\n","Iteration 54, loss = 0.13179988\n","Iteration 55, loss = 0.12984479\n","Iteration 56, loss = 0.12791968\n","Iteration 57, loss = 0.12676331\n","Iteration 58, loss = 0.12526836\n","Iteration 59, loss = 0.12386745\n","Iteration 60, loss = 0.12299536\n","Iteration 61, loss = 0.12228769\n","Iteration 62, loss = 0.12000951\n","Iteration 63, loss = 0.11918494\n","Iteration 64, loss = 0.11852337\n","Iteration 65, loss = 0.11761148\n","Iteration 66, loss = 0.11701814\n","Iteration 67, loss = 0.11456440\n","Iteration 68, loss = 0.11424819\n","Iteration 69, loss = 0.11262694\n","Iteration 70, loss = 0.11215043\n","Iteration 71, loss = 0.11043570\n","Iteration 72, loss = 0.10890394\n","Iteration 73, loss = 0.10866038\n","Iteration 74, loss = 0.10755236\n","Iteration 75, loss = 0.10616665\n","Iteration 76, loss = 0.10603664\n","Iteration 77, loss = 0.10405439\n","Iteration 78, loss = 0.10351365\n","Iteration 79, loss = 0.10280936\n","Iteration 80, loss = 0.10182792\n","Iteration 81, loss = 0.10022352\n","Iteration 82, loss = 0.09962833\n","Iteration 83, loss = 0.09810602\n","Iteration 84, loss = 0.09755141\n","Iteration 85, loss = 0.09801491\n","Iteration 86, loss = 0.09574896\n","Iteration 87, loss = 0.09518629\n","Iteration 88, loss = 0.09402215\n","Iteration 89, loss = 0.09410649\n","Iteration 90, loss = 0.09279147\n","Iteration 91, loss = 0.09197748\n","Iteration 92, loss = 0.09187029\n","Iteration 93, loss = 0.08977399\n","Iteration 94, loss = 0.08823890\n","Iteration 95, loss = 0.08852646\n","Iteration 96, loss = 0.08758992\n","Iteration 97, loss = 0.08663568\n","Iteration 98, loss = 0.08614471\n","Iteration 99, loss = 0.08497695\n","Iteration 100, loss = 0.08409644\n","Iteration 101, loss = 0.08437559\n","Iteration 102, loss = 0.08283580\n","Iteration 103, loss = 0.08159843\n","Iteration 104, loss = 0.08065253\n","Iteration 105, loss = 0.07900522\n","Iteration 106, loss = 0.08004619\n","Iteration 107, loss = 0.07850631\n","Iteration 108, loss = 0.07795007\n","Iteration 109, loss = 0.07749111\n","Iteration 110, loss = 0.07705121\n","Iteration 111, loss = 0.07565506\n","Iteration 112, loss = 0.07519153\n","Iteration 113, loss = 0.07493714\n","Iteration 114, loss = 0.07581975\n","Iteration 115, loss = 0.07390053\n","Iteration 116, loss = 0.07233015\n","Iteration 117, loss = 0.07208382\n","Iteration 118, loss = 0.07218001\n","Iteration 119, loss = 0.07071122\n","Iteration 120, loss = 0.06927041\n","Iteration 121, loss = 0.06888523\n","Iteration 122, loss = 0.06888340\n","Iteration 123, loss = 0.06746760\n","Iteration 124, loss = 0.06750703\n","Iteration 125, loss = 0.06668205\n","Iteration 126, loss = 0.06713231\n","Iteration 127, loss = 0.06557958\n","Iteration 128, loss = 0.06525242\n","Iteration 129, loss = 0.06522325\n","Iteration 130, loss = 0.06425010\n","Iteration 131, loss = 0.06262123\n","Iteration 132, loss = 0.06198902\n","Iteration 133, loss = 0.06161301\n","Iteration 134, loss = 0.06109167\n","Iteration 135, loss = 0.06084327\n","Iteration 136, loss = 0.05991400\n","Iteration 137, loss = 0.05925119\n","Iteration 138, loss = 0.05966492\n","Iteration 139, loss = 0.05822423\n","Iteration 140, loss = 0.05748083\n","Iteration 141, loss = 0.05779122\n","Iteration 142, loss = 0.05686078\n","Iteration 143, loss = 0.05627133\n","Iteration 144, loss = 0.05655350\n","Iteration 145, loss = 0.05530724\n","Iteration 146, loss = 0.05545812\n","Iteration 147, loss = 0.05425835\n","Iteration 148, loss = 0.05436912\n","Iteration 149, loss = 0.05366553\n","Iteration 150, loss = 0.05258327\n","Iteration 1, loss = 0.57895380\n","Iteration 2, loss = 0.31202010\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.24610613\n","Iteration 4, loss = 0.22684174\n","Iteration 5, loss = 0.21750890\n","Iteration 6, loss = 0.21196772\n","Iteration 7, loss = 0.20810228\n","Iteration 8, loss = 0.20492325\n","Iteration 9, loss = 0.20197131\n","Iteration 10, loss = 0.19935836\n","Iteration 11, loss = 0.19720204\n","Iteration 12, loss = 0.19515940\n","Iteration 13, loss = 0.19271500\n","Iteration 14, loss = 0.19094478\n","Iteration 15, loss = 0.18912422\n","Iteration 16, loss = 0.18722827\n","Iteration 17, loss = 0.18498306\n","Iteration 18, loss = 0.18329872\n","Iteration 19, loss = 0.18160525\n","Iteration 20, loss = 0.17985129\n","Iteration 21, loss = 0.17813484\n","Iteration 22, loss = 0.17648315\n","Iteration 23, loss = 0.17511342\n","Iteration 24, loss = 0.17360369\n","Iteration 25, loss = 0.17201177\n","Iteration 26, loss = 0.17032743\n","Iteration 27, loss = 0.16893868\n","Iteration 28, loss = 0.16744475\n","Iteration 29, loss = 0.16622363\n","Iteration 30, loss = 0.16421580\n","Iteration 31, loss = 0.16308780\n","Iteration 32, loss = 0.16146474\n","Iteration 33, loss = 0.16044422\n","Iteration 34, loss = 0.15893864\n","Iteration 35, loss = 0.15764350\n","Iteration 36, loss = 0.15582782\n","Iteration 37, loss = 0.15444458\n","Iteration 38, loss = 0.15308040\n","Iteration 39, loss = 0.15224978\n","Iteration 40, loss = 0.15083777\n","Iteration 41, loss = 0.14948123\n","Iteration 42, loss = 0.14815852\n","Iteration 43, loss = 0.14815900\n","Iteration 44, loss = 0.14626691\n","Iteration 45, loss = 0.14439431\n","Iteration 46, loss = 0.14375430\n","Iteration 47, loss = 0.14246547\n","Iteration 48, loss = 0.14141955\n","Iteration 49, loss = 0.13951023\n","Iteration 50, loss = 0.13903653\n","Iteration 51, loss = 0.13779251\n","Iteration 52, loss = 0.13596520\n","Iteration 53, loss = 0.13507319\n","Iteration 54, loss = 0.13334221\n","Iteration 55, loss = 0.13212261\n","Iteration 56, loss = 0.13120814\n","Iteration 57, loss = 0.12992988\n","Iteration 58, loss = 0.12912451\n","Iteration 59, loss = 0.12807574\n","Iteration 60, loss = 0.12623275\n","Iteration 61, loss = 0.12503541\n","Iteration 62, loss = 0.12476384\n","Iteration 63, loss = 0.12363144\n","Iteration 64, loss = 0.12214917\n","Iteration 65, loss = 0.12105878\n","Iteration 66, loss = 0.12012368\n","Iteration 67, loss = 0.11896657\n","Iteration 68, loss = 0.11894060\n","Iteration 69, loss = 0.11757554\n","Iteration 70, loss = 0.11715042\n","Iteration 71, loss = 0.11564841\n","Iteration 72, loss = 0.11427347\n","Iteration 73, loss = 0.11316145\n","Iteration 74, loss = 0.11228431\n","Iteration 75, loss = 0.11160911\n","Iteration 76, loss = 0.11082060\n","Iteration 77, loss = 0.11091959\n","Iteration 78, loss = 0.10861124\n","Iteration 79, loss = 0.10863581\n","Iteration 80, loss = 0.10760692\n","Iteration 81, loss = 0.10694010\n","Iteration 82, loss = 0.10508831\n","Iteration 83, loss = 0.10490731\n","Iteration 84, loss = 0.10388415\n","Iteration 85, loss = 0.10313114\n","Iteration 86, loss = 0.10213653\n","Iteration 87, loss = 0.10205429\n","Iteration 88, loss = 0.10035630\n","Iteration 89, loss = 0.09983469\n","Iteration 90, loss = 0.09887252\n","Iteration 91, loss = 0.09863817\n","Iteration 92, loss = 0.09783195\n","Iteration 93, loss = 0.09673930\n","Iteration 94, loss = 0.09545212\n","Iteration 95, loss = 0.09490925\n","Iteration 96, loss = 0.09394690\n","Iteration 97, loss = 0.09344174\n","Iteration 98, loss = 0.09255385\n","Iteration 99, loss = 0.09152866\n","Iteration 100, loss = 0.09060733\n","Iteration 101, loss = 0.09030635\n","Iteration 102, loss = 0.09006221\n","Iteration 103, loss = 0.08827911\n","Iteration 104, loss = 0.08699153\n","Iteration 105, loss = 0.08672728\n","Iteration 106, loss = 0.08625321\n","Iteration 107, loss = 0.08496319\n","Iteration 108, loss = 0.08456702\n","Iteration 109, loss = 0.08262376\n","Iteration 110, loss = 0.08259508\n","Iteration 111, loss = 0.08239289\n","Iteration 112, loss = 0.08108864\n","Iteration 113, loss = 0.08049097\n","Iteration 114, loss = 0.07917046\n","Iteration 115, loss = 0.07803993\n","Iteration 116, loss = 0.07796083\n","Iteration 117, loss = 0.07692416\n","Iteration 118, loss = 0.07656437\n","Iteration 119, loss = 0.07597935\n","Iteration 120, loss = 0.07509400\n","Iteration 121, loss = 0.07418997\n","Iteration 122, loss = 0.07329389\n","Iteration 123, loss = 0.07247528\n","Iteration 124, loss = 0.07117997\n","Iteration 125, loss = 0.07095385\n","Iteration 126, loss = 0.07013208\n","Iteration 127, loss = 0.06957089\n","Iteration 128, loss = 0.06969509\n","Iteration 129, loss = 0.06832857\n","Iteration 130, loss = 0.06736388\n","Iteration 131, loss = 0.06668282\n","Iteration 132, loss = 0.06634174\n","Iteration 133, loss = 0.06579769\n","Iteration 134, loss = 0.06533689\n","Iteration 135, loss = 0.06370362\n","Iteration 136, loss = 0.06391821\n","Iteration 137, loss = 0.06302295\n","Iteration 138, loss = 0.06219568\n","Iteration 139, loss = 0.06135689\n","Iteration 140, loss = 0.06002997\n","Iteration 141, loss = 0.06056345\n","Iteration 142, loss = 0.05928016\n","Iteration 143, loss = 0.05926682\n","Iteration 144, loss = 0.05831338\n","Iteration 145, loss = 0.05873811\n","Iteration 146, loss = 0.05815774\n","Iteration 147, loss = 0.05675773\n","Iteration 148, loss = 0.05593228\n","Iteration 149, loss = 0.05543844\n","Iteration 150, loss = 0.05506007\n","Iteration 1, loss = 0.42760221\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.25410108\n","Iteration 3, loss = 0.22315239\n","Iteration 4, loss = 0.21308077\n","Iteration 5, loss = 0.20697403\n","Iteration 6, loss = 0.20271244\n","Iteration 7, loss = 0.19999177\n","Iteration 8, loss = 0.19698471\n","Iteration 9, loss = 0.19451325\n","Iteration 10, loss = 0.19295148\n","Iteration 11, loss = 0.19067518\n","Iteration 12, loss = 0.18914555\n","Iteration 13, loss = 0.18643901\n","Iteration 14, loss = 0.18456151\n","Iteration 15, loss = 0.18305098\n","Iteration 16, loss = 0.18137355\n","Iteration 17, loss = 0.17950793\n","Iteration 18, loss = 0.17771526\n","Iteration 19, loss = 0.17623431\n","Iteration 20, loss = 0.17433881\n","Iteration 21, loss = 0.17226209\n","Iteration 22, loss = 0.17062110\n","Iteration 23, loss = 0.16834358\n","Iteration 24, loss = 0.16676243\n","Iteration 25, loss = 0.16602564\n","Iteration 26, loss = 0.16343064\n","Iteration 27, loss = 0.16172656\n","Iteration 28, loss = 0.16021605\n","Iteration 29, loss = 0.15858553\n","Iteration 30, loss = 0.15714427\n","Iteration 31, loss = 0.15632916\n","Iteration 32, loss = 0.15377751\n","Iteration 33, loss = 0.15260323\n","Iteration 34, loss = 0.15217643\n","Iteration 35, loss = 0.15019867\n","Iteration 36, loss = 0.14850229\n","Iteration 37, loss = 0.14693840\n","Iteration 38, loss = 0.14596795\n","Iteration 39, loss = 0.14468889\n","Iteration 40, loss = 0.14369933\n","Iteration 41, loss = 0.14213441\n","Iteration 42, loss = 0.14064689\n","Iteration 43, loss = 0.13911465\n","Iteration 44, loss = 0.13784067\n","Iteration 45, loss = 0.13683101\n","Iteration 46, loss = 0.13525362\n","Iteration 47, loss = 0.13541539\n","Iteration 48, loss = 0.13287600\n","Iteration 49, loss = 0.13150535\n","Iteration 50, loss = 0.13061416\n","Iteration 51, loss = 0.12880098\n","Iteration 52, loss = 0.12750964\n","Iteration 53, loss = 0.12668901\n","Iteration 54, loss = 0.12510350\n","Iteration 55, loss = 0.12468045\n","Iteration 56, loss = 0.12261771\n","Iteration 57, loss = 0.12201857\n","Iteration 58, loss = 0.12021340\n","Iteration 59, loss = 0.11895585\n","Iteration 60, loss = 0.11833219\n","Iteration 61, loss = 0.11817293\n","Iteration 62, loss = 0.11670675\n","Iteration 63, loss = 0.11500291\n","Iteration 64, loss = 0.11383678\n","Iteration 65, loss = 0.11304265\n","Iteration 66, loss = 0.11184799\n","Iteration 67, loss = 0.11120943\n","Iteration 68, loss = 0.10948206\n","Iteration 69, loss = 0.10918819\n","Iteration 70, loss = 0.10762183\n","Iteration 71, loss = 0.10632466\n","Iteration 72, loss = 0.10569803\n","Iteration 73, loss = 0.10390711\n","Iteration 74, loss = 0.10386798\n","Iteration 75, loss = 0.10258590\n","Iteration 76, loss = 0.10114093\n","Iteration 77, loss = 0.10048073\n","Iteration 78, loss = 0.09934234\n","Iteration 79, loss = 0.10022890\n","Iteration 80, loss = 0.09828505\n","Iteration 81, loss = 0.09684428\n","Iteration 82, loss = 0.09631878\n","Iteration 83, loss = 0.09469218\n","Iteration 84, loss = 0.09442933\n","Iteration 85, loss = 0.09375329\n","Iteration 86, loss = 0.09290954\n","Iteration 87, loss = 0.09210927\n","Iteration 88, loss = 0.09005401\n","Iteration 89, loss = 0.09030177\n","Iteration 90, loss = 0.08969762\n","Iteration 91, loss = 0.08793042\n","Iteration 92, loss = 0.08673461\n","Iteration 93, loss = 0.08682753\n","Iteration 94, loss = 0.08568034\n","Iteration 95, loss = 0.08456816\n","Iteration 96, loss = 0.08539143\n","Iteration 97, loss = 0.08386694\n","Iteration 98, loss = 0.08294549\n","Iteration 99, loss = 0.08151647\n","Iteration 100, loss = 0.08129041\n","Iteration 101, loss = 0.08039981\n","Iteration 102, loss = 0.08165971\n","Iteration 103, loss = 0.07991446\n","Iteration 104, loss = 0.07879868\n","Iteration 105, loss = 0.07756797\n","Iteration 106, loss = 0.07705867\n","Iteration 107, loss = 0.07612936\n","Iteration 108, loss = 0.07636308\n","Iteration 109, loss = 0.07557860\n","Iteration 110, loss = 0.07487865\n","Iteration 111, loss = 0.07386002\n","Iteration 112, loss = 0.07296989\n","Iteration 113, loss = 0.07334137\n","Iteration 114, loss = 0.07181082\n","Iteration 115, loss = 0.07077757\n","Iteration 116, loss = 0.06955076\n","Iteration 117, loss = 0.06907169\n","Iteration 118, loss = 0.06922828\n","Iteration 119, loss = 0.06797440\n","Iteration 120, loss = 0.06683765\n","Iteration 121, loss = 0.06635990\n","Iteration 122, loss = 0.06581794\n","Iteration 123, loss = 0.06516673\n","Iteration 124, loss = 0.06546626\n","Iteration 125, loss = 0.06387775\n","Iteration 126, loss = 0.06395613\n","Iteration 127, loss = 0.06300524\n","Iteration 128, loss = 0.06230433\n","Iteration 129, loss = 0.06154362\n","Iteration 130, loss = 0.06119054\n","Iteration 131, loss = 0.06043616\n","Iteration 132, loss = 0.05971384\n","Iteration 133, loss = 0.05929701\n","Iteration 134, loss = 0.05856028\n","Iteration 135, loss = 0.05902078\n","Iteration 136, loss = 0.05845233\n","Iteration 137, loss = 0.05764827\n","Iteration 138, loss = 0.05589474\n","Iteration 139, loss = 0.05549300\n","Iteration 140, loss = 0.05459829\n","Iteration 141, loss = 0.05469726\n","Iteration 142, loss = 0.05525465\n","Iteration 143, loss = 0.05273322\n","Iteration 144, loss = 0.05275510\n","Iteration 145, loss = 0.05179798\n","Iteration 146, loss = 0.05260326\n","Iteration 147, loss = 0.05134509\n","Iteration 148, loss = 0.05104614\n","Iteration 149, loss = 0.05012886\n","Iteration 150, loss = 0.05028486\n","Iteration 1, loss = 0.55215177\n","Iteration 2, loss = 0.33566547\n","Iteration 3, loss = 0.26100293\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.23253175\n","Iteration 5, loss = 0.22011162\n","Iteration 6, loss = 0.21321167\n","Iteration 7, loss = 0.20867389\n","Iteration 8, loss = 0.20471736\n","Iteration 9, loss = 0.20166906\n","Iteration 10, loss = 0.19880987\n","Iteration 11, loss = 0.19661509\n","Iteration 12, loss = 0.19453533\n","Iteration 13, loss = 0.19221384\n","Iteration 14, loss = 0.19005693\n","Iteration 15, loss = 0.18861650\n","Iteration 16, loss = 0.18619010\n","Iteration 17, loss = 0.18523037\n","Iteration 18, loss = 0.18279881\n","Iteration 19, loss = 0.18144625\n","Iteration 20, loss = 0.17947244\n","Iteration 21, loss = 0.17854921\n","Iteration 22, loss = 0.17729369\n","Iteration 23, loss = 0.17469232\n","Iteration 24, loss = 0.17361219\n","Iteration 25, loss = 0.17226304\n","Iteration 26, loss = 0.17067323\n","Iteration 27, loss = 0.16911498\n","Iteration 28, loss = 0.16779734\n","Iteration 29, loss = 0.16648915\n","Iteration 30, loss = 0.16507189\n","Iteration 31, loss = 0.16362973\n","Iteration 32, loss = 0.16239153\n","Iteration 33, loss = 0.16111700\n","Iteration 34, loss = 0.16009460\n","Iteration 35, loss = 0.15866640\n","Iteration 36, loss = 0.15685288\n","Iteration 37, loss = 0.15572192\n","Iteration 38, loss = 0.15478871\n","Iteration 39, loss = 0.15320048\n","Iteration 40, loss = 0.15205215\n","Iteration 41, loss = 0.15097763\n","Iteration 42, loss = 0.15042210\n","Iteration 43, loss = 0.14940799\n","Iteration 44, loss = 0.14797568\n","Iteration 45, loss = 0.14694420\n","Iteration 46, loss = 0.14558581\n","Iteration 47, loss = 0.14422823\n","Iteration 48, loss = 0.14272665\n","Iteration 49, loss = 0.14194353\n","Iteration 50, loss = 0.14111527\n","Iteration 51, loss = 0.13928621\n","Iteration 52, loss = 0.13857761\n","Iteration 53, loss = 0.13773773\n","Iteration 54, loss = 0.13595581\n","Iteration 55, loss = 0.13530661\n","Iteration 56, loss = 0.13419160\n","Iteration 57, loss = 0.13307469\n","Iteration 58, loss = 0.13224216\n","Iteration 59, loss = 0.13086915\n","Iteration 60, loss = 0.12901713\n","Iteration 61, loss = 0.12857932\n","Iteration 62, loss = 0.12812504\n","Iteration 63, loss = 0.12687697\n","Iteration 64, loss = 0.12549100\n","Iteration 65, loss = 0.12484174\n","Iteration 66, loss = 0.12430661\n","Iteration 67, loss = 0.12210329\n","Iteration 68, loss = 0.12179178\n","Iteration 69, loss = 0.12073380\n","Iteration 70, loss = 0.11956754\n","Iteration 71, loss = 0.11859749\n","Iteration 72, loss = 0.11715348\n","Iteration 73, loss = 0.11701783\n","Iteration 74, loss = 0.11633998\n","Iteration 75, loss = 0.11432778\n","Iteration 76, loss = 0.11398012\n","Iteration 77, loss = 0.11245240\n","Iteration 78, loss = 0.11213122\n","Iteration 79, loss = 0.11095773\n","Iteration 80, loss = 0.10938527\n","Iteration 81, loss = 0.10901017\n","Iteration 82, loss = 0.10759269\n","Iteration 83, loss = 0.10736995\n","Iteration 84, loss = 0.10655310\n","Iteration 85, loss = 0.10617327\n","Iteration 86, loss = 0.10474224\n","Iteration 87, loss = 0.10298405\n","Iteration 88, loss = 0.10154019\n","Iteration 89, loss = 0.10258722\n","Iteration 90, loss = 0.10221827\n","Iteration 91, loss = 0.10032076\n","Iteration 92, loss = 0.10080436\n","Iteration 93, loss = 0.09819870\n","Iteration 94, loss = 0.09721349\n","Iteration 95, loss = 0.09706200\n","Iteration 96, loss = 0.09571620\n","Iteration 97, loss = 0.09525674\n","Iteration 98, loss = 0.09364790\n","Iteration 99, loss = 0.09365455\n","Iteration 100, loss = 0.09319474\n","Iteration 101, loss = 0.09176784\n","Iteration 102, loss = 0.09130288\n","Iteration 103, loss = 0.09023520\n","Iteration 104, loss = 0.09069354\n","Iteration 105, loss = 0.08867077\n","Iteration 106, loss = 0.08881781\n","Iteration 107, loss = 0.08900495\n","Iteration 108, loss = 0.08741372\n","Iteration 109, loss = 0.08641677\n","Iteration 110, loss = 0.08556527\n","Iteration 111, loss = 0.08653580\n","Iteration 112, loss = 0.08418988\n","Iteration 113, loss = 0.08464746\n","Iteration 114, loss = 0.08374520\n","Iteration 115, loss = 0.08264854\n","Iteration 116, loss = 0.08295712\n","Iteration 117, loss = 0.08103450\n","Iteration 118, loss = 0.08005041\n","Iteration 119, loss = 0.07986938\n","Iteration 120, loss = 0.07962089\n","Iteration 121, loss = 0.07856389\n","Iteration 122, loss = 0.07731230\n","Iteration 123, loss = 0.07687978\n","Iteration 124, loss = 0.07558811\n","Iteration 125, loss = 0.07576869\n","Iteration 126, loss = 0.07532145\n","Iteration 127, loss = 0.07551215\n","Iteration 128, loss = 0.07390498\n","Iteration 129, loss = 0.07324584\n","Iteration 130, loss = 0.07378671\n","Iteration 131, loss = 0.07272792\n","Iteration 132, loss = 0.07220270\n","Iteration 133, loss = 0.07216999\n","Iteration 134, loss = 0.07101339\n","Iteration 135, loss = 0.07067144\n","Iteration 136, loss = 0.06882432\n","Iteration 137, loss = 0.06856324\n","Iteration 138, loss = 0.06770990\n","Iteration 139, loss = 0.06851021\n","Iteration 140, loss = 0.06725772\n","Iteration 141, loss = 0.06769786\n","Iteration 142, loss = 0.06622399\n","Iteration 143, loss = 0.06602056\n","Iteration 144, loss = 0.06639336\n","Iteration 145, loss = 0.06392202\n","Iteration 146, loss = 0.06525002\n","Iteration 147, loss = 0.06417625\n","Iteration 148, loss = 0.06252528\n","Iteration 149, loss = 0.06341908\n","Iteration 150, loss = 0.06246250\n","Iteration 1, loss = 0.61171816\n","Iteration 2, loss = 0.33503812\n","Iteration 3, loss = 0.25757289\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.23105146\n","Iteration 5, loss = 0.21916462\n","Iteration 6, loss = 0.21227898\n","Iteration 7, loss = 0.20817513\n","Iteration 8, loss = 0.20448142\n","Iteration 9, loss = 0.20189868\n","Iteration 10, loss = 0.19973812\n","Iteration 11, loss = 0.19764818\n","Iteration 12, loss = 0.19568135\n","Iteration 13, loss = 0.19405043\n","Iteration 14, loss = 0.19223773\n","Iteration 15, loss = 0.19059059\n","Iteration 16, loss = 0.18917507\n","Iteration 17, loss = 0.18720919\n","Iteration 18, loss = 0.18571002\n","Iteration 19, loss = 0.18424389\n","Iteration 20, loss = 0.18242152\n","Iteration 21, loss = 0.18093719\n","Iteration 22, loss = 0.17939260\n","Iteration 23, loss = 0.17793150\n","Iteration 24, loss = 0.17621909\n","Iteration 25, loss = 0.17500256\n","Iteration 26, loss = 0.17333264\n","Iteration 27, loss = 0.17155033\n","Iteration 28, loss = 0.17127893\n","Iteration 29, loss = 0.16867806\n","Iteration 30, loss = 0.16698754\n","Iteration 31, loss = 0.16527764\n","Iteration 32, loss = 0.16406834\n","Iteration 33, loss = 0.16280112\n","Iteration 34, loss = 0.16135769\n","Iteration 35, loss = 0.16032872\n","Iteration 36, loss = 0.15880043\n","Iteration 37, loss = 0.15733714\n","Iteration 38, loss = 0.15574046\n","Iteration 39, loss = 0.15477187\n","Iteration 40, loss = 0.15352217\n","Iteration 41, loss = 0.15165615\n","Iteration 42, loss = 0.15071489\n","Iteration 43, loss = 0.14911882\n","Iteration 44, loss = 0.14747444\n","Iteration 45, loss = 0.14613624\n","Iteration 46, loss = 0.14489369\n","Iteration 47, loss = 0.14365842\n","Iteration 48, loss = 0.14315618\n","Iteration 49, loss = 0.14122920\n","Iteration 50, loss = 0.13979802\n","Iteration 51, loss = 0.13862792\n","Iteration 52, loss = 0.13769630\n","Iteration 53, loss = 0.13595816\n","Iteration 54, loss = 0.13464999\n","Iteration 55, loss = 0.13415556\n","Iteration 56, loss = 0.13305117\n","Iteration 57, loss = 0.13139982\n","Iteration 58, loss = 0.13008497\n","Iteration 59, loss = 0.12862389\n","Iteration 60, loss = 0.12692824\n","Iteration 61, loss = 0.12631331\n","Iteration 62, loss = 0.12508204\n","Iteration 63, loss = 0.12423742\n","Iteration 64, loss = 0.12360224\n","Iteration 65, loss = 0.12212479\n","Iteration 66, loss = 0.12113128\n","Iteration 67, loss = 0.12008077\n","Iteration 68, loss = 0.11992099\n","Iteration 69, loss = 0.11761529\n","Iteration 70, loss = 0.11636665\n","Iteration 71, loss = 0.11551197\n","Iteration 72, loss = 0.11411944\n","Iteration 73, loss = 0.11355975\n","Iteration 74, loss = 0.11306296\n","Iteration 75, loss = 0.11190637\n","Iteration 76, loss = 0.11040163\n","Iteration 77, loss = 0.10927977\n","Iteration 78, loss = 0.10892603\n","Iteration 79, loss = 0.10805976\n","Iteration 80, loss = 0.10677762\n","Iteration 81, loss = 0.10563354\n","Iteration 82, loss = 0.10471806\n","Iteration 83, loss = 0.10351127\n","Iteration 84, loss = 0.10337303\n","Iteration 85, loss = 0.10191748\n","Iteration 86, loss = 0.10089904\n","Iteration 87, loss = 0.10078822\n","Iteration 88, loss = 0.09936114\n","Iteration 89, loss = 0.09891072\n","Iteration 90, loss = 0.09826713\n","Iteration 91, loss = 0.09638885\n","Iteration 92, loss = 0.09628726\n","Iteration 93, loss = 0.09532439\n","Iteration 94, loss = 0.09402545\n","Iteration 95, loss = 0.09338517\n","Iteration 96, loss = 0.09316755\n","Iteration 97, loss = 0.09232887\n","Iteration 98, loss = 0.09076132\n","Iteration 99, loss = 0.09040861\n","Iteration 100, loss = 0.08927380\n","Iteration 101, loss = 0.08810137\n","Iteration 102, loss = 0.08872559\n","Iteration 103, loss = 0.08776174\n","Iteration 104, loss = 0.08610311\n","Iteration 105, loss = 0.08570880\n","Iteration 106, loss = 0.08502361\n","Iteration 107, loss = 0.08454472\n","Iteration 108, loss = 0.08354485\n","Iteration 109, loss = 0.08252867\n","Iteration 110, loss = 0.08230318\n","Iteration 111, loss = 0.08182772\n","Iteration 112, loss = 0.08080296\n","Iteration 113, loss = 0.07977037\n","Iteration 114, loss = 0.07955496\n","Iteration 115, loss = 0.07840329\n","Iteration 116, loss = 0.07795394\n","Iteration 117, loss = 0.07726757\n","Iteration 118, loss = 0.07642953\n","Iteration 119, loss = 0.07616230\n","Iteration 120, loss = 0.07528052\n","Iteration 121, loss = 0.07481630\n","Iteration 122, loss = 0.07437315\n","Iteration 123, loss = 0.07534842\n","Iteration 124, loss = 0.07414659\n","Iteration 125, loss = 0.07265174\n","Iteration 126, loss = 0.07226574\n","Iteration 127, loss = 0.07135158\n","Iteration 128, loss = 0.07208753\n","Iteration 129, loss = 0.06993327\n","Iteration 130, loss = 0.06937278\n","Iteration 131, loss = 0.06803379\n","Iteration 132, loss = 0.06796508\n","Iteration 133, loss = 0.06733827\n","Iteration 134, loss = 0.06738568\n","Iteration 135, loss = 0.06650920\n","Iteration 136, loss = 0.06628997\n","Iteration 137, loss = 0.06555406\n","Iteration 138, loss = 0.06586515\n","Iteration 139, loss = 0.06442702\n","Iteration 140, loss = 0.06461478\n","Iteration 141, loss = 0.06327083\n","Iteration 142, loss = 0.06208484\n","Iteration 143, loss = 0.06253226\n","Iteration 144, loss = 0.06149465\n","Iteration 145, loss = 0.06126419\n","Iteration 146, loss = 0.06074409\n","Iteration 147, loss = 0.05988108\n","Iteration 148, loss = 0.05886749\n","Iteration 149, loss = 0.05958507\n","Iteration 150, loss = 0.05827795\n","Iteration 1, loss = 0.58656265\n","Iteration 2, loss = 0.47317071\n","Iteration 3, loss = 0.40215674\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.35639245\n","Iteration 5, loss = 0.32509354\n","Iteration 6, loss = 0.30250306\n","Iteration 7, loss = 0.28605524\n","Iteration 8, loss = 0.27381300\n","Iteration 9, loss = 0.26445084\n","Iteration 10, loss = 0.25734327\n","Iteration 11, loss = 0.25163250\n","Iteration 12, loss = 0.24710525\n","Iteration 13, loss = 0.24346693\n","Iteration 14, loss = 0.24042736\n","Iteration 15, loss = 0.23782467\n","Iteration 16, loss = 0.23561676\n","Iteration 17, loss = 0.23370935\n","Iteration 18, loss = 0.23197615\n","Iteration 19, loss = 0.23043754\n","Iteration 20, loss = 0.22909300\n","Iteration 21, loss = 0.22784192\n","Iteration 22, loss = 0.22671363\n","Iteration 23, loss = 0.22566430\n","Iteration 24, loss = 0.22468920\n","Iteration 25, loss = 0.22374736\n","Iteration 26, loss = 0.22292417\n","Iteration 27, loss = 0.22215145\n","Iteration 28, loss = 0.22143627\n","Iteration 29, loss = 0.22073128\n","Iteration 30, loss = 0.22012369\n","Iteration 31, loss = 0.21947223\n","Iteration 32, loss = 0.21887662\n","Iteration 33, loss = 0.21833178\n","Iteration 34, loss = 0.21779515\n","Iteration 35, loss = 0.21723964\n","Iteration 36, loss = 0.21671666\n","Iteration 37, loss = 0.21622683\n","Iteration 38, loss = 0.21579516\n","Iteration 39, loss = 0.21531856\n","Iteration 40, loss = 0.21489834\n","Iteration 41, loss = 0.21449664\n","Iteration 42, loss = 0.21407051\n","Iteration 43, loss = 0.21368926\n","Iteration 44, loss = 0.21334726\n","Iteration 45, loss = 0.21289741\n","Iteration 46, loss = 0.21253385\n","Iteration 47, loss = 0.21216642\n","Iteration 48, loss = 0.21179495\n","Iteration 49, loss = 0.21148398\n","Iteration 50, loss = 0.21113283\n","Iteration 51, loss = 0.21084520\n","Iteration 52, loss = 0.21046490\n","Iteration 53, loss = 0.21012442\n","Iteration 54, loss = 0.20983059\n","Iteration 55, loss = 0.20947252\n","Iteration 56, loss = 0.20918847\n","Iteration 57, loss = 0.20888122\n","Iteration 58, loss = 0.20855551\n","Iteration 59, loss = 0.20823948\n","Iteration 60, loss = 0.20796606\n","Iteration 61, loss = 0.20767436\n","Iteration 62, loss = 0.20735679\n","Iteration 63, loss = 0.20706556\n","Iteration 64, loss = 0.20679313\n","Iteration 65, loss = 0.20651401\n","Iteration 66, loss = 0.20627187\n","Iteration 67, loss = 0.20601015\n","Iteration 68, loss = 0.20575401\n","Iteration 69, loss = 0.20548031\n","Iteration 70, loss = 0.20523251\n","Iteration 71, loss = 0.20497631\n","Iteration 72, loss = 0.20475007\n","Iteration 73, loss = 0.20447263\n","Iteration 74, loss = 0.20422637\n","Iteration 75, loss = 0.20398275\n","Iteration 76, loss = 0.20373596\n","Iteration 77, loss = 0.20352280\n","Iteration 78, loss = 0.20326532\n","Iteration 79, loss = 0.20303121\n","Iteration 80, loss = 0.20279436\n","Iteration 81, loss = 0.20257694\n","Iteration 82, loss = 0.20240725\n","Iteration 83, loss = 0.20206254\n","Iteration 84, loss = 0.20185886\n","Iteration 85, loss = 0.20162881\n","Iteration 86, loss = 0.20135871\n","Iteration 87, loss = 0.20112436\n","Iteration 88, loss = 0.20091821\n","Iteration 89, loss = 0.20067434\n","Iteration 90, loss = 0.20044701\n","Iteration 91, loss = 0.20020505\n","Iteration 92, loss = 0.19999752\n","Iteration 93, loss = 0.19978215\n","Iteration 94, loss = 0.19956448\n","Iteration 95, loss = 0.19935282\n","Iteration 96, loss = 0.19912165\n","Iteration 97, loss = 0.19895446\n","Iteration 98, loss = 0.19866343\n","Iteration 99, loss = 0.19847538\n","Iteration 100, loss = 0.19825176\n","Iteration 101, loss = 0.19805382\n","Iteration 102, loss = 0.19780863\n","Iteration 103, loss = 0.19763048\n","Iteration 104, loss = 0.19740955\n","Iteration 105, loss = 0.19718241\n","Iteration 106, loss = 0.19702615\n","Iteration 107, loss = 0.19679670\n","Iteration 108, loss = 0.19658872\n","Iteration 109, loss = 0.19641224\n","Iteration 110, loss = 0.19618467\n","Iteration 111, loss = 0.19596294\n","Iteration 112, loss = 0.19576631\n","Iteration 113, loss = 0.19558870\n","Iteration 114, loss = 0.19536531\n","Iteration 115, loss = 0.19520202\n","Iteration 116, loss = 0.19498561\n","Iteration 117, loss = 0.19475345\n","Iteration 118, loss = 0.19456018\n","Iteration 119, loss = 0.19437091\n","Iteration 120, loss = 0.19415848\n","Iteration 121, loss = 0.19394502\n","Iteration 122, loss = 0.19374682\n","Iteration 123, loss = 0.19354533\n","Iteration 124, loss = 0.19334920\n","Iteration 125, loss = 0.19314381\n","Iteration 126, loss = 0.19295425\n","Iteration 127, loss = 0.19276535\n","Iteration 128, loss = 0.19258090\n","Iteration 129, loss = 0.19235225\n","Iteration 130, loss = 0.19217516\n","Iteration 131, loss = 0.19201382\n","Iteration 132, loss = 0.19179497\n","Iteration 133, loss = 0.19158306\n","Iteration 134, loss = 0.19146344\n","Iteration 135, loss = 0.19123113\n","Iteration 136, loss = 0.19101499\n","Iteration 137, loss = 0.19079901\n","Iteration 138, loss = 0.19059813\n","Iteration 139, loss = 0.19044800\n","Iteration 140, loss = 0.19027069\n","Iteration 141, loss = 0.19009248\n","Iteration 142, loss = 0.18987123\n","Iteration 143, loss = 0.18972207\n","Iteration 144, loss = 0.18954758\n","Iteration 145, loss = 0.18933526\n","Iteration 146, loss = 0.18915231\n","Iteration 147, loss = 0.18898181\n","Iteration 148, loss = 0.18881780\n","Iteration 149, loss = 0.18865402\n","Iteration 150, loss = 0.18845547\n","Iteration 1, loss = 0.60146798\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.46498876\n","Iteration 3, loss = 0.38434205\n","Iteration 4, loss = 0.33598645\n","Iteration 5, loss = 0.30522559\n","Iteration 6, loss = 0.28512853\n","Iteration 7, loss = 0.27149428\n","Iteration 8, loss = 0.26174063\n","Iteration 9, loss = 0.25452238\n","Iteration 10, loss = 0.24904153\n","Iteration 11, loss = 0.24479472\n","Iteration 12, loss = 0.24123045\n","Iteration 13, loss = 0.23833142\n","Iteration 14, loss = 0.23581869\n","Iteration 15, loss = 0.23373499\n","Iteration 16, loss = 0.23190698\n","Iteration 17, loss = 0.23026544\n","Iteration 18, loss = 0.22882085\n","Iteration 19, loss = 0.22751820\n","Iteration 20, loss = 0.22634364\n","Iteration 21, loss = 0.22532899\n","Iteration 22, loss = 0.22432870\n","Iteration 23, loss = 0.22341304\n","Iteration 24, loss = 0.22260820\n","Iteration 25, loss = 0.22182681\n","Iteration 26, loss = 0.22109676\n","Iteration 27, loss = 0.22035591\n","Iteration 28, loss = 0.21969706\n","Iteration 29, loss = 0.21905053\n","Iteration 30, loss = 0.21848101\n","Iteration 31, loss = 0.21790907\n","Iteration 32, loss = 0.21739648\n","Iteration 33, loss = 0.21681883\n","Iteration 34, loss = 0.21628237\n","Iteration 35, loss = 0.21582154\n","Iteration 36, loss = 0.21535354\n","Iteration 37, loss = 0.21485830\n","Iteration 38, loss = 0.21441038\n","Iteration 39, loss = 0.21399076\n","Iteration 40, loss = 0.21359648\n","Iteration 41, loss = 0.21315360\n","Iteration 42, loss = 0.21275614\n","Iteration 43, loss = 0.21232811\n","Iteration 44, loss = 0.21197679\n","Iteration 45, loss = 0.21156998\n","Iteration 46, loss = 0.21125709\n","Iteration 47, loss = 0.21086475\n","Iteration 48, loss = 0.21051543\n","Iteration 49, loss = 0.21017736\n","Iteration 50, loss = 0.20984154\n","Iteration 51, loss = 0.20954076\n","Iteration 52, loss = 0.20925218\n","Iteration 53, loss = 0.20886336\n","Iteration 54, loss = 0.20851105\n","Iteration 55, loss = 0.20818048\n","Iteration 56, loss = 0.20789014\n","Iteration 57, loss = 0.20767640\n","Iteration 58, loss = 0.20725260\n","Iteration 59, loss = 0.20703366\n","Iteration 60, loss = 0.20665396\n","Iteration 61, loss = 0.20644662\n","Iteration 62, loss = 0.20608331\n","Iteration 63, loss = 0.20580476\n","Iteration 64, loss = 0.20551815\n","Iteration 65, loss = 0.20525763\n","Iteration 66, loss = 0.20498216\n","Iteration 67, loss = 0.20473850\n","Iteration 68, loss = 0.20443140\n","Iteration 69, loss = 0.20420903\n","Iteration 70, loss = 0.20392306\n","Iteration 71, loss = 0.20364717\n","Iteration 72, loss = 0.20344853\n","Iteration 73, loss = 0.20316181\n","Iteration 74, loss = 0.20291222\n","Iteration 75, loss = 0.20266044\n","Iteration 76, loss = 0.20238389\n","Iteration 77, loss = 0.20218615\n","Iteration 78, loss = 0.20191398\n","Iteration 79, loss = 0.20169311\n","Iteration 80, loss = 0.20146099\n","Iteration 81, loss = 0.20123931\n","Iteration 82, loss = 0.20097687\n","Iteration 83, loss = 0.20072958\n","Iteration 84, loss = 0.20053400\n","Iteration 85, loss = 0.20031752\n","Iteration 86, loss = 0.20008986\n","Iteration 87, loss = 0.19986712\n","Iteration 88, loss = 0.19962725\n","Iteration 89, loss = 0.19947215\n","Iteration 90, loss = 0.19917688\n","Iteration 91, loss = 0.19901749\n","Iteration 92, loss = 0.19877181\n","Iteration 93, loss = 0.19857925\n","Iteration 94, loss = 0.19834043\n","Iteration 95, loss = 0.19814856\n","Iteration 96, loss = 0.19793345\n","Iteration 97, loss = 0.19769675\n","Iteration 98, loss = 0.19750841\n","Iteration 99, loss = 0.19731331\n","Iteration 100, loss = 0.19706179\n","Iteration 101, loss = 0.19688983\n","Iteration 102, loss = 0.19667204\n","Iteration 103, loss = 0.19644069\n","Iteration 104, loss = 0.19625456\n","Iteration 105, loss = 0.19610665\n","Iteration 106, loss = 0.19594229\n","Iteration 107, loss = 0.19571602\n","Iteration 108, loss = 0.19553104\n","Iteration 109, loss = 0.19527667\n","Iteration 110, loss = 0.19509821\n","Iteration 111, loss = 0.19491126\n","Iteration 112, loss = 0.19466862\n","Iteration 113, loss = 0.19447767\n","Iteration 114, loss = 0.19437459\n","Iteration 115, loss = 0.19411253\n","Iteration 116, loss = 0.19394309\n","Iteration 117, loss = 0.19373944\n","Iteration 118, loss = 0.19358296\n","Iteration 119, loss = 0.19336790\n","Iteration 120, loss = 0.19319672\n","Iteration 121, loss = 0.19298215\n","Iteration 122, loss = 0.19276524\n","Iteration 123, loss = 0.19265458\n","Iteration 124, loss = 0.19244468\n","Iteration 125, loss = 0.19228877\n","Iteration 126, loss = 0.19208135\n","Iteration 127, loss = 0.19189267\n","Iteration 128, loss = 0.19173806\n","Iteration 129, loss = 0.19152136\n","Iteration 130, loss = 0.19131115\n","Iteration 131, loss = 0.19113950\n","Iteration 132, loss = 0.19095704\n","Iteration 133, loss = 0.19075004\n","Iteration 134, loss = 0.19058624\n","Iteration 135, loss = 0.19046294\n","Iteration 136, loss = 0.19024602\n","Iteration 137, loss = 0.19000866\n","Iteration 138, loss = 0.18983081\n","Iteration 139, loss = 0.18964529\n","Iteration 140, loss = 0.18942974\n","Iteration 141, loss = 0.18928688\n","Iteration 142, loss = 0.18908246\n","Iteration 143, loss = 0.18888638\n","Iteration 144, loss = 0.18872231\n","Iteration 145, loss = 0.18851257\n","Iteration 146, loss = 0.18836351\n","Iteration 147, loss = 0.18817010\n","Iteration 148, loss = 0.18799740\n","Iteration 149, loss = 0.18780998\n","Iteration 150, loss = 0.18760348\n","Iteration 1, loss = 0.68118827\n","Iteration 2, loss = 0.50771886\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.42165362\n","Iteration 4, loss = 0.36816557\n","Iteration 5, loss = 0.33102663\n","Iteration 6, loss = 0.30448656\n","Iteration 7, loss = 0.28551820\n","Iteration 8, loss = 0.27157720\n","Iteration 9, loss = 0.26117860\n","Iteration 10, loss = 0.25321972\n","Iteration 11, loss = 0.24718869\n","Iteration 12, loss = 0.24235628\n","Iteration 13, loss = 0.23836774\n","Iteration 14, loss = 0.23515752\n","Iteration 15, loss = 0.23245825\n","Iteration 16, loss = 0.23014509\n","Iteration 17, loss = 0.22826680\n","Iteration 18, loss = 0.22648987\n","Iteration 19, loss = 0.22492572\n","Iteration 20, loss = 0.22360693\n","Iteration 21, loss = 0.22235258\n","Iteration 22, loss = 0.22122699\n","Iteration 23, loss = 0.22033173\n","Iteration 24, loss = 0.21932605\n","Iteration 25, loss = 0.21847563\n","Iteration 26, loss = 0.21767695\n","Iteration 27, loss = 0.21696246\n","Iteration 28, loss = 0.21624716\n","Iteration 29, loss = 0.21560374\n","Iteration 30, loss = 0.21500436\n","Iteration 31, loss = 0.21441738\n","Iteration 32, loss = 0.21387837\n","Iteration 33, loss = 0.21335714\n","Iteration 34, loss = 0.21280258\n","Iteration 35, loss = 0.21237686\n","Iteration 36, loss = 0.21183066\n","Iteration 37, loss = 0.21137734\n","Iteration 38, loss = 0.21095080\n","Iteration 39, loss = 0.21050125\n","Iteration 40, loss = 0.21009879\n","Iteration 41, loss = 0.20972242\n","Iteration 42, loss = 0.20926756\n","Iteration 43, loss = 0.20896098\n","Iteration 44, loss = 0.20852944\n","Iteration 45, loss = 0.20824893\n","Iteration 46, loss = 0.20784552\n","Iteration 47, loss = 0.20751954\n","Iteration 48, loss = 0.20716328\n","Iteration 49, loss = 0.20681677\n","Iteration 50, loss = 0.20648820\n","Iteration 51, loss = 0.20620090\n","Iteration 52, loss = 0.20588745\n","Iteration 53, loss = 0.20562727\n","Iteration 54, loss = 0.20533564\n","Iteration 55, loss = 0.20496083\n","Iteration 56, loss = 0.20466662\n","Iteration 57, loss = 0.20442861\n","Iteration 58, loss = 0.20414685\n","Iteration 59, loss = 0.20385764\n","Iteration 60, loss = 0.20360407\n","Iteration 61, loss = 0.20328867\n","Iteration 62, loss = 0.20309229\n","Iteration 63, loss = 0.20280589\n","Iteration 64, loss = 0.20257074\n","Iteration 65, loss = 0.20230536\n","Iteration 66, loss = 0.20208409\n","Iteration 67, loss = 0.20182716\n","Iteration 68, loss = 0.20157247\n","Iteration 69, loss = 0.20129172\n","Iteration 70, loss = 0.20104159\n","Iteration 71, loss = 0.20081365\n","Iteration 72, loss = 0.20060322\n","Iteration 73, loss = 0.20033155\n","Iteration 74, loss = 0.20013936\n","Iteration 75, loss = 0.19987635\n","Iteration 76, loss = 0.19966088\n","Iteration 77, loss = 0.19943817\n","Iteration 78, loss = 0.19918506\n","Iteration 79, loss = 0.19898049\n","Iteration 80, loss = 0.19875567\n","Iteration 81, loss = 0.19852123\n","Iteration 82, loss = 0.19840975\n","Iteration 83, loss = 0.19807277\n","Iteration 84, loss = 0.19790210\n","Iteration 85, loss = 0.19769069\n","Iteration 86, loss = 0.19743042\n","Iteration 87, loss = 0.19721093\n","Iteration 88, loss = 0.19699797\n","Iteration 89, loss = 0.19679170\n","Iteration 90, loss = 0.19657191\n","Iteration 91, loss = 0.19640290\n","Iteration 92, loss = 0.19618988\n","Iteration 93, loss = 0.19594838\n","Iteration 94, loss = 0.19585808\n","Iteration 95, loss = 0.19553471\n","Iteration 96, loss = 0.19536912\n","Iteration 97, loss = 0.19523133\n","Iteration 98, loss = 0.19498454\n","Iteration 99, loss = 0.19473459\n","Iteration 100, loss = 0.19457401\n","Iteration 101, loss = 0.19434447\n","Iteration 102, loss = 0.19415720\n","Iteration 103, loss = 0.19399719\n","Iteration 104, loss = 0.19377438\n","Iteration 105, loss = 0.19361568\n","Iteration 106, loss = 0.19337085\n","Iteration 107, loss = 0.19323191\n","Iteration 108, loss = 0.19304834\n","Iteration 109, loss = 0.19285213\n","Iteration 110, loss = 0.19265474\n","Iteration 111, loss = 0.19247685\n","Iteration 112, loss = 0.19237845\n","Iteration 113, loss = 0.19208037\n","Iteration 114, loss = 0.19193422\n","Iteration 115, loss = 0.19174452\n","Iteration 116, loss = 0.19159048\n","Iteration 117, loss = 0.19136560\n","Iteration 118, loss = 0.19125215\n","Iteration 119, loss = 0.19108033\n","Iteration 120, loss = 0.19078817\n","Iteration 121, loss = 0.19061977\n","Iteration 122, loss = 0.19046009\n","Iteration 123, loss = 0.19029346\n","Iteration 124, loss = 0.19011922\n","Iteration 125, loss = 0.18988624\n","Iteration 126, loss = 0.18973562\n","Iteration 127, loss = 0.18953288\n","Iteration 128, loss = 0.18939498\n","Iteration 129, loss = 0.18916667\n","Iteration 130, loss = 0.18900534\n","Iteration 131, loss = 0.18886304\n","Iteration 132, loss = 0.18866580\n","Iteration 133, loss = 0.18847017\n","Iteration 134, loss = 0.18830625\n","Iteration 135, loss = 0.18812801\n","Iteration 136, loss = 0.18796509\n","Iteration 137, loss = 0.18776782\n","Iteration 138, loss = 0.18764454\n","Iteration 139, loss = 0.18746367\n","Iteration 140, loss = 0.18729979\n","Iteration 141, loss = 0.18710941\n","Iteration 142, loss = 0.18693002\n","Iteration 143, loss = 0.18677200\n","Iteration 144, loss = 0.18660121\n","Iteration 145, loss = 0.18648313\n","Iteration 146, loss = 0.18625664\n","Iteration 147, loss = 0.18615175\n","Iteration 148, loss = 0.18598189\n","Iteration 149, loss = 0.18575563\n","Iteration 150, loss = 0.18567948\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.66121529\n","Iteration 2, loss = 0.54663079\n","Iteration 3, loss = 0.46971307\n","Iteration 4, loss = 0.41080065\n","Iteration 5, loss = 0.36590747\n","Iteration 6, loss = 0.33265995\n","Iteration 7, loss = 0.30823560\n","Iteration 8, loss = 0.29043184\n","Iteration 9, loss = 0.27702445\n","Iteration 10, loss = 0.26703089\n","Iteration 11, loss = 0.25933348\n","Iteration 12, loss = 0.25330426\n","Iteration 13, loss = 0.24839471\n","Iteration 14, loss = 0.24434674\n","Iteration 15, loss = 0.24102022\n","Iteration 16, loss = 0.23817233\n","Iteration 17, loss = 0.23580835\n","Iteration 18, loss = 0.23371954\n","Iteration 19, loss = 0.23192847\n","Iteration 20, loss = 0.23037051\n","Iteration 21, loss = 0.22892838\n","Iteration 22, loss = 0.22769730\n","Iteration 23, loss = 0.22650803\n","Iteration 24, loss = 0.22544821\n","Iteration 25, loss = 0.22447622\n","Iteration 26, loss = 0.22360302\n","Iteration 27, loss = 0.22275416\n","Iteration 28, loss = 0.22202245\n","Iteration 29, loss = 0.22130603\n","Iteration 30, loss = 0.22065032\n","Iteration 31, loss = 0.22004452\n","Iteration 32, loss = 0.21945091\n","Iteration 33, loss = 0.21886657\n","Iteration 34, loss = 0.21833645\n","Iteration 35, loss = 0.21780494\n","Iteration 36, loss = 0.21731515\n","Iteration 37, loss = 0.21685337\n","Iteration 38, loss = 0.21641018\n","Iteration 39, loss = 0.21599971\n","Iteration 40, loss = 0.21557772\n","Iteration 41, loss = 0.21520520\n","Iteration 42, loss = 0.21480253\n","Iteration 43, loss = 0.21444159\n","Iteration 44, loss = 0.21405537\n","Iteration 45, loss = 0.21370102\n","Iteration 46, loss = 0.21341437\n","Iteration 47, loss = 0.21299931\n","Iteration 48, loss = 0.21268990\n","Iteration 49, loss = 0.21238917\n","Iteration 50, loss = 0.21203172\n","Iteration 51, loss = 0.21171662\n","Iteration 52, loss = 0.21140697\n","Iteration 53, loss = 0.21110093\n","Iteration 54, loss = 0.21081064\n","Iteration 55, loss = 0.21054790\n","Iteration 56, loss = 0.21022259\n","Iteration 57, loss = 0.20994045\n","Iteration 58, loss = 0.20966620\n","Iteration 59, loss = 0.20944339\n","Iteration 60, loss = 0.20906966\n","Iteration 61, loss = 0.20882365\n","Iteration 62, loss = 0.20853417\n","Iteration 63, loss = 0.20828621\n","Iteration 64, loss = 0.20806464\n","Iteration 65, loss = 0.20779411\n","Iteration 66, loss = 0.20754264\n","Iteration 67, loss = 0.20728426\n","Iteration 68, loss = 0.20705598\n","Iteration 69, loss = 0.20681460\n","Iteration 70, loss = 0.20655849\n","Iteration 71, loss = 0.20631722\n","Iteration 72, loss = 0.20604987\n","Iteration 73, loss = 0.20581390\n","Iteration 74, loss = 0.20558442\n","Iteration 75, loss = 0.20533531\n","Iteration 76, loss = 0.20511027\n","Iteration 77, loss = 0.20487513\n","Iteration 78, loss = 0.20467597\n","Iteration 79, loss = 0.20445073\n","Iteration 80, loss = 0.20418659\n","Iteration 81, loss = 0.20398518\n","Iteration 82, loss = 0.20379853\n","Iteration 83, loss = 0.20355075\n","Iteration 84, loss = 0.20331729\n","Iteration 85, loss = 0.20314088\n","Iteration 86, loss = 0.20291056\n","Iteration 87, loss = 0.20268972\n","Iteration 88, loss = 0.20250693\n","Iteration 89, loss = 0.20225005\n","Iteration 90, loss = 0.20205585\n","Iteration 91, loss = 0.20185731\n","Iteration 92, loss = 0.20169588\n","Iteration 93, loss = 0.20144099\n","Iteration 94, loss = 0.20123444\n","Iteration 95, loss = 0.20101787\n","Iteration 96, loss = 0.20079023\n","Iteration 97, loss = 0.20061608\n","Iteration 98, loss = 0.20043567\n","Iteration 99, loss = 0.20024220\n","Iteration 100, loss = 0.19999725\n","Iteration 101, loss = 0.19985835\n","Iteration 102, loss = 0.19962561\n","Iteration 103, loss = 0.19943576\n","Iteration 104, loss = 0.19921311\n","Iteration 105, loss = 0.19898550\n","Iteration 106, loss = 0.19878264\n","Iteration 107, loss = 0.19861590\n","Iteration 108, loss = 0.19843444\n","Iteration 109, loss = 0.19821347\n","Iteration 110, loss = 0.19800681\n","Iteration 111, loss = 0.19780526\n","Iteration 112, loss = 0.19759901\n","Iteration 113, loss = 0.19743734\n","Iteration 114, loss = 0.19720102\n","Iteration 115, loss = 0.19701517\n","Iteration 116, loss = 0.19680822\n","Iteration 117, loss = 0.19660479\n","Iteration 118, loss = 0.19644318\n","Iteration 119, loss = 0.19627036\n","Iteration 120, loss = 0.19603237\n","Iteration 121, loss = 0.19588083\n","Iteration 122, loss = 0.19568309\n","Iteration 123, loss = 0.19545923\n","Iteration 124, loss = 0.19530031\n","Iteration 125, loss = 0.19511449\n","Iteration 126, loss = 0.19490992\n","Iteration 127, loss = 0.19471350\n","Iteration 128, loss = 0.19447873\n","Iteration 129, loss = 0.19431608\n","Iteration 130, loss = 0.19413850\n","Iteration 131, loss = 0.19393858\n","Iteration 132, loss = 0.19375106\n","Iteration 133, loss = 0.19353828\n","Iteration 134, loss = 0.19339155\n","Iteration 135, loss = 0.19319574\n","Iteration 136, loss = 0.19299463\n","Iteration 137, loss = 0.19285337\n","Iteration 138, loss = 0.19262495\n","Iteration 139, loss = 0.19246305\n","Iteration 140, loss = 0.19228788\n","Iteration 141, loss = 0.19208001\n","Iteration 142, loss = 0.19191451\n","Iteration 143, loss = 0.19170874\n","Iteration 144, loss = 0.19152251\n","Iteration 145, loss = 0.19135327\n","Iteration 146, loss = 0.19119560\n","Iteration 147, loss = 0.19098474\n","Iteration 148, loss = 0.19083131\n","Iteration 149, loss = 0.19064691\n","Iteration 150, loss = 0.19049643\n","Iteration 1, loss = 0.67711993\n","Iteration 2, loss = 0.50210944\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.40452713\n","Iteration 4, loss = 0.34701611\n","Iteration 5, loss = 0.31142873\n","Iteration 6, loss = 0.28867064\n","Iteration 7, loss = 0.27320101\n","Iteration 8, loss = 0.26238799\n","Iteration 9, loss = 0.25461112\n","Iteration 10, loss = 0.24872763\n","Iteration 11, loss = 0.24416576\n","Iteration 12, loss = 0.24054253\n","Iteration 13, loss = 0.23752613\n","Iteration 14, loss = 0.23507325\n","Iteration 15, loss = 0.23294618\n","Iteration 16, loss = 0.23111673\n","Iteration 17, loss = 0.22953533\n","Iteration 18, loss = 0.22809516\n","Iteration 19, loss = 0.22683477\n","Iteration 20, loss = 0.22565484\n","Iteration 21, loss = 0.22461171\n","Iteration 22, loss = 0.22363217\n","Iteration 23, loss = 0.22275219\n","Iteration 24, loss = 0.22194198\n","Iteration 25, loss = 0.22114773\n","Iteration 26, loss = 0.22041570\n","Iteration 27, loss = 0.21973768\n","Iteration 28, loss = 0.21910835\n","Iteration 29, loss = 0.21847723\n","Iteration 30, loss = 0.21785994\n","Iteration 31, loss = 0.21733840\n","Iteration 32, loss = 0.21680554\n","Iteration 33, loss = 0.21628175\n","Iteration 34, loss = 0.21572330\n","Iteration 35, loss = 0.21529815\n","Iteration 36, loss = 0.21483896\n","Iteration 37, loss = 0.21434246\n","Iteration 38, loss = 0.21396264\n","Iteration 39, loss = 0.21351941\n","Iteration 40, loss = 0.21305808\n","Iteration 41, loss = 0.21269827\n","Iteration 42, loss = 0.21227697\n","Iteration 43, loss = 0.21185114\n","Iteration 44, loss = 0.21146361\n","Iteration 45, loss = 0.21107022\n","Iteration 46, loss = 0.21073751\n","Iteration 47, loss = 0.21032130\n","Iteration 48, loss = 0.20994480\n","Iteration 49, loss = 0.20958882\n","Iteration 50, loss = 0.20920572\n","Iteration 51, loss = 0.20890004\n","Iteration 52, loss = 0.20853914\n","Iteration 53, loss = 0.20818550\n","Iteration 54, loss = 0.20786097\n","Iteration 55, loss = 0.20751243\n","Iteration 56, loss = 0.20720979\n","Iteration 57, loss = 0.20688027\n","Iteration 58, loss = 0.20661914\n","Iteration 59, loss = 0.20627538\n","Iteration 60, loss = 0.20593128\n","Iteration 61, loss = 0.20564234\n","Iteration 62, loss = 0.20534774\n","Iteration 63, loss = 0.20503699\n","Iteration 64, loss = 0.20474946\n","Iteration 65, loss = 0.20449219\n","Iteration 66, loss = 0.20415520\n","Iteration 67, loss = 0.20389034\n","Iteration 68, loss = 0.20360787\n","Iteration 69, loss = 0.20333121\n","Iteration 70, loss = 0.20305180\n","Iteration 71, loss = 0.20283231\n","Iteration 72, loss = 0.20253523\n","Iteration 73, loss = 0.20229126\n","Iteration 74, loss = 0.20201558\n","Iteration 75, loss = 0.20176099\n","Iteration 76, loss = 0.20148901\n","Iteration 77, loss = 0.20124072\n","Iteration 78, loss = 0.20100014\n","Iteration 79, loss = 0.20073520\n","Iteration 80, loss = 0.20051115\n","Iteration 81, loss = 0.20025466\n","Iteration 82, loss = 0.20000651\n","Iteration 83, loss = 0.19973305\n","Iteration 84, loss = 0.19950083\n","Iteration 85, loss = 0.19925579\n","Iteration 86, loss = 0.19898907\n","Iteration 87, loss = 0.19876639\n","Iteration 88, loss = 0.19853259\n","Iteration 89, loss = 0.19827471\n","Iteration 90, loss = 0.19805855\n","Iteration 91, loss = 0.19785217\n","Iteration 92, loss = 0.19762004\n","Iteration 93, loss = 0.19738082\n","Iteration 94, loss = 0.19708876\n","Iteration 95, loss = 0.19687151\n","Iteration 96, loss = 0.19664271\n","Iteration 97, loss = 0.19643253\n","Iteration 98, loss = 0.19622943\n","Iteration 99, loss = 0.19594254\n","Iteration 100, loss = 0.19576033\n","Iteration 101, loss = 0.19548732\n","Iteration 102, loss = 0.19528816\n","Iteration 103, loss = 0.19503160\n","Iteration 104, loss = 0.19483170\n","Iteration 105, loss = 0.19461654\n","Iteration 106, loss = 0.19441037\n","Iteration 107, loss = 0.19418853\n","Iteration 108, loss = 0.19397246\n","Iteration 109, loss = 0.19374213\n","Iteration 110, loss = 0.19350577\n","Iteration 111, loss = 0.19336127\n","Iteration 112, loss = 0.19314070\n","Iteration 113, loss = 0.19289327\n","Iteration 114, loss = 0.19265446\n","Iteration 115, loss = 0.19245556\n","Iteration 116, loss = 0.19224781\n","Iteration 117, loss = 0.19205018\n","Iteration 118, loss = 0.19183437\n","Iteration 119, loss = 0.19161812\n","Iteration 120, loss = 0.19143654\n","Iteration 121, loss = 0.19123213\n","Iteration 122, loss = 0.19100107\n","Iteration 123, loss = 0.19079984\n","Iteration 124, loss = 0.19056308\n","Iteration 125, loss = 0.19039463\n","Iteration 126, loss = 0.19015684\n","Iteration 127, loss = 0.18995233\n","Iteration 128, loss = 0.18972561\n","Iteration 129, loss = 0.18953649\n","Iteration 130, loss = 0.18934353\n","Iteration 131, loss = 0.18910238\n","Iteration 132, loss = 0.18893235\n","Iteration 133, loss = 0.18870453\n","Iteration 134, loss = 0.18851084\n","Iteration 135, loss = 0.18831121\n","Iteration 136, loss = 0.18816337\n","Iteration 137, loss = 0.18791174\n","Iteration 138, loss = 0.18769811\n","Iteration 139, loss = 0.18750343\n","Iteration 140, loss = 0.18733081\n","Iteration 141, loss = 0.18710547\n","Iteration 142, loss = 0.18689802\n","Iteration 143, loss = 0.18671372\n","Iteration 144, loss = 0.18652034\n","Iteration 145, loss = 0.18636615\n","Iteration 146, loss = 0.18611119\n","Iteration 147, loss = 0.18591894\n","Iteration 148, loss = 0.18575705\n","Iteration 149, loss = 0.18553147\n","Iteration 150, loss = 0.18534656\n","Iteration 1, loss = 0.45601726\n","Iteration 2, loss = 0.25662079\n","Iteration 3, loss = 0.22752566\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.21672435\n","Iteration 5, loss = 0.21018764\n","Iteration 6, loss = 0.20587109\n","Iteration 7, loss = 0.20168090\n","Iteration 8, loss = 0.19796496\n","Iteration 9, loss = 0.19498064\n","Iteration 10, loss = 0.19192496\n","Iteration 11, loss = 0.18915566\n","Iteration 12, loss = 0.18622311\n","Iteration 13, loss = 0.18384848\n","Iteration 14, loss = 0.18094156\n","Iteration 15, loss = 0.17795581\n","Iteration 16, loss = 0.17539268\n","Iteration 17, loss = 0.17212784\n","Iteration 18, loss = 0.17092331\n","Iteration 19, loss = 0.16752249\n","Iteration 20, loss = 0.16516974\n","Iteration 21, loss = 0.16214679\n","Iteration 22, loss = 0.15970588\n","Iteration 23, loss = 0.15685150\n","Iteration 24, loss = 0.15376699\n","Iteration 25, loss = 0.15170160\n","Iteration 26, loss = 0.14892900\n","Iteration 27, loss = 0.14582320\n","Iteration 28, loss = 0.14378239\n","Iteration 29, loss = 0.14135208\n","Iteration 30, loss = 0.13786870\n","Iteration 31, loss = 0.13600543\n","Iteration 32, loss = 0.13222863\n","Iteration 33, loss = 0.12954642\n","Iteration 34, loss = 0.12679262\n","Iteration 35, loss = 0.12602263\n","Iteration 36, loss = 0.12211254\n","Iteration 37, loss = 0.11854204\n","Iteration 38, loss = 0.11627853\n","Iteration 39, loss = 0.11433085\n","Iteration 40, loss = 0.11281656\n","Iteration 41, loss = 0.10949064\n","Iteration 42, loss = 0.10695812\n","Iteration 43, loss = 0.10442580\n","Iteration 44, loss = 0.10179217\n","Iteration 45, loss = 0.10063946\n","Iteration 46, loss = 0.09637634\n","Iteration 47, loss = 0.09570720\n","Iteration 48, loss = 0.09309783\n","Iteration 49, loss = 0.09142550\n","Iteration 50, loss = 0.08829004\n","Iteration 51, loss = 0.08626149\n","Iteration 52, loss = 0.08331621\n","Iteration 53, loss = 0.08155222\n","Iteration 54, loss = 0.08027797\n","Iteration 55, loss = 0.07916712\n","Iteration 56, loss = 0.07602824\n","Iteration 57, loss = 0.07384628\n","Iteration 58, loss = 0.07211133\n","Iteration 59, loss = 0.06875440\n","Iteration 60, loss = 0.06662854\n","Iteration 61, loss = 0.06588213\n","Iteration 62, loss = 0.06590951\n","Iteration 63, loss = 0.06340509\n","Iteration 64, loss = 0.06138021\n","Iteration 65, loss = 0.05838267\n","Iteration 66, loss = 0.05674984\n","Iteration 67, loss = 0.05588880\n","Iteration 68, loss = 0.05360601\n","Iteration 69, loss = 0.05179885\n","Iteration 70, loss = 0.05208420\n","Iteration 71, loss = 0.04933752\n","Iteration 72, loss = 0.04700724\n","Iteration 73, loss = 0.04553109\n","Iteration 74, loss = 0.04464868\n","Iteration 75, loss = 0.04419142\n","Iteration 76, loss = 0.04214245\n","Iteration 77, loss = 0.04056003\n","Iteration 78, loss = 0.03895803\n","Iteration 79, loss = 0.03918741\n","Iteration 80, loss = 0.03819883\n","Iteration 81, loss = 0.03666570\n","Iteration 82, loss = 0.03567834\n","Iteration 83, loss = 0.03439687\n","Iteration 84, loss = 0.03373136\n","Iteration 85, loss = 0.03227867\n","Iteration 86, loss = 0.03162557\n","Iteration 87, loss = 0.03049994\n","Iteration 88, loss = 0.02885827\n","Iteration 89, loss = 0.02808899\n","Iteration 90, loss = 0.02872727\n","Iteration 91, loss = 0.02835957\n","Iteration 92, loss = 0.02611081\n","Iteration 93, loss = 0.02524312\n","Iteration 94, loss = 0.02475147\n","Iteration 95, loss = 0.02356170\n","Iteration 96, loss = 0.02312408\n","Iteration 97, loss = 0.02266468\n","Iteration 98, loss = 0.02257911\n","Iteration 99, loss = 0.02133367\n","Iteration 100, loss = 0.02061860\n","Iteration 101, loss = 0.01984555\n","Iteration 102, loss = 0.01963814\n","Iteration 103, loss = 0.01925581\n","Iteration 104, loss = 0.01847789\n","Iteration 105, loss = 0.01777439\n","Iteration 106, loss = 0.01732349\n","Iteration 107, loss = 0.01673042\n","Iteration 108, loss = 0.01648571\n","Iteration 109, loss = 0.01611640\n","Iteration 110, loss = 0.01578533\n","Iteration 111, loss = 0.01531654\n","Iteration 112, loss = 0.01452962\n","Iteration 113, loss = 0.01425962\n","Iteration 114, loss = 0.01410198\n","Iteration 115, loss = 0.01363440\n","Iteration 116, loss = 0.01350715\n","Iteration 117, loss = 0.01324110\n","Iteration 118, loss = 0.01278608\n","Iteration 119, loss = 0.01249940\n","Iteration 120, loss = 0.01219661\n","Iteration 121, loss = 0.01213101\n","Iteration 122, loss = 0.01201022\n","Iteration 123, loss = 0.01184229\n","Iteration 124, loss = 0.01136904\n","Iteration 125, loss = 0.01115665\n","Iteration 126, loss = 0.01065650\n","Iteration 127, loss = 0.01053605\n","Iteration 128, loss = 0.01018920\n","Iteration 129, loss = 0.01014911\n","Iteration 130, loss = 0.00988705\n","Iteration 131, loss = 0.00980239\n","Iteration 132, loss = 0.00969940\n","Iteration 133, loss = 0.00952808\n","Iteration 134, loss = 0.00921031\n","Iteration 135, loss = 0.00913554\n","Iteration 136, loss = 0.00910376\n","Iteration 137, loss = 0.00883641\n","Iteration 138, loss = 0.00865012\n","Iteration 139, loss = 0.00860881\n","Iteration 140, loss = 0.00852412\n","Iteration 141, loss = 0.00837771\n","Iteration 142, loss = 0.00829254\n","Iteration 143, loss = 0.00809865\n","Iteration 144, loss = 0.00803974\n","Iteration 145, loss = 0.00785503\n","Iteration 146, loss = 0.00772304\n","Iteration 147, loss = 0.00774149\n","Iteration 148, loss = 0.00763237\n","Iteration 149, loss = 0.00751108\n","Iteration 150, loss = 0.00741904\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.40983021\n","Iteration 2, loss = 0.23967630\n","Iteration 3, loss = 0.21923253\n","Iteration 4, loss = 0.21140261\n","Iteration 5, loss = 0.20650229\n","Iteration 6, loss = 0.20187851\n","Iteration 7, loss = 0.19769273\n","Iteration 8, loss = 0.19456120\n","Iteration 9, loss = 0.19110548\n","Iteration 10, loss = 0.18750484\n","Iteration 11, loss = 0.18433370\n","Iteration 12, loss = 0.18114173\n","Iteration 13, loss = 0.17738400\n","Iteration 14, loss = 0.17648010\n","Iteration 15, loss = 0.17192174\n","Iteration 16, loss = 0.16871538\n","Iteration 17, loss = 0.16687923\n","Iteration 18, loss = 0.16223131\n","Iteration 19, loss = 0.15945020\n","Iteration 20, loss = 0.15740983\n","Iteration 21, loss = 0.15285992\n","Iteration 22, loss = 0.14997486\n","Iteration 23, loss = 0.14773684\n","Iteration 24, loss = 0.14369140\n","Iteration 25, loss = 0.14185790\n","Iteration 26, loss = 0.13754564\n","Iteration 27, loss = 0.13496108\n","Iteration 28, loss = 0.13217655\n","Iteration 29, loss = 0.12882164\n","Iteration 30, loss = 0.12560139\n","Iteration 31, loss = 0.12257230\n","Iteration 32, loss = 0.11973140\n","Iteration 33, loss = 0.11599661\n","Iteration 34, loss = 0.11402368\n","Iteration 35, loss = 0.11009891\n","Iteration 36, loss = 0.10700100\n","Iteration 37, loss = 0.10468393\n","Iteration 38, loss = 0.10252148\n","Iteration 39, loss = 0.09913942\n","Iteration 40, loss = 0.09594481\n","Iteration 41, loss = 0.09301318\n","Iteration 42, loss = 0.09063151\n","Iteration 43, loss = 0.08797836\n","Iteration 44, loss = 0.08410547\n","Iteration 45, loss = 0.08307278\n","Iteration 46, loss = 0.08083696\n","Iteration 47, loss = 0.07908547\n","Iteration 48, loss = 0.07602804\n","Iteration 49, loss = 0.07460047\n","Iteration 50, loss = 0.07185804\n","Iteration 51, loss = 0.06952795\n","Iteration 52, loss = 0.06783307\n","Iteration 53, loss = 0.06581606\n","Iteration 54, loss = 0.06264956\n","Iteration 55, loss = 0.06130953\n","Iteration 56, loss = 0.05866670\n","Iteration 57, loss = 0.05773225\n","Iteration 58, loss = 0.05442401\n","Iteration 59, loss = 0.05297205\n","Iteration 60, loss = 0.05162079\n","Iteration 61, loss = 0.05021544\n","Iteration 62, loss = 0.04825491\n","Iteration 63, loss = 0.04648870\n","Iteration 64, loss = 0.04533530\n","Iteration 65, loss = 0.04341872\n","Iteration 66, loss = 0.04132732\n","Iteration 67, loss = 0.04087746\n","Iteration 68, loss = 0.03992585\n","Iteration 69, loss = 0.03730318\n","Iteration 70, loss = 0.03639263\n","Iteration 71, loss = 0.03573735\n","Iteration 72, loss = 0.03442425\n","Iteration 73, loss = 0.03348186\n","Iteration 74, loss = 0.03124517\n","Iteration 75, loss = 0.03065839\n","Iteration 76, loss = 0.02989788\n","Iteration 77, loss = 0.02857335\n","Iteration 78, loss = 0.02708035\n","Iteration 79, loss = 0.02694111\n","Iteration 80, loss = 0.02550276\n","Iteration 81, loss = 0.02515658\n","Iteration 82, loss = 0.02418739\n","Iteration 83, loss = 0.02361965\n","Iteration 84, loss = 0.02314168\n","Iteration 85, loss = 0.02123362\n","Iteration 86, loss = 0.02130798\n","Iteration 87, loss = 0.02039303\n","Iteration 88, loss = 0.01964960\n","Iteration 89, loss = 0.01884119\n","Iteration 90, loss = 0.01853694\n","Iteration 91, loss = 0.01794259\n","Iteration 92, loss = 0.01771558\n","Iteration 93, loss = 0.01692164\n","Iteration 94, loss = 0.01677223\n","Iteration 95, loss = 0.01612677\n","Iteration 96, loss = 0.01551151\n","Iteration 97, loss = 0.01487197\n","Iteration 98, loss = 0.01451065\n","Iteration 99, loss = 0.01431345\n","Iteration 100, loss = 0.01398158\n","Iteration 101, loss = 0.01390545\n","Iteration 102, loss = 0.01321237\n","Iteration 103, loss = 0.01290855\n","Iteration 104, loss = 0.01280113\n","Iteration 105, loss = 0.01213301\n","Iteration 106, loss = 0.01200106\n","Iteration 107, loss = 0.01189562\n","Iteration 108, loss = 0.01134837\n","Iteration 109, loss = 0.01099627\n","Iteration 110, loss = 0.01064522\n","Iteration 111, loss = 0.01114637\n","Iteration 112, loss = 0.01051047\n","Iteration 113, loss = 0.01025872\n","Iteration 114, loss = 0.00987138\n","Iteration 115, loss = 0.00991084\n","Iteration 116, loss = 0.00991305\n","Iteration 117, loss = 0.00951929\n","Iteration 118, loss = 0.00958030\n","Iteration 119, loss = 0.00888663\n","Iteration 120, loss = 0.00885704\n","Iteration 121, loss = 0.00870876\n","Iteration 122, loss = 0.00865692\n","Iteration 123, loss = 0.00836039\n","Iteration 124, loss = 0.00829103\n","Iteration 125, loss = 0.00810623\n","Iteration 126, loss = 0.00799896\n","Iteration 127, loss = 0.00781816\n","Iteration 128, loss = 0.00766282\n","Iteration 129, loss = 0.00758885\n","Iteration 130, loss = 0.00745715\n","Iteration 131, loss = 0.00741412\n","Iteration 132, loss = 0.00739977\n","Iteration 133, loss = 0.00721348\n","Iteration 134, loss = 0.00721610\n","Iteration 135, loss = 0.00709429\n","Iteration 136, loss = 0.00697490\n","Iteration 137, loss = 0.00690919\n","Iteration 138, loss = 0.00687457\n","Iteration 139, loss = 0.00674668\n","Iteration 140, loss = 0.00671500\n","Iteration 141, loss = 0.00672345\n","Iteration 142, loss = 0.00659664\n","Iteration 143, loss = 0.00649082\n","Iteration 144, loss = 0.00648875\n","Iteration 145, loss = 0.00641163\n","Iteration 146, loss = 0.00631003\n","Iteration 147, loss = 0.00622508\n","Iteration 148, loss = 0.00619404\n","Iteration 149, loss = 0.00616760\n","Iteration 150, loss = 0.00619014\n","Iteration 1, loss = 0.39254514\n","Iteration 2, loss = 0.23305427\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.21490234\n","Iteration 4, loss = 0.20684265\n","Iteration 5, loss = 0.20133150\n","Iteration 6, loss = 0.19633586\n","Iteration 7, loss = 0.19221449\n","Iteration 8, loss = 0.18875509\n","Iteration 9, loss = 0.18491790\n","Iteration 10, loss = 0.18260321\n","Iteration 11, loss = 0.17873687\n","Iteration 12, loss = 0.17584462\n","Iteration 13, loss = 0.17292278\n","Iteration 14, loss = 0.17070747\n","Iteration 15, loss = 0.16730162\n","Iteration 16, loss = 0.16382304\n","Iteration 17, loss = 0.16113422\n","Iteration 18, loss = 0.15889281\n","Iteration 19, loss = 0.15548475\n","Iteration 20, loss = 0.15277351\n","Iteration 21, loss = 0.14986800\n","Iteration 22, loss = 0.14725031\n","Iteration 23, loss = 0.14588145\n","Iteration 24, loss = 0.14120123\n","Iteration 25, loss = 0.13877326\n","Iteration 26, loss = 0.13733784\n","Iteration 27, loss = 0.13428228\n","Iteration 28, loss = 0.13034739\n","Iteration 29, loss = 0.12664086\n","Iteration 30, loss = 0.12429411\n","Iteration 31, loss = 0.12290868\n","Iteration 32, loss = 0.11934062\n","Iteration 33, loss = 0.11823047\n","Iteration 34, loss = 0.11637156\n","Iteration 35, loss = 0.11137021\n","Iteration 36, loss = 0.11006868\n","Iteration 37, loss = 0.10659599\n","Iteration 38, loss = 0.10278109\n","Iteration 39, loss = 0.10132686\n","Iteration 40, loss = 0.09778565\n","Iteration 41, loss = 0.09657162\n","Iteration 42, loss = 0.09381981\n","Iteration 43, loss = 0.09138386\n","Iteration 44, loss = 0.08942076\n","Iteration 45, loss = 0.08573668\n","Iteration 46, loss = 0.08469729\n","Iteration 47, loss = 0.08260770\n","Iteration 48, loss = 0.08069323\n","Iteration 49, loss = 0.08111813\n","Iteration 50, loss = 0.07620738\n","Iteration 51, loss = 0.07490189\n","Iteration 52, loss = 0.07504385\n","Iteration 53, loss = 0.06969062\n","Iteration 54, loss = 0.06926806\n","Iteration 55, loss = 0.06608286\n","Iteration 56, loss = 0.06591634\n","Iteration 57, loss = 0.06360335\n","Iteration 58, loss = 0.06204376\n","Iteration 59, loss = 0.05875264\n","Iteration 60, loss = 0.05703841\n","Iteration 61, loss = 0.05672042\n","Iteration 62, loss = 0.05415731\n","Iteration 63, loss = 0.05226905\n","Iteration 64, loss = 0.05090876\n","Iteration 65, loss = 0.04947880\n","Iteration 66, loss = 0.04840644\n","Iteration 67, loss = 0.04627664\n","Iteration 68, loss = 0.04349830\n","Iteration 69, loss = 0.04261888\n","Iteration 70, loss = 0.04185758\n","Iteration 71, loss = 0.04053900\n","Iteration 72, loss = 0.03891196\n","Iteration 73, loss = 0.03843177\n","Iteration 74, loss = 0.03708935\n","Iteration 75, loss = 0.03563420\n","Iteration 76, loss = 0.03820150\n","Iteration 77, loss = 0.03340504\n","Iteration 78, loss = 0.03177721\n","Iteration 79, loss = 0.03060020\n","Iteration 80, loss = 0.03155539\n","Iteration 81, loss = 0.03119998\n","Iteration 82, loss = 0.02853106\n","Iteration 83, loss = 0.02754575\n","Iteration 84, loss = 0.02722197\n","Iteration 85, loss = 0.02651936\n","Iteration 86, loss = 0.02561685\n","Iteration 87, loss = 0.02571032\n","Iteration 88, loss = 0.02387434\n","Iteration 89, loss = 0.02243704\n","Iteration 90, loss = 0.02205718\n","Iteration 91, loss = 0.02071207\n","Iteration 92, loss = 0.02052765\n","Iteration 93, loss = 0.02001441\n","Iteration 94, loss = 0.01946923\n","Iteration 95, loss = 0.01883913\n","Iteration 96, loss = 0.01873811\n","Iteration 97, loss = 0.01874170\n","Iteration 98, loss = 0.01733904\n","Iteration 99, loss = 0.01701019\n","Iteration 100, loss = 0.01678520\n","Iteration 101, loss = 0.01598170\n","Iteration 102, loss = 0.01543604\n","Iteration 103, loss = 0.01513198\n","Iteration 104, loss = 0.01508639\n","Iteration 105, loss = 0.01416407\n","Iteration 106, loss = 0.01391640\n","Iteration 107, loss = 0.01380178\n","Iteration 108, loss = 0.01315605\n","Iteration 109, loss = 0.01345445\n","Iteration 110, loss = 0.01249170\n","Iteration 111, loss = 0.01210962\n","Iteration 112, loss = 0.01194905\n","Iteration 113, loss = 0.01187587\n","Iteration 114, loss = 0.01175110\n","Iteration 115, loss = 0.01114465\n","Iteration 116, loss = 0.01069214\n","Iteration 117, loss = 0.01053385\n","Iteration 118, loss = 0.01019032\n","Iteration 119, loss = 0.01009325\n","Iteration 120, loss = 0.00997372\n","Iteration 121, loss = 0.00996485\n","Iteration 122, loss = 0.00959683\n","Iteration 123, loss = 0.00963732\n","Iteration 124, loss = 0.00910909\n","Iteration 125, loss = 0.00910325\n","Iteration 126, loss = 0.00879585\n","Iteration 127, loss = 0.00866550\n","Iteration 128, loss = 0.00851217\n","Iteration 129, loss = 0.00835143\n","Iteration 130, loss = 0.00816709\n","Iteration 131, loss = 0.00832589\n","Iteration 132, loss = 0.00823648\n","Iteration 133, loss = 0.00804832\n","Iteration 134, loss = 0.00779762\n","Iteration 135, loss = 0.00813057\n","Iteration 136, loss = 0.00774914\n","Iteration 137, loss = 0.00749845\n","Iteration 138, loss = 0.00733588\n","Iteration 139, loss = 0.00713293\n","Iteration 140, loss = 0.00722991\n","Iteration 141, loss = 0.00706474\n","Iteration 142, loss = 0.00694894\n","Iteration 143, loss = 0.00677881\n","Iteration 144, loss = 0.00667869\n","Iteration 145, loss = 0.00668625\n","Iteration 146, loss = 0.00672354\n","Iteration 147, loss = 0.00656738\n","Iteration 148, loss = 0.00658072\n","Iteration 149, loss = 0.00639077\n","Iteration 150, loss = 0.00624538\n","Iteration 1, loss = 0.40038623\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.23527622\n","Iteration 3, loss = 0.21869265\n","Iteration 4, loss = 0.21057371\n","Iteration 5, loss = 0.20490738\n","Iteration 6, loss = 0.20042816\n","Iteration 7, loss = 0.19685412\n","Iteration 8, loss = 0.19245722\n","Iteration 9, loss = 0.18976062\n","Iteration 10, loss = 0.18620140\n","Iteration 11, loss = 0.18276195\n","Iteration 12, loss = 0.18061554\n","Iteration 13, loss = 0.17683370\n","Iteration 14, loss = 0.17432496\n","Iteration 15, loss = 0.17122804\n","Iteration 16, loss = 0.16844775\n","Iteration 17, loss = 0.16540469\n","Iteration 18, loss = 0.16374811\n","Iteration 19, loss = 0.16058649\n","Iteration 20, loss = 0.15747262\n","Iteration 21, loss = 0.15529653\n","Iteration 22, loss = 0.15355172\n","Iteration 23, loss = 0.14988555\n","Iteration 24, loss = 0.14670269\n","Iteration 25, loss = 0.14472149\n","Iteration 26, loss = 0.14209835\n","Iteration 27, loss = 0.13891184\n","Iteration 28, loss = 0.13573444\n","Iteration 29, loss = 0.13342539\n","Iteration 30, loss = 0.13052483\n","Iteration 31, loss = 0.12807587\n","Iteration 32, loss = 0.12510467\n","Iteration 33, loss = 0.12309947\n","Iteration 34, loss = 0.11984310\n","Iteration 35, loss = 0.11720334\n","Iteration 36, loss = 0.11439508\n","Iteration 37, loss = 0.11148943\n","Iteration 38, loss = 0.11141196\n","Iteration 39, loss = 0.10615466\n","Iteration 40, loss = 0.10418057\n","Iteration 41, loss = 0.10118051\n","Iteration 42, loss = 0.09945580\n","Iteration 43, loss = 0.09651024\n","Iteration 44, loss = 0.09506395\n","Iteration 45, loss = 0.09204478\n","Iteration 46, loss = 0.08935512\n","Iteration 47, loss = 0.08613411\n","Iteration 48, loss = 0.08614281\n","Iteration 49, loss = 0.08154903\n","Iteration 50, loss = 0.07974889\n","Iteration 51, loss = 0.07867106\n","Iteration 52, loss = 0.07565985\n","Iteration 53, loss = 0.07283661\n","Iteration 54, loss = 0.07091373\n","Iteration 55, loss = 0.07089259\n","Iteration 56, loss = 0.06775381\n","Iteration 57, loss = 0.06426923\n","Iteration 58, loss = 0.06090512\n","Iteration 59, loss = 0.06116978\n","Iteration 60, loss = 0.05827508\n","Iteration 61, loss = 0.05676411\n","Iteration 62, loss = 0.05707083\n","Iteration 63, loss = 0.05350031\n","Iteration 64, loss = 0.05087076\n","Iteration 65, loss = 0.05091920\n","Iteration 66, loss = 0.04766611\n","Iteration 67, loss = 0.04817972\n","Iteration 68, loss = 0.04695258\n","Iteration 69, loss = 0.04394918\n","Iteration 70, loss = 0.04383820\n","Iteration 71, loss = 0.04212993\n","Iteration 72, loss = 0.04015688\n","Iteration 73, loss = 0.03886365\n","Iteration 74, loss = 0.03740537\n","Iteration 75, loss = 0.03528630\n","Iteration 76, loss = 0.03413648\n","Iteration 77, loss = 0.03369863\n","Iteration 78, loss = 0.03259156\n","Iteration 79, loss = 0.03206883\n","Iteration 80, loss = 0.03048877\n","Iteration 81, loss = 0.03223590\n","Iteration 82, loss = 0.02842384\n","Iteration 83, loss = 0.02692830\n","Iteration 84, loss = 0.02617031\n","Iteration 85, loss = 0.02581725\n","Iteration 86, loss = 0.02483846\n","Iteration 87, loss = 0.02447941\n","Iteration 88, loss = 0.02291401\n","Iteration 89, loss = 0.02261231\n","Iteration 90, loss = 0.02242562\n","Iteration 91, loss = 0.02113571\n","Iteration 92, loss = 0.02003641\n","Iteration 93, loss = 0.01948299\n","Iteration 94, loss = 0.01859325\n","Iteration 95, loss = 0.01841872\n","Iteration 96, loss = 0.01833451\n","Iteration 97, loss = 0.01841568\n","Iteration 98, loss = 0.01675800\n","Iteration 99, loss = 0.01618603\n","Iteration 100, loss = 0.01576362\n","Iteration 101, loss = 0.01562695\n","Iteration 102, loss = 0.01463838\n","Iteration 103, loss = 0.01470581\n","Iteration 104, loss = 0.01401616\n","Iteration 105, loss = 0.01375903\n","Iteration 106, loss = 0.01392573\n","Iteration 107, loss = 0.01309645\n","Iteration 108, loss = 0.01283132\n","Iteration 109, loss = 0.01216269\n","Iteration 110, loss = 0.01191027\n","Iteration 111, loss = 0.01155733\n","Iteration 112, loss = 0.01132998\n","Iteration 113, loss = 0.01114520\n","Iteration 114, loss = 0.01085325\n","Iteration 115, loss = 0.01077709\n","Iteration 116, loss = 0.01049389\n","Iteration 117, loss = 0.01001348\n","Iteration 118, loss = 0.00983035\n","Iteration 119, loss = 0.01019024\n","Iteration 120, loss = 0.00953947\n","Iteration 121, loss = 0.00910524\n","Iteration 122, loss = 0.00910853\n","Iteration 123, loss = 0.00892217\n","Iteration 124, loss = 0.00899920\n","Iteration 125, loss = 0.00858418\n","Iteration 126, loss = 0.00847175\n","Iteration 127, loss = 0.00817461\n","Iteration 128, loss = 0.00826233\n","Iteration 129, loss = 0.00806549\n","Iteration 130, loss = 0.00784151\n","Iteration 131, loss = 0.00768992\n","Iteration 132, loss = 0.00767194\n","Iteration 133, loss = 0.00751238\n","Iteration 134, loss = 0.00742340\n","Iteration 135, loss = 0.00729847\n","Iteration 136, loss = 0.00710258\n","Iteration 137, loss = 0.00706437\n","Iteration 138, loss = 0.00726066\n","Iteration 139, loss = 0.00698605\n","Iteration 140, loss = 0.00689196\n","Iteration 141, loss = 0.00689286\n","Iteration 142, loss = 0.00688277\n","Iteration 143, loss = 0.00669365\n","Iteration 144, loss = 0.00660094\n","Iteration 145, loss = 0.00649819\n","Iteration 146, loss = 0.00635196\n","Iteration 147, loss = 0.00630429\n","Iteration 148, loss = 0.00623756\n","Iteration 149, loss = 0.00621160\n","Iteration 150, loss = 0.00622125\n","Iteration 1, loss = 0.47936079\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.24502343\n","Iteration 3, loss = 0.21894334\n","Iteration 4, loss = 0.21065957\n","Iteration 5, loss = 0.20536930\n","Iteration 6, loss = 0.20101864\n","Iteration 7, loss = 0.19709820\n","Iteration 8, loss = 0.19319786\n","Iteration 9, loss = 0.18991355\n","Iteration 10, loss = 0.18696799\n","Iteration 11, loss = 0.18355128\n","Iteration 12, loss = 0.18143445\n","Iteration 13, loss = 0.17783509\n","Iteration 14, loss = 0.17428244\n","Iteration 15, loss = 0.17136607\n","Iteration 16, loss = 0.16819457\n","Iteration 17, loss = 0.16520133\n","Iteration 18, loss = 0.16132141\n","Iteration 19, loss = 0.15891176\n","Iteration 20, loss = 0.15569899\n","Iteration 21, loss = 0.15323235\n","Iteration 22, loss = 0.15018925\n","Iteration 23, loss = 0.14717698\n","Iteration 24, loss = 0.14457262\n","Iteration 25, loss = 0.14138155\n","Iteration 26, loss = 0.13800719\n","Iteration 27, loss = 0.13430511\n","Iteration 28, loss = 0.13182213\n","Iteration 29, loss = 0.13025142\n","Iteration 30, loss = 0.12757704\n","Iteration 31, loss = 0.12394440\n","Iteration 32, loss = 0.12035647\n","Iteration 33, loss = 0.11720103\n","Iteration 34, loss = 0.11395338\n","Iteration 35, loss = 0.11194580\n","Iteration 36, loss = 0.11068246\n","Iteration 37, loss = 0.10596825\n","Iteration 38, loss = 0.10407372\n","Iteration 39, loss = 0.10055967\n","Iteration 40, loss = 0.09937066\n","Iteration 41, loss = 0.09761510\n","Iteration 42, loss = 0.09378466\n","Iteration 43, loss = 0.09122917\n","Iteration 44, loss = 0.08958158\n","Iteration 45, loss = 0.08568177\n","Iteration 46, loss = 0.08387615\n","Iteration 47, loss = 0.08095861\n","Iteration 48, loss = 0.07866716\n","Iteration 49, loss = 0.07663311\n","Iteration 50, loss = 0.07418585\n","Iteration 51, loss = 0.07227710\n","Iteration 52, loss = 0.06888056\n","Iteration 53, loss = 0.06917774\n","Iteration 54, loss = 0.06647359\n","Iteration 55, loss = 0.06429846\n","Iteration 56, loss = 0.06373454\n","Iteration 57, loss = 0.06180896\n","Iteration 58, loss = 0.05774729\n","Iteration 59, loss = 0.05620362\n","Iteration 60, loss = 0.05563257\n","Iteration 61, loss = 0.05332156\n","Iteration 62, loss = 0.05155753\n","Iteration 63, loss = 0.04951472\n","Iteration 64, loss = 0.04742061\n","Iteration 65, loss = 0.04569416\n","Iteration 66, loss = 0.04444660\n","Iteration 67, loss = 0.04314847\n","Iteration 68, loss = 0.04282840\n","Iteration 69, loss = 0.04054282\n","Iteration 70, loss = 0.03939603\n","Iteration 71, loss = 0.03885020\n","Iteration 72, loss = 0.03723847\n","Iteration 73, loss = 0.03575473\n","Iteration 74, loss = 0.03490457\n","Iteration 75, loss = 0.03438908\n","Iteration 76, loss = 0.03322374\n","Iteration 77, loss = 0.03141147\n","Iteration 78, loss = 0.03025817\n","Iteration 79, loss = 0.02874268\n","Iteration 80, loss = 0.02868358\n","Iteration 81, loss = 0.02724941\n","Iteration 82, loss = 0.02695237\n","Iteration 83, loss = 0.02652778\n","Iteration 84, loss = 0.02490589\n","Iteration 85, loss = 0.02352044\n","Iteration 86, loss = 0.02305010\n","Iteration 87, loss = 0.02228494\n","Iteration 88, loss = 0.02171660\n","Iteration 89, loss = 0.02088916\n","Iteration 90, loss = 0.02030038\n","Iteration 91, loss = 0.01930142\n","Iteration 92, loss = 0.01844296\n","Iteration 93, loss = 0.01853125\n","Iteration 94, loss = 0.01799706\n","Iteration 95, loss = 0.01690310\n","Iteration 96, loss = 0.01723519\n","Iteration 97, loss = 0.01670707\n","Iteration 98, loss = 0.01576678\n","Iteration 99, loss = 0.01499711\n","Iteration 100, loss = 0.01497540\n","Iteration 101, loss = 0.01416776\n","Iteration 102, loss = 0.01412474\n","Iteration 103, loss = 0.01395189\n","Iteration 104, loss = 0.01368876\n","Iteration 105, loss = 0.01305350\n","Iteration 106, loss = 0.01241636\n","Iteration 107, loss = 0.01182457\n","Iteration 108, loss = 0.01177258\n","Iteration 109, loss = 0.01192253\n","Iteration 110, loss = 0.01108240\n","Iteration 111, loss = 0.01147538\n","Iteration 112, loss = 0.01080978\n","Iteration 113, loss = 0.01073794\n","Iteration 114, loss = 0.01025106\n","Iteration 115, loss = 0.01012072\n","Iteration 116, loss = 0.01004013\n","Iteration 117, loss = 0.00996247\n","Iteration 118, loss = 0.00933512\n","Iteration 119, loss = 0.00937280\n","Iteration 120, loss = 0.00905871\n","Iteration 121, loss = 0.00889507\n","Iteration 122, loss = 0.00859863\n","Iteration 123, loss = 0.00839169\n","Iteration 124, loss = 0.00832165\n","Iteration 125, loss = 0.00826751\n","Iteration 126, loss = 0.00802158\n","Iteration 127, loss = 0.00809597\n","Iteration 128, loss = 0.00792281\n","Iteration 129, loss = 0.00780570\n","Iteration 130, loss = 0.00757059\n","Iteration 131, loss = 0.00745803\n","Iteration 132, loss = 0.00760072\n","Iteration 133, loss = 0.00752027\n","Iteration 134, loss = 0.00735128\n","Iteration 135, loss = 0.00721788\n","Iteration 136, loss = 0.00696735\n","Iteration 137, loss = 0.00698563\n","Iteration 138, loss = 0.00694291\n","Iteration 139, loss = 0.00682976\n","Iteration 140, loss = 0.00665214\n","Iteration 141, loss = 0.00658875\n","Iteration 142, loss = 0.00661696\n","Iteration 143, loss = 0.00656906\n","Iteration 144, loss = 0.00641588\n","Iteration 145, loss = 0.00637891\n","Iteration 146, loss = 0.00630532\n","Iteration 147, loss = 0.00651864\n","Iteration 148, loss = 0.00624022\n","Iteration 149, loss = 0.00619926\n","Iteration 150, loss = 0.00609936\n","Iteration 1, loss = 0.68703978\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.52182673\n","Iteration 3, loss = 0.44538473\n","Iteration 4, loss = 0.39320570\n","Iteration 5, loss = 0.35338739\n","Iteration 6, loss = 0.32344780\n","Iteration 7, loss = 0.30135887\n","Iteration 8, loss = 0.28551317\n","Iteration 9, loss = 0.27390021\n","Iteration 10, loss = 0.26530627\n","Iteration 11, loss = 0.25870100\n","Iteration 12, loss = 0.25351972\n","Iteration 13, loss = 0.24934191\n","Iteration 14, loss = 0.24588468\n","Iteration 15, loss = 0.24295759\n","Iteration 16, loss = 0.24045668\n","Iteration 17, loss = 0.23829622\n","Iteration 18, loss = 0.23642068\n","Iteration 19, loss = 0.23475761\n","Iteration 20, loss = 0.23320836\n","Iteration 21, loss = 0.23185853\n","Iteration 22, loss = 0.23067707\n","Iteration 23, loss = 0.22954509\n","Iteration 24, loss = 0.22852755\n","Iteration 25, loss = 0.22765411\n","Iteration 26, loss = 0.22680248\n","Iteration 27, loss = 0.22600157\n","Iteration 28, loss = 0.22528912\n","Iteration 29, loss = 0.22460955\n","Iteration 30, loss = 0.22397461\n","Iteration 31, loss = 0.22337421\n","Iteration 32, loss = 0.22277047\n","Iteration 33, loss = 0.22221691\n","Iteration 34, loss = 0.22171013\n","Iteration 35, loss = 0.22121675\n","Iteration 36, loss = 0.22074071\n","Iteration 37, loss = 0.22025306\n","Iteration 38, loss = 0.21980817\n","Iteration 39, loss = 0.21940118\n","Iteration 40, loss = 0.21900329\n","Iteration 41, loss = 0.21859211\n","Iteration 42, loss = 0.21821039\n","Iteration 43, loss = 0.21784418\n","Iteration 44, loss = 0.21746229\n","Iteration 45, loss = 0.21711262\n","Iteration 46, loss = 0.21676593\n","Iteration 47, loss = 0.21641640\n","Iteration 48, loss = 0.21605670\n","Iteration 49, loss = 0.21574708\n","Iteration 50, loss = 0.21544336\n","Iteration 51, loss = 0.21510710\n","Iteration 52, loss = 0.21483078\n","Iteration 53, loss = 0.21452602\n","Iteration 54, loss = 0.21422683\n","Iteration 55, loss = 0.21392203\n","Iteration 56, loss = 0.21361141\n","Iteration 57, loss = 0.21335304\n","Iteration 58, loss = 0.21309239\n","Iteration 59, loss = 0.21284747\n","Iteration 60, loss = 0.21252248\n","Iteration 61, loss = 0.21231379\n","Iteration 62, loss = 0.21204575\n","Iteration 63, loss = 0.21176273\n","Iteration 64, loss = 0.21154470\n","Iteration 65, loss = 0.21129260\n","Iteration 66, loss = 0.21107039\n","Iteration 67, loss = 0.21091304\n","Iteration 68, loss = 0.21059484\n","Iteration 69, loss = 0.21038586\n","Iteration 70, loss = 0.21015649\n","Iteration 71, loss = 0.20991809\n","Iteration 72, loss = 0.20968647\n","Iteration 73, loss = 0.20949572\n","Iteration 74, loss = 0.20929667\n","Iteration 75, loss = 0.20903236\n","Iteration 76, loss = 0.20885898\n","Iteration 77, loss = 0.20862724\n","Iteration 78, loss = 0.20843349\n","Iteration 79, loss = 0.20823113\n","Iteration 80, loss = 0.20802125\n","Iteration 81, loss = 0.20781118\n","Iteration 82, loss = 0.20759887\n","Iteration 83, loss = 0.20743489\n","Iteration 84, loss = 0.20725165\n","Iteration 85, loss = 0.20705693\n","Iteration 86, loss = 0.20684014\n","Iteration 87, loss = 0.20664828\n","Iteration 88, loss = 0.20648166\n","Iteration 89, loss = 0.20626384\n","Iteration 90, loss = 0.20609075\n","Iteration 91, loss = 0.20590939\n","Iteration 92, loss = 0.20579556\n","Iteration 93, loss = 0.20559169\n","Iteration 94, loss = 0.20542299\n","Iteration 95, loss = 0.20529837\n","Iteration 96, loss = 0.20503264\n","Iteration 97, loss = 0.20487653\n","Iteration 98, loss = 0.20470649\n","Iteration 99, loss = 0.20454570\n","Iteration 100, loss = 0.20434592\n","Iteration 101, loss = 0.20418645\n","Iteration 102, loss = 0.20405391\n","Iteration 103, loss = 0.20382879\n","Iteration 104, loss = 0.20367464\n","Iteration 105, loss = 0.20353641\n","Iteration 106, loss = 0.20336082\n","Iteration 107, loss = 0.20318916\n","Iteration 108, loss = 0.20302694\n","Iteration 109, loss = 0.20284782\n","Iteration 110, loss = 0.20273032\n","Iteration 111, loss = 0.20253897\n","Iteration 112, loss = 0.20239563\n","Iteration 113, loss = 0.20226707\n","Iteration 114, loss = 0.20206418\n","Iteration 115, loss = 0.20194938\n","Iteration 116, loss = 0.20178436\n","Iteration 117, loss = 0.20162565\n","Iteration 118, loss = 0.20148732\n","Iteration 119, loss = 0.20136791\n","Iteration 120, loss = 0.20117763\n","Iteration 121, loss = 0.20109948\n","Iteration 122, loss = 0.20091100\n","Iteration 123, loss = 0.20077471\n","Iteration 124, loss = 0.20065553\n","Iteration 125, loss = 0.20052349\n","Iteration 126, loss = 0.20038742\n","Iteration 127, loss = 0.20021341\n","Iteration 128, loss = 0.20009683\n","Iteration 129, loss = 0.19997396\n","Iteration 130, loss = 0.19983845\n","Iteration 131, loss = 0.19969566\n","Iteration 132, loss = 0.19958250\n","Iteration 133, loss = 0.19943079\n","Iteration 134, loss = 0.19928767\n","Iteration 135, loss = 0.19917417\n","Iteration 136, loss = 0.19906865\n","Iteration 137, loss = 0.19893662\n","Iteration 138, loss = 0.19879043\n","Iteration 139, loss = 0.19865500\n","Iteration 140, loss = 0.19853511\n","Iteration 141, loss = 0.19840639\n","Iteration 142, loss = 0.19824490\n","Iteration 143, loss = 0.19814319\n","Iteration 144, loss = 0.19801679\n","Iteration 145, loss = 0.19787565\n","Iteration 146, loss = 0.19775403\n","Iteration 147, loss = 0.19761725\n","Iteration 148, loss = 0.19750267\n","Iteration 149, loss = 0.19734802\n","Iteration 150, loss = 0.19725484\n","Iteration 1, loss = 0.83990421\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.55959778\n","Iteration 3, loss = 0.43707152\n","Iteration 4, loss = 0.37481362\n","Iteration 5, loss = 0.33704520\n","Iteration 6, loss = 0.31188862\n","Iteration 7, loss = 0.29450391\n","Iteration 8, loss = 0.28195468\n","Iteration 9, loss = 0.27249604\n","Iteration 10, loss = 0.26517859\n","Iteration 11, loss = 0.25944017\n","Iteration 12, loss = 0.25483422\n","Iteration 13, loss = 0.25112537\n","Iteration 14, loss = 0.24802324\n","Iteration 15, loss = 0.24539902\n","Iteration 16, loss = 0.24314557\n","Iteration 17, loss = 0.24118051\n","Iteration 18, loss = 0.23939419\n","Iteration 19, loss = 0.23782567\n","Iteration 20, loss = 0.23637646\n","Iteration 21, loss = 0.23512627\n","Iteration 22, loss = 0.23399118\n","Iteration 23, loss = 0.23293781\n","Iteration 24, loss = 0.23194719\n","Iteration 25, loss = 0.23105365\n","Iteration 26, loss = 0.23021109\n","Iteration 27, loss = 0.22942140\n","Iteration 28, loss = 0.22868013\n","Iteration 29, loss = 0.22799907\n","Iteration 30, loss = 0.22731452\n","Iteration 31, loss = 0.22673150\n","Iteration 32, loss = 0.22609578\n","Iteration 33, loss = 0.22557071\n","Iteration 34, loss = 0.22499578\n","Iteration 35, loss = 0.22449722\n","Iteration 36, loss = 0.22395418\n","Iteration 37, loss = 0.22349224\n","Iteration 38, loss = 0.22306354\n","Iteration 39, loss = 0.22258298\n","Iteration 40, loss = 0.22216963\n","Iteration 41, loss = 0.22175474\n","Iteration 42, loss = 0.22135904\n","Iteration 43, loss = 0.22095569\n","Iteration 44, loss = 0.22058923\n","Iteration 45, loss = 0.22018336\n","Iteration 46, loss = 0.21983624\n","Iteration 47, loss = 0.21950683\n","Iteration 48, loss = 0.21916848\n","Iteration 49, loss = 0.21887498\n","Iteration 50, loss = 0.21854433\n","Iteration 51, loss = 0.21823915\n","Iteration 52, loss = 0.21792613\n","Iteration 53, loss = 0.21762860\n","Iteration 54, loss = 0.21732276\n","Iteration 55, loss = 0.21705943\n","Iteration 56, loss = 0.21678753\n","Iteration 57, loss = 0.21650273\n","Iteration 58, loss = 0.21620103\n","Iteration 59, loss = 0.21593939\n","Iteration 60, loss = 0.21567612\n","Iteration 61, loss = 0.21541500\n","Iteration 62, loss = 0.21515103\n","Iteration 63, loss = 0.21488269\n","Iteration 64, loss = 0.21464853\n","Iteration 65, loss = 0.21440869\n","Iteration 66, loss = 0.21418532\n","Iteration 67, loss = 0.21394744\n","Iteration 68, loss = 0.21370183\n","Iteration 69, loss = 0.21345764\n","Iteration 70, loss = 0.21324649\n","Iteration 71, loss = 0.21304160\n","Iteration 72, loss = 0.21280876\n","Iteration 73, loss = 0.21258221\n","Iteration 74, loss = 0.21238345\n","Iteration 75, loss = 0.21218074\n","Iteration 76, loss = 0.21198419\n","Iteration 77, loss = 0.21173700\n","Iteration 78, loss = 0.21155503\n","Iteration 79, loss = 0.21136943\n","Iteration 80, loss = 0.21121124\n","Iteration 81, loss = 0.21095864\n","Iteration 82, loss = 0.21076988\n","Iteration 83, loss = 0.21061623\n","Iteration 84, loss = 0.21039967\n","Iteration 85, loss = 0.21022285\n","Iteration 86, loss = 0.21004370\n","Iteration 87, loss = 0.20988167\n","Iteration 88, loss = 0.20967047\n","Iteration 89, loss = 0.20952328\n","Iteration 90, loss = 0.20939326\n","Iteration 91, loss = 0.20916736\n","Iteration 92, loss = 0.20899727\n","Iteration 93, loss = 0.20883231\n","Iteration 94, loss = 0.20868118\n","Iteration 95, loss = 0.20850238\n","Iteration 96, loss = 0.20834203\n","Iteration 97, loss = 0.20817793\n","Iteration 98, loss = 0.20801929\n","Iteration 99, loss = 0.20785667\n","Iteration 100, loss = 0.20769798\n","Iteration 101, loss = 0.20758680\n","Iteration 102, loss = 0.20737914\n","Iteration 103, loss = 0.20722876\n","Iteration 104, loss = 0.20708709\n","Iteration 105, loss = 0.20693403\n","Iteration 106, loss = 0.20679775\n","Iteration 107, loss = 0.20663432\n","Iteration 108, loss = 0.20650619\n","Iteration 109, loss = 0.20633624\n","Iteration 110, loss = 0.20621191\n","Iteration 111, loss = 0.20605162\n","Iteration 112, loss = 0.20594120\n","Iteration 113, loss = 0.20578343\n","Iteration 114, loss = 0.20563755\n","Iteration 115, loss = 0.20551673\n","Iteration 116, loss = 0.20543970\n","Iteration 117, loss = 0.20523233\n","Iteration 118, loss = 0.20510824\n","Iteration 119, loss = 0.20496276\n","Iteration 120, loss = 0.20484058\n","Iteration 121, loss = 0.20471831\n","Iteration 122, loss = 0.20457059\n","Iteration 123, loss = 0.20442070\n","Iteration 124, loss = 0.20429500\n","Iteration 125, loss = 0.20415209\n","Iteration 126, loss = 0.20405338\n","Iteration 127, loss = 0.20393660\n","Iteration 128, loss = 0.20378535\n","Iteration 129, loss = 0.20364244\n","Iteration 130, loss = 0.20352529\n","Iteration 131, loss = 0.20339410\n","Iteration 132, loss = 0.20329046\n","Iteration 133, loss = 0.20318918\n","Iteration 134, loss = 0.20305561\n","Iteration 135, loss = 0.20291591\n","Iteration 136, loss = 0.20278010\n","Iteration 137, loss = 0.20265826\n","Iteration 138, loss = 0.20254839\n","Iteration 139, loss = 0.20241762\n","Iteration 140, loss = 0.20228234\n","Iteration 141, loss = 0.20217238\n","Iteration 142, loss = 0.20209979\n","Iteration 143, loss = 0.20194153\n","Iteration 144, loss = 0.20181244\n","Iteration 145, loss = 0.20169849\n","Iteration 146, loss = 0.20157561\n","Iteration 147, loss = 0.20146468\n","Iteration 148, loss = 0.20135599\n","Iteration 149, loss = 0.20123939\n","Iteration 150, loss = 0.20112361\n","Iteration 1, loss = 0.59350674\n","Iteration 2, loss = 0.52135185\n","Iteration 3, loss = 0.48873522\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.46562785\n","Iteration 5, loss = 0.44690886\n","Iteration 6, loss = 0.43088425\n","Iteration 7, loss = 0.41671257\n","Iteration 8, loss = 0.40350630\n","Iteration 9, loss = 0.39105158\n","Iteration 10, loss = 0.37893698\n","Iteration 11, loss = 0.36674354\n","Iteration 12, loss = 0.35440124\n","Iteration 13, loss = 0.34174044\n","Iteration 14, loss = 0.32899611\n","Iteration 15, loss = 0.31637550\n","Iteration 16, loss = 0.30427460\n","Iteration 17, loss = 0.29317046\n","Iteration 18, loss = 0.28298305\n","Iteration 19, loss = 0.27407863\n","Iteration 20, loss = 0.26643751\n","Iteration 21, loss = 0.25979050\n","Iteration 22, loss = 0.25424349\n","Iteration 23, loss = 0.24943434\n","Iteration 24, loss = 0.24540215\n","Iteration 25, loss = 0.24189120\n","Iteration 26, loss = 0.23887785\n","Iteration 27, loss = 0.23627930\n","Iteration 28, loss = 0.23393739\n","Iteration 29, loss = 0.23192500\n","Iteration 30, loss = 0.23003477\n","Iteration 31, loss = 0.22839678\n","Iteration 32, loss = 0.22689607\n","Iteration 33, loss = 0.22550634\n","Iteration 34, loss = 0.22425443\n","Iteration 35, loss = 0.22308283\n","Iteration 36, loss = 0.22203124\n","Iteration 37, loss = 0.22100991\n","Iteration 38, loss = 0.22011366\n","Iteration 39, loss = 0.21921059\n","Iteration 40, loss = 0.21843942\n","Iteration 41, loss = 0.21763987\n","Iteration 42, loss = 0.21698154\n","Iteration 43, loss = 0.21624190\n","Iteration 44, loss = 0.21558882\n","Iteration 45, loss = 0.21500814\n","Iteration 46, loss = 0.21438658\n","Iteration 47, loss = 0.21385545\n","Iteration 48, loss = 0.21325159\n","Iteration 49, loss = 0.21276048\n","Iteration 50, loss = 0.21222026\n","Iteration 51, loss = 0.21175733\n","Iteration 52, loss = 0.21134256\n","Iteration 53, loss = 0.21081639\n","Iteration 54, loss = 0.21045790\n","Iteration 55, loss = 0.20999873\n","Iteration 56, loss = 0.20960011\n","Iteration 57, loss = 0.20916062\n","Iteration 58, loss = 0.20879543\n","Iteration 59, loss = 0.20844492\n","Iteration 60, loss = 0.20808519\n","Iteration 61, loss = 0.20778010\n","Iteration 62, loss = 0.20740629\n","Iteration 63, loss = 0.20703477\n","Iteration 64, loss = 0.20672050\n","Iteration 65, loss = 0.20640685\n","Iteration 66, loss = 0.20609944\n","Iteration 67, loss = 0.20580807\n","Iteration 68, loss = 0.20548930\n","Iteration 69, loss = 0.20519984\n","Iteration 70, loss = 0.20490621\n","Iteration 71, loss = 0.20465515\n","Iteration 72, loss = 0.20438086\n","Iteration 73, loss = 0.20411119\n","Iteration 74, loss = 0.20385924\n","Iteration 75, loss = 0.20355473\n","Iteration 76, loss = 0.20332505\n","Iteration 77, loss = 0.20303471\n","Iteration 78, loss = 0.20283189\n","Iteration 79, loss = 0.20258185\n","Iteration 80, loss = 0.20236576\n","Iteration 81, loss = 0.20209641\n","Iteration 82, loss = 0.20182043\n","Iteration 83, loss = 0.20159876\n","Iteration 84, loss = 0.20135358\n","Iteration 85, loss = 0.20116923\n","Iteration 86, loss = 0.20092078\n","Iteration 87, loss = 0.20068836\n","Iteration 88, loss = 0.20046438\n","Iteration 89, loss = 0.20022637\n","Iteration 90, loss = 0.20006695\n","Iteration 91, loss = 0.19985055\n","Iteration 92, loss = 0.19961481\n","Iteration 93, loss = 0.19938223\n","Iteration 94, loss = 0.19916311\n","Iteration 95, loss = 0.19898792\n","Iteration 96, loss = 0.19885972\n","Iteration 97, loss = 0.19856144\n","Iteration 98, loss = 0.19836068\n","Iteration 99, loss = 0.19816576\n","Iteration 100, loss = 0.19798054\n","Iteration 101, loss = 0.19779110\n","Iteration 102, loss = 0.19759552\n","Iteration 103, loss = 0.19739326\n","Iteration 104, loss = 0.19719166\n","Iteration 105, loss = 0.19700455\n","Iteration 106, loss = 0.19683978\n","Iteration 107, loss = 0.19669062\n","Iteration 108, loss = 0.19646328\n","Iteration 109, loss = 0.19635561\n","Iteration 110, loss = 0.19609233\n","Iteration 111, loss = 0.19589645\n","Iteration 112, loss = 0.19575118\n","Iteration 113, loss = 0.19557730\n","Iteration 114, loss = 0.19540005\n","Iteration 115, loss = 0.19520747\n","Iteration 116, loss = 0.19504014\n","Iteration 117, loss = 0.19488388\n","Iteration 118, loss = 0.19479558\n","Iteration 119, loss = 0.19456753\n","Iteration 120, loss = 0.19444831\n","Iteration 121, loss = 0.19422008\n","Iteration 122, loss = 0.19408496\n","Iteration 123, loss = 0.19388777\n","Iteration 124, loss = 0.19372951\n","Iteration 125, loss = 0.19359239\n","Iteration 126, loss = 0.19341014\n","Iteration 127, loss = 0.19325678\n","Iteration 128, loss = 0.19312841\n","Iteration 129, loss = 0.19298838\n","Iteration 130, loss = 0.19281879\n","Iteration 131, loss = 0.19267154\n","Iteration 132, loss = 0.19255170\n","Iteration 133, loss = 0.19236846\n","Iteration 134, loss = 0.19221978\n","Iteration 135, loss = 0.19206919\n","Iteration 136, loss = 0.19192076\n","Iteration 137, loss = 0.19177450\n","Iteration 138, loss = 0.19163724\n","Iteration 139, loss = 0.19149017\n","Iteration 140, loss = 0.19134757\n","Iteration 141, loss = 0.19121295\n","Iteration 142, loss = 0.19106341\n","Iteration 143, loss = 0.19092076\n","Iteration 144, loss = 0.19077417\n","Iteration 145, loss = 0.19068131\n","Iteration 146, loss = 0.19057106\n","Iteration 147, loss = 0.19035069\n","Iteration 148, loss = 0.19027547\n","Iteration 149, loss = 0.19011027\n","Iteration 150, loss = 0.18998501\n","Iteration 1, loss = 0.81056895\n","Iteration 2, loss = 0.66734300\n","Iteration 3, loss = 0.58511667\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.51933746\n","Iteration 5, loss = 0.46303444\n","Iteration 6, loss = 0.41565604\n","Iteration 7, loss = 0.37685468\n","Iteration 8, loss = 0.34571524\n","Iteration 9, loss = 0.32116524\n","Iteration 10, loss = 0.30249194\n","Iteration 11, loss = 0.28833928\n","Iteration 12, loss = 0.27767425\n","Iteration 13, loss = 0.26954469\n","Iteration 14, loss = 0.26312050\n","Iteration 15, loss = 0.25786550\n","Iteration 16, loss = 0.25357063\n","Iteration 17, loss = 0.24999356\n","Iteration 18, loss = 0.24692026\n","Iteration 19, loss = 0.24422205\n","Iteration 20, loss = 0.24187102\n","Iteration 21, loss = 0.23974890\n","Iteration 22, loss = 0.23785574\n","Iteration 23, loss = 0.23607552\n","Iteration 24, loss = 0.23447813\n","Iteration 25, loss = 0.23302400\n","Iteration 26, loss = 0.23161678\n","Iteration 27, loss = 0.23034458\n","Iteration 28, loss = 0.22916739\n","Iteration 29, loss = 0.22802251\n","Iteration 30, loss = 0.22699356\n","Iteration 31, loss = 0.22605751\n","Iteration 32, loss = 0.22508898\n","Iteration 33, loss = 0.22421990\n","Iteration 34, loss = 0.22337657\n","Iteration 35, loss = 0.22258990\n","Iteration 36, loss = 0.22177299\n","Iteration 37, loss = 0.22101792\n","Iteration 38, loss = 0.22030160\n","Iteration 39, loss = 0.21963442\n","Iteration 40, loss = 0.21897796\n","Iteration 41, loss = 0.21835408\n","Iteration 42, loss = 0.21771981\n","Iteration 43, loss = 0.21713903\n","Iteration 44, loss = 0.21657816\n","Iteration 45, loss = 0.21605588\n","Iteration 46, loss = 0.21548970\n","Iteration 47, loss = 0.21507117\n","Iteration 48, loss = 0.21445935\n","Iteration 49, loss = 0.21407356\n","Iteration 50, loss = 0.21354293\n","Iteration 51, loss = 0.21309787\n","Iteration 52, loss = 0.21266748\n","Iteration 53, loss = 0.21222959\n","Iteration 54, loss = 0.21177349\n","Iteration 55, loss = 0.21137018\n","Iteration 56, loss = 0.21097820\n","Iteration 57, loss = 0.21060216\n","Iteration 58, loss = 0.21023575\n","Iteration 59, loss = 0.20986932\n","Iteration 60, loss = 0.20948824\n","Iteration 61, loss = 0.20914183\n","Iteration 62, loss = 0.20881848\n","Iteration 63, loss = 0.20844090\n","Iteration 64, loss = 0.20809536\n","Iteration 65, loss = 0.20776063\n","Iteration 66, loss = 0.20742345\n","Iteration 67, loss = 0.20709880\n","Iteration 68, loss = 0.20678827\n","Iteration 69, loss = 0.20648527\n","Iteration 70, loss = 0.20617064\n","Iteration 71, loss = 0.20588272\n","Iteration 72, loss = 0.20556554\n","Iteration 73, loss = 0.20525837\n","Iteration 74, loss = 0.20496767\n","Iteration 75, loss = 0.20467485\n","Iteration 76, loss = 0.20439592\n","Iteration 77, loss = 0.20415161\n","Iteration 78, loss = 0.20385193\n","Iteration 79, loss = 0.20364716\n","Iteration 80, loss = 0.20335767\n","Iteration 81, loss = 0.20304473\n","Iteration 82, loss = 0.20278203\n","Iteration 83, loss = 0.20257027\n","Iteration 84, loss = 0.20232439\n","Iteration 85, loss = 0.20204784\n","Iteration 86, loss = 0.20179903\n","Iteration 87, loss = 0.20154274\n","Iteration 88, loss = 0.20128383\n","Iteration 89, loss = 0.20103532\n","Iteration 90, loss = 0.20083415\n","Iteration 91, loss = 0.20057764\n","Iteration 92, loss = 0.20032795\n","Iteration 93, loss = 0.20013832\n","Iteration 94, loss = 0.19991217\n","Iteration 95, loss = 0.19969763\n","Iteration 96, loss = 0.19951061\n","Iteration 97, loss = 0.19925912\n","Iteration 98, loss = 0.19904714\n","Iteration 99, loss = 0.19884770\n","Iteration 100, loss = 0.19864553\n","Iteration 101, loss = 0.19848746\n","Iteration 102, loss = 0.19822652\n","Iteration 103, loss = 0.19802510\n","Iteration 104, loss = 0.19781823\n","Iteration 105, loss = 0.19766860\n","Iteration 106, loss = 0.19744896\n","Iteration 107, loss = 0.19730858\n","Iteration 108, loss = 0.19712471\n","Iteration 109, loss = 0.19686513\n","Iteration 110, loss = 0.19667780\n","Iteration 111, loss = 0.19654722\n","Iteration 112, loss = 0.19629156\n","Iteration 113, loss = 0.19612606\n","Iteration 114, loss = 0.19594921\n","Iteration 115, loss = 0.19574938\n","Iteration 116, loss = 0.19556881\n","Iteration 117, loss = 0.19539373\n","Iteration 118, loss = 0.19522175\n","Iteration 119, loss = 0.19505657\n","Iteration 120, loss = 0.19486535\n","Iteration 121, loss = 0.19467529\n","Iteration 122, loss = 0.19451783\n","Iteration 123, loss = 0.19438112\n","Iteration 124, loss = 0.19419274\n","Iteration 125, loss = 0.19404066\n","Iteration 126, loss = 0.19387052\n","Iteration 127, loss = 0.19368296\n","Iteration 128, loss = 0.19356261\n","Iteration 129, loss = 0.19335323\n","Iteration 130, loss = 0.19324099\n","Iteration 131, loss = 0.19305125\n","Iteration 132, loss = 0.19290352\n","Iteration 133, loss = 0.19276234\n","Iteration 134, loss = 0.19258524\n","Iteration 135, loss = 0.19241178\n","Iteration 136, loss = 0.19227636\n","Iteration 137, loss = 0.19211678\n","Iteration 138, loss = 0.19194824\n","Iteration 139, loss = 0.19181181\n","Iteration 140, loss = 0.19165723\n","Iteration 141, loss = 0.19152231\n","Iteration 142, loss = 0.19137004\n","Iteration 143, loss = 0.19119988\n","Iteration 144, loss = 0.19107345\n","Iteration 145, loss = 0.19093849\n","Iteration 146, loss = 0.19080599\n","Iteration 147, loss = 0.19061391\n","Iteration 148, loss = 0.19048777\n","Iteration 149, loss = 0.19033627\n","Iteration 150, loss = 0.19018161\n","Iteration 1, loss = 0.91086727\n","Iteration 2, loss = 0.61980728\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.49561454\n","Iteration 4, loss = 0.42107635\n","Iteration 5, loss = 0.37078139\n","Iteration 6, loss = 0.33588058\n","Iteration 7, loss = 0.31121411\n","Iteration 8, loss = 0.29325386\n","Iteration 9, loss = 0.28010886\n","Iteration 10, loss = 0.27011594\n","Iteration 11, loss = 0.26241769\n","Iteration 12, loss = 0.25627684\n","Iteration 13, loss = 0.25134769\n","Iteration 14, loss = 0.24722375\n","Iteration 15, loss = 0.24382452\n","Iteration 16, loss = 0.24076152\n","Iteration 17, loss = 0.23821451\n","Iteration 18, loss = 0.23594154\n","Iteration 19, loss = 0.23391743\n","Iteration 20, loss = 0.23212790\n","Iteration 21, loss = 0.23051573\n","Iteration 22, loss = 0.22904000\n","Iteration 23, loss = 0.22777121\n","Iteration 24, loss = 0.22654615\n","Iteration 25, loss = 0.22542790\n","Iteration 26, loss = 0.22442091\n","Iteration 27, loss = 0.22344142\n","Iteration 28, loss = 0.22254877\n","Iteration 29, loss = 0.22172399\n","Iteration 30, loss = 0.22093425\n","Iteration 31, loss = 0.22021188\n","Iteration 32, loss = 0.21948343\n","Iteration 33, loss = 0.21882321\n","Iteration 34, loss = 0.21819584\n","Iteration 35, loss = 0.21758312\n","Iteration 36, loss = 0.21703173\n","Iteration 37, loss = 0.21648747\n","Iteration 38, loss = 0.21596083\n","Iteration 39, loss = 0.21543826\n","Iteration 40, loss = 0.21497340\n","Iteration 41, loss = 0.21450498\n","Iteration 42, loss = 0.21406165\n","Iteration 43, loss = 0.21369286\n","Iteration 44, loss = 0.21320169\n","Iteration 45, loss = 0.21282267\n","Iteration 46, loss = 0.21245981\n","Iteration 47, loss = 0.21201344\n","Iteration 48, loss = 0.21165056\n","Iteration 49, loss = 0.21129938\n","Iteration 50, loss = 0.21094304\n","Iteration 51, loss = 0.21063124\n","Iteration 52, loss = 0.21031685\n","Iteration 53, loss = 0.20995961\n","Iteration 54, loss = 0.20963828\n","Iteration 55, loss = 0.20931065\n","Iteration 56, loss = 0.20902886\n","Iteration 57, loss = 0.20870535\n","Iteration 58, loss = 0.20838270\n","Iteration 59, loss = 0.20809461\n","Iteration 60, loss = 0.20785964\n","Iteration 61, loss = 0.20750266\n","Iteration 62, loss = 0.20724997\n","Iteration 63, loss = 0.20700027\n","Iteration 64, loss = 0.20674371\n","Iteration 65, loss = 0.20646143\n","Iteration 66, loss = 0.20619330\n","Iteration 67, loss = 0.20593421\n","Iteration 68, loss = 0.20574550\n","Iteration 69, loss = 0.20548098\n","Iteration 70, loss = 0.20522657\n","Iteration 71, loss = 0.20501641\n","Iteration 72, loss = 0.20476568\n","Iteration 73, loss = 0.20455715\n","Iteration 74, loss = 0.20435386\n","Iteration 75, loss = 0.20414226\n","Iteration 76, loss = 0.20390165\n","Iteration 77, loss = 0.20369569\n","Iteration 78, loss = 0.20348489\n","Iteration 79, loss = 0.20327407\n","Iteration 80, loss = 0.20307504\n","Iteration 81, loss = 0.20286968\n","Iteration 82, loss = 0.20264879\n","Iteration 83, loss = 0.20246072\n","Iteration 84, loss = 0.20226918\n","Iteration 85, loss = 0.20211733\n","Iteration 86, loss = 0.20189454\n","Iteration 87, loss = 0.20169384\n","Iteration 88, loss = 0.20153110\n","Iteration 89, loss = 0.20135814\n","Iteration 90, loss = 0.20117402\n","Iteration 91, loss = 0.20097899\n","Iteration 92, loss = 0.20078796\n","Iteration 93, loss = 0.20064399\n","Iteration 94, loss = 0.20044368\n","Iteration 95, loss = 0.20027619\n","Iteration 96, loss = 0.20009998\n","Iteration 97, loss = 0.19993651\n","Iteration 98, loss = 0.19974552\n","Iteration 99, loss = 0.19957375\n","Iteration 100, loss = 0.19941266\n","Iteration 101, loss = 0.19925864\n","Iteration 102, loss = 0.19909222\n","Iteration 103, loss = 0.19893576\n","Iteration 104, loss = 0.19871294\n","Iteration 105, loss = 0.19857067\n","Iteration 106, loss = 0.19841741\n","Iteration 107, loss = 0.19825114\n","Iteration 108, loss = 0.19808708\n","Iteration 109, loss = 0.19792301\n","Iteration 110, loss = 0.19776010\n","Iteration 111, loss = 0.19758547\n","Iteration 112, loss = 0.19746061\n","Iteration 113, loss = 0.19729740\n","Iteration 114, loss = 0.19714594\n","Iteration 115, loss = 0.19696939\n","Iteration 116, loss = 0.19680163\n","Iteration 117, loss = 0.19667295\n","Iteration 118, loss = 0.19652777\n","Iteration 119, loss = 0.19636676\n","Iteration 120, loss = 0.19623232\n","Iteration 121, loss = 0.19608020\n","Iteration 122, loss = 0.19592207\n","Iteration 123, loss = 0.19579016\n","Iteration 124, loss = 0.19561965\n","Iteration 125, loss = 0.19547691\n","Iteration 126, loss = 0.19535341\n","Iteration 127, loss = 0.19517008\n","Iteration 128, loss = 0.19505785\n","Iteration 129, loss = 0.19492831\n","Iteration 130, loss = 0.19478059\n","Iteration 131, loss = 0.19463312\n","Iteration 132, loss = 0.19449190\n","Iteration 133, loss = 0.19434811\n","Iteration 134, loss = 0.19420481\n","Iteration 135, loss = 0.19406082\n","Iteration 136, loss = 0.19393995\n","Iteration 137, loss = 0.19380393\n","Iteration 138, loss = 0.19367491\n","Iteration 139, loss = 0.19353175\n","Iteration 140, loss = 0.19339072\n","Iteration 141, loss = 0.19328847\n","Iteration 142, loss = 0.19314353\n","Iteration 143, loss = 0.19301266\n","Iteration 144, loss = 0.19287454\n","Iteration 145, loss = 0.19275141\n","Iteration 146, loss = 0.19258774\n","Iteration 147, loss = 0.19248320\n","Iteration 148, loss = 0.19237861\n","Iteration 149, loss = 0.19221658\n","Iteration 150, loss = 0.19207661\n","Iteration 1, loss = 0.50172810\n","Iteration 2, loss = 0.29255849\n","Iteration 3, loss = 0.23899428"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration 4, loss = 0.22327495\n","Iteration 5, loss = 0.21565960\n","Iteration 6, loss = 0.21143941\n","Iteration 7, loss = 0.20716148\n","Iteration 8, loss = 0.20413287\n","Iteration 9, loss = 0.20145498\n","Iteration 10, loss = 0.19926781\n","Iteration 11, loss = 0.19726154\n","Iteration 12, loss = 0.19530765\n","Iteration 13, loss = 0.19359048\n","Iteration 14, loss = 0.19137059\n","Iteration 15, loss = 0.19042635\n","Iteration 16, loss = 0.18838213\n","Iteration 17, loss = 0.18725135\n","Iteration 18, loss = 0.18538963\n","Iteration 19, loss = 0.18432309\n","Iteration 20, loss = 0.18238269\n","Iteration 21, loss = 0.18073595\n","Iteration 22, loss = 0.17960294\n","Iteration 23, loss = 0.17830813\n","Iteration 24, loss = 0.17674127\n","Iteration 25, loss = 0.17498968\n","Iteration 26, loss = 0.17452845\n","Iteration 27, loss = 0.17226451\n","Iteration 28, loss = 0.17106698\n","Iteration 29, loss = 0.16965582\n","Iteration 30, loss = 0.16773200\n","Iteration 31, loss = 0.16722989\n","Iteration 32, loss = 0.16564892\n","Iteration 33, loss = 0.16373751\n","Iteration 34, loss = 0.16246624\n","Iteration 35, loss = 0.16221624\n","Iteration 36, loss = 0.15977282\n","Iteration 37, loss = 0.15834534\n","Iteration 38, loss = 0.15763254\n","Iteration 39, loss = 0.15591138\n","Iteration 40, loss = 0.15458431\n","Iteration 41, loss = 0.15361000\n","Iteration 42, loss = 0.15236060\n","Iteration 43, loss = 0.15086715\n","Iteration 44, loss = 0.14885579\n","Iteration 45, loss = 0.14836677\n","Iteration 46, loss = 0.14707143\n","Iteration 47, loss = 0.14585028\n","Iteration 48, loss = 0.14421219\n","Iteration 49, loss = 0.14243277\n","Iteration 50, loss = 0.14188533\n","Iteration 51, loss = 0.14119589\n","Iteration 52, loss = 0.13925731\n","Iteration 53, loss = 0.13764775\n","Iteration 54, loss = 0.13664584\n","Iteration 55, loss = 0.13514706\n","Iteration 56, loss = 0.13420540\n","Iteration 57, loss = 0.13306209\n","Iteration 58, loss = 0.13263452\n","Iteration 59, loss = 0.13167487\n","Iteration 60, loss = 0.12983375\n","Iteration 61, loss = 0.12895886\n","Iteration 62, loss = 0.12806901\n","Iteration 63, loss = 0.12705783\n","Iteration 64, loss = 0.12661399\n","Iteration 65, loss = 0.12633607\n","Iteration 66, loss = 0.12393870\n","Iteration 67, loss = 0.12312000\n","Iteration 68, loss = 0.12247825\n","Iteration 69, loss = 0.12081976\n","Iteration 70, loss = 0.12046817\n","Iteration 71, loss = 0.11911095\n","Iteration 72, loss = 0.11881711\n","Iteration 73, loss = 0.11708677\n","Iteration 74, loss = 0.11625596\n","Iteration 75, loss = 0.11555737\n","Iteration 76, loss = 0.11401029\n","Iteration 77, loss = 0.11410269\n","Iteration 78, loss = 0.11250755\n","Iteration 79, loss = 0.11144048\n","Iteration 80, loss = 0.11235196\n","Iteration 81, loss = 0.11094187\n","Iteration 82, loss = 0.11005732\n","Iteration 83, loss = 0.10971354\n","Iteration 84, loss = 0.10815939\n","Iteration 85, loss = 0.10748869\n","Iteration 86, loss = 0.10647400\n","Iteration 87, loss = 0.10462244\n","Iteration 88, loss = 0.10421146\n","Iteration 89, loss = 0.10392456\n","Iteration 90, loss = 0.10318055\n","Iteration 91, loss = 0.10341035\n","Iteration 92, loss = 0.10246984\n","Iteration 93, loss = 0.10188155\n","Iteration 94, loss = 0.10075941\n","Iteration 95, loss = 0.10002655\n","Iteration 96, loss = 0.09858029\n","Iteration 97, loss = 0.09733495\n","Iteration 98, loss = 0.09767951\n","Iteration 99, loss = 0.09639982\n","Iteration 100, loss = 0.09585421\n","Iteration 101, loss = 0.09573246\n","Iteration 102, loss = 0.09438064\n","Iteration 103, loss = 0.09442763\n","Iteration 104, loss = 0.09242367\n","Iteration 105, loss = 0.09255023\n","Iteration 106, loss = 0.09160502\n","Iteration 107, loss = 0.09077957\n","Iteration 108, loss = 0.09081034\n","Iteration 109, loss = 0.08898730\n","Iteration 110, loss = 0.08857293\n","Iteration 111, loss = 0.08876121\n","Iteration 112, loss = 0.08764426\n","Iteration 113, loss = 0.08651172\n","Iteration 114, loss = 0.08601322\n","Iteration 115, loss = 0.08586943\n","Iteration 116, loss = 0.08562141\n","Iteration 117, loss = 0.08432976\n","Iteration 118, loss = 0.08397271\n","Iteration 119, loss = 0.08378434\n","Iteration 120, loss = 0.08208720\n","Iteration 121, loss = 0.08177054\n","Iteration 122, loss = 0.08127361\n","Iteration 123, loss = 0.08134933\n","Iteration 124, loss = 0.08061528\n","Iteration 125, loss = 0.07977476\n","Iteration 126, loss = 0.07871812\n","Iteration 127, loss = 0.07761638\n","Iteration 128, loss = 0.07838762\n","Iteration 129, loss = 0.07744952\n","Iteration 130, loss = 0.07738284\n","Iteration 131, loss = 0.07643615\n","Iteration 132, loss = 0.07569750\n","Iteration 133, loss = 0.07633220\n","Iteration 134, loss = 0.07538866\n","Iteration 135, loss = 0.07547333\n","Iteration 136, loss = 0.07405925\n","Iteration 137, loss = 0.07300961\n","Iteration 138, loss = 0.07371443\n","Iteration 139, loss = 0.07243227\n","Iteration 140, loss = 0.07175520\n","Iteration 141, loss = 0.07231505\n","Iteration 142, loss = 0.07079525\n","Iteration 143, loss = 0.07065389\n","Iteration 144, loss = 0.07104438\n","Iteration 145, loss = 0.07040033\n","Iteration 146, loss = 0.06962108\n","Iteration 147, loss = 0.06870942\n","Iteration 148, loss = 0.06872226\n","Iteration 149, loss = 0.06793182\n","Iteration 150, loss = 0.06711155\n","Iteration 1, loss = 0.67372933\n","Iteration 2, loss = 0.30865726\n","Iteration 3, loss = 0.25420327\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.23415592\n","Iteration 5, loss = 0.22386218\n","Iteration 6, loss = 0.21773629\n","Iteration 7, loss = 0.21307698\n","Iteration 8, loss = 0.20954404\n","Iteration 9, loss = 0.20603303\n","Iteration 10, loss = 0.20324355\n","Iteration 11, loss = 0.20132988\n","Iteration 12, loss = 0.19929182\n","Iteration 13, loss = 0.19705226\n","Iteration 14, loss = 0.19536820\n","Iteration 15, loss = 0.19294881\n","Iteration 16, loss = 0.19142593\n","Iteration 17, loss = 0.18934864\n","Iteration 18, loss = 0.18779531\n","Iteration 19, loss = 0.18643861\n","Iteration 20, loss = 0.18515558\n","Iteration 21, loss = 0.18399342\n","Iteration 22, loss = 0.18175947\n","Iteration 23, loss = 0.18069932\n","Iteration 24, loss = 0.17901549\n","Iteration 25, loss = 0.17776814\n","Iteration 26, loss = 0.17607903\n","Iteration 27, loss = 0.17484660\n","Iteration 28, loss = 0.17377437\n","Iteration 29, loss = 0.17199645\n","Iteration 30, loss = 0.17059925\n","Iteration 31, loss = 0.16983595\n","Iteration 32, loss = 0.16775719\n","Iteration 33, loss = 0.16637385\n","Iteration 34, loss = 0.16542742\n","Iteration 35, loss = 0.16391878\n","Iteration 36, loss = 0.16224807\n","Iteration 37, loss = 0.16160685\n","Iteration 38, loss = 0.16017868\n","Iteration 39, loss = 0.15848686\n","Iteration 40, loss = 0.15720191\n","Iteration 41, loss = 0.15596444\n","Iteration 42, loss = 0.15461834\n","Iteration 43, loss = 0.15354128\n","Iteration 44, loss = 0.15301420\n","Iteration 45, loss = 0.15241364\n","Iteration 46, loss = 0.14970369\n","Iteration 47, loss = 0.14877314\n","Iteration 48, loss = 0.14821320\n","Iteration 49, loss = 0.14601479\n","Iteration 50, loss = 0.14446369\n","Iteration 51, loss = 0.14439773\n","Iteration 52, loss = 0.14326372\n","Iteration 53, loss = 0.14147042\n","Iteration 54, loss = 0.14034064\n","Iteration 55, loss = 0.13906168\n","Iteration 56, loss = 0.13772646\n","Iteration 57, loss = 0.13675226\n","Iteration 58, loss = 0.13536400\n","Iteration 59, loss = 0.13477692\n","Iteration 60, loss = 0.13388265\n","Iteration 61, loss = 0.13253996\n","Iteration 62, loss = 0.13158240\n","Iteration 63, loss = 0.13132134\n","Iteration 64, loss = 0.12885073\n","Iteration 65, loss = 0.12839769\n","Iteration 66, loss = 0.12701235\n","Iteration 67, loss = 0.12572764\n","Iteration 68, loss = 0.12516135\n","Iteration 69, loss = 0.12444424\n","Iteration 70, loss = 0.12306430\n","Iteration 71, loss = 0.12219476\n","Iteration 72, loss = 0.12150950\n","Iteration 73, loss = 0.12003449\n","Iteration 74, loss = 0.11934557\n","Iteration 75, loss = 0.11912475\n","Iteration 76, loss = 0.11838101\n","Iteration 77, loss = 0.11636364\n","Iteration 78, loss = 0.11555637\n","Iteration 79, loss = 0.11501322\n","Iteration 80, loss = 0.11475048\n","Iteration 81, loss = 0.11294266\n","Iteration 82, loss = 0.11293277\n","Iteration 83, loss = 0.11118547\n","Iteration 84, loss = 0.11170824\n","Iteration 85, loss = 0.11018284\n","Iteration 86, loss = 0.10809411\n","Iteration 87, loss = 0.10849860\n","Iteration 88, loss = 0.10686148\n","Iteration 89, loss = 0.10687337\n","Iteration 90, loss = 0.10576403\n","Iteration 91, loss = 0.10506980\n","Iteration 92, loss = 0.10376130\n","Iteration 93, loss = 0.10380995\n","Iteration 94, loss = 0.10216073\n","Iteration 95, loss = 0.10091241\n","Iteration 96, loss = 0.10111886\n","Iteration 97, loss = 0.09966091\n","Iteration 98, loss = 0.09912473\n","Iteration 99, loss = 0.09844754\n","Iteration 100, loss = 0.09757126\n","Iteration 101, loss = 0.09712106\n","Iteration 102, loss = 0.09621642\n","Iteration 103, loss = 0.09564387\n","Iteration 104, loss = 0.09403567\n","Iteration 105, loss = 0.09332374\n","Iteration 106, loss = 0.09584766\n","Iteration 107, loss = 0.09309531\n","Iteration 108, loss = 0.09164284\n","Iteration 109, loss = 0.09209739\n","Iteration 110, loss = 0.09022989\n","Iteration 111, loss = 0.09014997\n","Iteration 112, loss = 0.08952511\n","Iteration 113, loss = 0.08791489\n","Iteration 114, loss = 0.08773098\n","Iteration 115, loss = 0.08758564\n","Iteration 116, loss = 0.08617634\n","Iteration 117, loss = 0.08593328\n","Iteration 118, loss = 0.08507380\n","Iteration 119, loss = 0.08490186\n","Iteration 120, loss = 0.08335056\n","Iteration 121, loss = 0.08359137\n","Iteration 122, loss = 0.08275281\n","Iteration 123, loss = 0.08199135\n","Iteration 124, loss = 0.08196964\n","Iteration 125, loss = 0.08043633\n","Iteration 126, loss = 0.08047498\n","Iteration 127, loss = 0.08123098\n","Iteration 128, loss = 0.07946112\n","Iteration 129, loss = 0.07833551\n","Iteration 130, loss = 0.07769793\n","Iteration 131, loss = 0.07790886\n","Iteration 132, loss = 0.07674822\n","Iteration 133, loss = 0.07666131\n","Iteration 134, loss = 0.07656045\n","Iteration 135, loss = 0.07506654\n","Iteration 136, loss = 0.07493672\n","Iteration 137, loss = 0.07401931\n","Iteration 138, loss = 0.07260520\n","Iteration 139, loss = 0.07182009\n","Iteration 140, loss = 0.07168829\n","Iteration 141, loss = 0.07161490\n","Iteration 142, loss = 0.07021947\n","Iteration 143, loss = 0.07019892\n","Iteration 144, loss = 0.06931165\n","Iteration 145, loss = 0.06904539\n","Iteration 146, loss = 0.06895046\n","Iteration 147, loss = 0.06887828\n","Iteration 148, loss = 0.06850315\n","Iteration 149, loss = 0.06859589\n","Iteration 150, loss = 0.06643884\n","Iteration 1, loss = 0.53326419\n","Iteration 2, loss = 0.40232450\n","Iteration 3, loss = 0.27889706\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.22942247\n","Iteration 5, loss = 0.21587936\n","Iteration 6, loss = 0.20907107\n","Iteration 7, loss = 0.20446942\n","Iteration 8, loss = 0.20071170\n","Iteration 9, loss = 0.19777399\n","Iteration 10, loss = 0.19529263\n","Iteration 11, loss = 0.19309076\n","Iteration 12, loss = 0.19131380\n","Iteration 13, loss = 0.18924623\n","Iteration 14, loss = 0.18781715\n","Iteration 15, loss = 0.18575488\n","Iteration 16, loss = 0.18346734\n","Iteration 17, loss = 0.18175598\n","Iteration 18, loss = 0.17994253\n","Iteration 19, loss = 0.17894739\n","Iteration 20, loss = 0.17685188\n","Iteration 21, loss = 0.17514383\n","Iteration 22, loss = 0.17398256\n","Iteration 23, loss = 0.17262433\n","Iteration 24, loss = 0.17066819\n","Iteration 25, loss = 0.16919481\n","Iteration 26, loss = 0.16764405\n","Iteration 27, loss = 0.16620838\n","Iteration 28, loss = 0.16478821\n","Iteration 29, loss = 0.16278545\n","Iteration 30, loss = 0.16210405\n","Iteration 31, loss = 0.16039129\n","Iteration 32, loss = 0.15841553\n","Iteration 33, loss = 0.15694679\n","Iteration 34, loss = 0.15520674\n","Iteration 35, loss = 0.15385211\n","Iteration 36, loss = 0.15285730\n","Iteration 37, loss = 0.15134984\n","Iteration 38, loss = 0.14953847\n","Iteration 39, loss = 0.14791116\n","Iteration 40, loss = 0.14836485\n","Iteration 41, loss = 0.14540898\n","Iteration 42, loss = 0.14491698\n","Iteration 43, loss = 0.14188341\n","Iteration 44, loss = 0.14125426\n","Iteration 45, loss = 0.14011683\n","Iteration 46, loss = 0.13804618\n","Iteration 47, loss = 0.13672541\n","Iteration 48, loss = 0.13488610\n","Iteration 49, loss = 0.13446327\n","Iteration 50, loss = 0.13218498\n","Iteration 51, loss = 0.13082664\n","Iteration 52, loss = 0.13098647\n","Iteration 53, loss = 0.12839509\n","Iteration 54, loss = 0.12737615\n","Iteration 55, loss = 0.12619421\n","Iteration 56, loss = 0.12459937\n","Iteration 57, loss = 0.12305835\n","Iteration 58, loss = 0.12227305\n","Iteration 59, loss = 0.12194070\n","Iteration 60, loss = 0.12060494\n","Iteration 61, loss = 0.11897694\n","Iteration 62, loss = 0.11781042\n","Iteration 63, loss = 0.11563057\n","Iteration 64, loss = 0.11603997\n","Iteration 65, loss = 0.11567885\n","Iteration 66, loss = 0.11345666\n","Iteration 67, loss = 0.11228854\n","Iteration 68, loss = 0.11225231\n","Iteration 69, loss = 0.11020657\n","Iteration 70, loss = 0.10928855\n","Iteration 71, loss = 0.10987806\n","Iteration 72, loss = 0.10735978\n","Iteration 73, loss = 0.10602376\n","Iteration 74, loss = 0.10595004\n","Iteration 75, loss = 0.10476999\n","Iteration 76, loss = 0.10407982\n","Iteration 77, loss = 0.10266552\n","Iteration 78, loss = 0.10206271\n","Iteration 79, loss = 0.10052802\n","Iteration 80, loss = 0.09989327\n","Iteration 81, loss = 0.09940181\n","Iteration 82, loss = 0.09792320\n","Iteration 83, loss = 0.09802366\n","Iteration 84, loss = 0.09570402\n","Iteration 85, loss = 0.09671773\n","Iteration 86, loss = 0.09483463\n","Iteration 87, loss = 0.09356144\n","Iteration 88, loss = 0.09273608\n","Iteration 89, loss = 0.09171614\n","Iteration 90, loss = 0.09058212\n","Iteration 91, loss = 0.09061669\n","Iteration 92, loss = 0.09132776\n","Iteration 93, loss = 0.08967133\n","Iteration 94, loss = 0.08858864\n","Iteration 95, loss = 0.08643775\n","Iteration 96, loss = 0.08705050\n","Iteration 97, loss = 0.08590631\n","Iteration 98, loss = 0.08488699\n","Iteration 99, loss = 0.08376461\n","Iteration 100, loss = 0.08362928\n","Iteration 101, loss = 0.08199614\n","Iteration 102, loss = 0.08198720\n","Iteration 103, loss = 0.08128120\n","Iteration 104, loss = 0.07959996\n","Iteration 105, loss = 0.07835485\n","Iteration 106, loss = 0.07896788\n","Iteration 107, loss = 0.07789403\n","Iteration 108, loss = 0.07774408\n","Iteration 109, loss = 0.07690991\n","Iteration 110, loss = 0.07588097\n","Iteration 111, loss = 0.07486574\n","Iteration 112, loss = 0.07440903\n","Iteration 113, loss = 0.07353174\n","Iteration 114, loss = 0.07331471\n","Iteration 115, loss = 0.07307335\n","Iteration 116, loss = 0.07219819\n","Iteration 117, loss = 0.07100424\n","Iteration 118, loss = 0.07061190\n","Iteration 119, loss = 0.07104682\n","Iteration 120, loss = 0.06862614\n","Iteration 121, loss = 0.06961751\n","Iteration 122, loss = 0.06825958\n","Iteration 123, loss = 0.06856546\n","Iteration 124, loss = 0.06661501\n","Iteration 125, loss = 0.06579514\n","Iteration 126, loss = 0.06530502\n","Iteration 127, loss = 0.06484829\n","Iteration 128, loss = 0.06402252\n","Iteration 129, loss = 0.06445479\n","Iteration 130, loss = 0.06287262\n","Iteration 131, loss = 0.06243378\n","Iteration 132, loss = 0.06206674\n","Iteration 133, loss = 0.06073213\n","Iteration 134, loss = 0.06046665\n","Iteration 135, loss = 0.06011094\n","Iteration 136, loss = 0.05948549\n","Iteration 137, loss = 0.06042814\n","Iteration 138, loss = 0.05992221\n","Iteration 139, loss = 0.05739081\n","Iteration 140, loss = 0.05778774\n","Iteration 141, loss = 0.05656441\n","Iteration 142, loss = 0.05649277\n","Iteration 143, loss = 0.05619104\n","Iteration 144, loss = 0.05496336\n","Iteration 145, loss = 0.05482439\n","Iteration 146, loss = 0.05415050\n","Iteration 147, loss = 0.05462172\n","Iteration 148, loss = 0.05350250\n","Iteration 149, loss = 0.05144696\n","Iteration 150, loss = 0.05216365\n","Iteration 1, loss = 0.45774635\n","Iteration 2, loss = 0.28120013\n","Iteration 3, loss = 0.24192813\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.22648504\n","Iteration 5, loss = 0.21806591\n","Iteration 6, loss = 0.21284766\n","Iteration 7, loss = 0.20904630\n","Iteration 8, loss = 0.20554699\n","Iteration 9, loss = 0.20241786\n","Iteration 10, loss = 0.19973882\n","Iteration 11, loss = 0.19730885\n","Iteration 12, loss = 0.19543657\n","Iteration 13, loss = 0.19345472\n","Iteration 14, loss = 0.19200513\n","Iteration 15, loss = 0.19007828\n","Iteration 16, loss = 0.18789209\n","Iteration 17, loss = 0.18632065\n","Iteration 18, loss = 0.18442330\n","Iteration 19, loss = 0.18296889\n","Iteration 20, loss = 0.18111767\n","Iteration 21, loss = 0.17923831\n","Iteration 22, loss = 0.17719152\n","Iteration 23, loss = 0.17578040\n","Iteration 24, loss = 0.17434189\n","Iteration 25, loss = 0.17211462\n","Iteration 26, loss = 0.17034321\n","Iteration 27, loss = 0.16856139\n","Iteration 28, loss = 0.16707294\n","Iteration 29, loss = 0.16537060\n","Iteration 30, loss = 0.16382065\n","Iteration 31, loss = 0.16222759\n","Iteration 32, loss = 0.16056347\n","Iteration 33, loss = 0.15874822\n","Iteration 34, loss = 0.15694752\n","Iteration 35, loss = 0.15552988\n","Iteration 36, loss = 0.15450119\n","Iteration 37, loss = 0.15326663\n","Iteration 38, loss = 0.15045934\n","Iteration 39, loss = 0.14861990\n","Iteration 40, loss = 0.14736983\n","Iteration 41, loss = 0.14603993\n","Iteration 42, loss = 0.14461407\n","Iteration 43, loss = 0.14271769\n","Iteration 44, loss = 0.14160528\n","Iteration 45, loss = 0.13939294\n","Iteration 46, loss = 0.13837199\n","Iteration 47, loss = 0.13800014\n","Iteration 48, loss = 0.13514080\n","Iteration 49, loss = 0.13381954\n","Iteration 50, loss = 0.13295542\n","Iteration 51, loss = 0.13109373\n","Iteration 52, loss = 0.13030808\n","Iteration 53, loss = 0.12933940\n","Iteration 54, loss = 0.12681051\n","Iteration 55, loss = 0.12558562\n","Iteration 56, loss = 0.12428551\n","Iteration 57, loss = 0.12280187\n","Iteration 58, loss = 0.12182543\n","Iteration 59, loss = 0.12131481\n","Iteration 60, loss = 0.11881307\n","Iteration 61, loss = 0.11756573\n","Iteration 62, loss = 0.11724157\n","Iteration 63, loss = 0.11579163\n","Iteration 64, loss = 0.11370518\n","Iteration 65, loss = 0.11232314\n","Iteration 66, loss = 0.11210993\n","Iteration 67, loss = 0.11034057\n","Iteration 68, loss = 0.10950845\n","Iteration 69, loss = 0.10884248\n","Iteration 70, loss = 0.10789730\n","Iteration 71, loss = 0.10630353\n","Iteration 72, loss = 0.10455949\n","Iteration 73, loss = 0.10340652\n","Iteration 74, loss = 0.10313136\n","Iteration 75, loss = 0.10212678\n","Iteration 76, loss = 0.10105196\n","Iteration 77, loss = 0.09944405\n","Iteration 78, loss = 0.09781909\n","Iteration 79, loss = 0.09776972\n","Iteration 80, loss = 0.09539385\n","Iteration 81, loss = 0.09515864\n","Iteration 82, loss = 0.09394662\n","Iteration 83, loss = 0.09322359\n","Iteration 84, loss = 0.09196057\n","Iteration 85, loss = 0.09136502\n","Iteration 86, loss = 0.09040853\n","Iteration 87, loss = 0.08941551\n","Iteration 88, loss = 0.08840364\n","Iteration 89, loss = 0.08840787\n","Iteration 90, loss = 0.08593535\n","Iteration 91, loss = 0.08505285\n","Iteration 92, loss = 0.08397702\n","Iteration 93, loss = 0.08407591\n","Iteration 94, loss = 0.08211155\n","Iteration 95, loss = 0.08301242\n","Iteration 96, loss = 0.08246843\n","Iteration 97, loss = 0.08158528\n","Iteration 98, loss = 0.07978714\n","Iteration 99, loss = 0.07923436\n","Iteration 100, loss = 0.07768658\n","Iteration 101, loss = 0.07702123\n","Iteration 102, loss = 0.07626824\n","Iteration 103, loss = 0.07573529\n","Iteration 104, loss = 0.07410231\n","Iteration 105, loss = 0.07462396\n","Iteration 106, loss = 0.07425170\n","Iteration 107, loss = 0.07244761\n","Iteration 108, loss = 0.07257382\n","Iteration 109, loss = 0.07130493\n","Iteration 110, loss = 0.07239063\n","Iteration 111, loss = 0.07012590\n","Iteration 112, loss = 0.06972792\n","Iteration 113, loss = 0.06878041\n","Iteration 114, loss = 0.06904732\n","Iteration 115, loss = 0.06913736\n","Iteration 116, loss = 0.06719740\n","Iteration 117, loss = 0.06633492\n","Iteration 118, loss = 0.06606339\n","Iteration 119, loss = 0.06600776\n","Iteration 120, loss = 0.06561787\n","Iteration 121, loss = 0.06616658\n","Iteration 122, loss = 0.06331682\n","Iteration 123, loss = 0.06252181\n","Iteration 124, loss = 0.06180974\n","Iteration 125, loss = 0.06238766\n","Iteration 126, loss = 0.06135104\n","Iteration 127, loss = 0.06092658\n","Iteration 128, loss = 0.06024212\n","Iteration 129, loss = 0.05938066\n","Iteration 130, loss = 0.05929907\n","Iteration 131, loss = 0.05822515\n","Iteration 132, loss = 0.05809289\n","Iteration 133, loss = 0.05686814\n","Iteration 134, loss = 0.05713713\n","Iteration 135, loss = 0.05650696\n","Iteration 136, loss = 0.05622349\n","Iteration 137, loss = 0.05523976\n","Iteration 138, loss = 0.05518252\n","Iteration 139, loss = 0.05434096\n","Iteration 140, loss = 0.05392518\n","Iteration 141, loss = 0.05274804\n","Iteration 142, loss = 0.05293114\n","Iteration 143, loss = 0.05254042\n","Iteration 144, loss = 0.05174522\n","Iteration 145, loss = 0.05208742\n","Iteration 146, loss = 0.05111218\n","Iteration 147, loss = 0.05110549\n","Iteration 148, loss = 0.05029401\n","Iteration 149, loss = 0.04947433\n","Iteration 150, loss = 0.04995648\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.67513120\n","Iteration 2, loss = 0.36291253\n","Iteration 3, loss = 0.24896691\n","Iteration 4, loss = 0.22407786\n","Iteration 5, loss = 0.21538242\n","Iteration 6, loss = 0.21073072\n","Iteration 7, loss = 0.20798475\n","Iteration 8, loss = 0.20548480\n","Iteration 9, loss = 0.20327633\n","Iteration 10, loss = 0.20145326\n","Iteration 11, loss = 0.19970454\n","Iteration 12, loss = 0.19826613\n","Iteration 13, loss = 0.19700978\n","Iteration 14, loss = 0.19556326\n","Iteration 15, loss = 0.19445531\n","Iteration 16, loss = 0.19313085\n","Iteration 17, loss = 0.19186878\n","Iteration 18, loss = 0.19038057\n","Iteration 19, loss = 0.18924583\n","Iteration 20, loss = 0.18741277\n","Iteration 21, loss = 0.18643271\n","Iteration 22, loss = 0.18519007\n","Iteration 23, loss = 0.18453467\n","Iteration 24, loss = 0.18229589\n","Iteration 25, loss = 0.18124356\n","Iteration 26, loss = 0.18009389\n","Iteration 27, loss = 0.17863140\n","Iteration 28, loss = 0.17715158\n","Iteration 29, loss = 0.17565091\n","Iteration 30, loss = 0.17434252\n","Iteration 31, loss = 0.17318293\n","Iteration 32, loss = 0.17160446\n","Iteration 33, loss = 0.17051888\n","Iteration 34, loss = 0.16858006\n","Iteration 35, loss = 0.16754390\n","Iteration 36, loss = 0.16599677\n","Iteration 37, loss = 0.16448486\n","Iteration 38, loss = 0.16338667\n","Iteration 39, loss = 0.16164987\n","Iteration 40, loss = 0.16026719\n","Iteration 41, loss = 0.15929515\n","Iteration 42, loss = 0.15834031\n","Iteration 43, loss = 0.15644599\n","Iteration 44, loss = 0.15538535\n","Iteration 45, loss = 0.15400833\n","Iteration 46, loss = 0.15315445\n","Iteration 47, loss = 0.15221643\n","Iteration 48, loss = 0.15029503\n","Iteration 49, loss = 0.14960762\n","Iteration 50, loss = 0.14814245\n","Iteration 51, loss = 0.14669045\n","Iteration 52, loss = 0.14612299\n","Iteration 53, loss = 0.14471532\n","Iteration 54, loss = 0.14350662\n","Iteration 55, loss = 0.14231636\n","Iteration 56, loss = 0.14112907\n","Iteration 57, loss = 0.13980302\n","Iteration 58, loss = 0.13930864\n","Iteration 59, loss = 0.13841080\n","Iteration 60, loss = 0.13684387\n","Iteration 61, loss = 0.13576794\n","Iteration 62, loss = 0.13439368\n","Iteration 63, loss = 0.13391211\n","Iteration 64, loss = 0.13219268\n","Iteration 65, loss = 0.13110848\n","Iteration 66, loss = 0.13065942\n","Iteration 67, loss = 0.12903991\n","Iteration 68, loss = 0.12818054\n","Iteration 69, loss = 0.12646441\n","Iteration 70, loss = 0.12644340\n","Iteration 71, loss = 0.12460996\n","Iteration 72, loss = 0.12433845\n","Iteration 73, loss = 0.12295939\n","Iteration 74, loss = 0.12157100\n","Iteration 75, loss = 0.12097112\n","Iteration 76, loss = 0.12037230\n","Iteration 77, loss = 0.11829585\n","Iteration 78, loss = 0.11845453\n","Iteration 79, loss = 0.11701499\n","Iteration 80, loss = 0.11579437\n","Iteration 81, loss = 0.11678388\n","Iteration 82, loss = 0.11418229\n","Iteration 83, loss = 0.11432659\n","Iteration 84, loss = 0.11312251\n","Iteration 85, loss = 0.11224446\n","Iteration 86, loss = 0.11052768\n","Iteration 87, loss = 0.11018987\n","Iteration 88, loss = 0.10904427\n","Iteration 89, loss = 0.10834255\n","Iteration 90, loss = 0.10784642\n","Iteration 91, loss = 0.10707540\n","Iteration 92, loss = 0.10626985\n","Iteration 93, loss = 0.10491089\n","Iteration 94, loss = 0.10485124\n","Iteration 95, loss = 0.10416493\n","Iteration 96, loss = 0.10282010\n","Iteration 97, loss = 0.10274786\n","Iteration 98, loss = 0.10162428\n","Iteration 99, loss = 0.09974765\n","Iteration 100, loss = 0.10045840\n","Iteration 101, loss = 0.09938242\n","Iteration 102, loss = 0.09858703\n","Iteration 103, loss = 0.09757533\n","Iteration 104, loss = 0.09634367\n","Iteration 105, loss = 0.09547171\n","Iteration 106, loss = 0.09525165\n","Iteration 107, loss = 0.09447753\n","Iteration 108, loss = 0.09355741\n","Iteration 109, loss = 0.09277227\n","Iteration 110, loss = 0.09193120\n","Iteration 111, loss = 0.09120776\n","Iteration 112, loss = 0.08989813\n","Iteration 113, loss = 0.08911557\n","Iteration 114, loss = 0.08879553\n","Iteration 115, loss = 0.08857468\n","Iteration 116, loss = 0.08787113\n","Iteration 117, loss = 0.08725260\n","Iteration 118, loss = 0.08579720\n","Iteration 119, loss = 0.08621366\n","Iteration 120, loss = 0.08592466\n","Iteration 121, loss = 0.08441339\n","Iteration 122, loss = 0.08476114\n","Iteration 123, loss = 0.08326154\n","Iteration 124, loss = 0.08263167\n","Iteration 125, loss = 0.08205041\n","Iteration 126, loss = 0.08143177\n","Iteration 127, loss = 0.08143775\n","Iteration 128, loss = 0.07967573\n","Iteration 129, loss = 0.07873829\n","Iteration 130, loss = 0.07911035\n","Iteration 131, loss = 0.07827314\n","Iteration 132, loss = 0.07837436\n","Iteration 133, loss = 0.07723384\n","Iteration 134, loss = 0.07640793\n","Iteration 135, loss = 0.07546198\n","Iteration 136, loss = 0.07512905\n","Iteration 137, loss = 0.07522524\n","Iteration 138, loss = 0.07401861\n","Iteration 139, loss = 0.07382114\n","Iteration 140, loss = 0.07292517\n","Iteration 141, loss = 0.07327229\n","Iteration 142, loss = 0.07238627\n","Iteration 143, loss = 0.07157545\n","Iteration 144, loss = 0.07089746\n","Iteration 145, loss = 0.07087988\n","Iteration 146, loss = 0.06993156\n","Iteration 147, loss = 0.06900840\n","Iteration 148, loss = 0.06964759\n","Iteration 149, loss = 0.06919063\n","Iteration 150, loss = 0.06792246\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.60266826\n","Iteration 2, loss = 0.41156300\n","Iteration 3, loss = 0.33310742\n","Iteration 4, loss = 0.29455440\n","Iteration 5, loss = 0.27370551\n","Iteration 6, loss = 0.26115757\n","Iteration 7, loss = 0.25306436\n","Iteration 8, loss = 0.24761149\n","Iteration 9, loss = 0.24356022\n","Iteration 10, loss = 0.24045601\n","Iteration 11, loss = 0.23795491\n","Iteration 12, loss = 0.23584331\n","Iteration 13, loss = 0.23406585\n","Iteration 14, loss = 0.23253044\n","Iteration 15, loss = 0.23120589\n","Iteration 16, loss = 0.23004045\n","Iteration 17, loss = 0.22895859\n","Iteration 18, loss = 0.22800273\n","Iteration 19, loss = 0.22710864\n","Iteration 20, loss = 0.22629269\n","Iteration 21, loss = 0.22559680\n","Iteration 22, loss = 0.22483042\n","Iteration 23, loss = 0.22422972\n","Iteration 24, loss = 0.22361878\n","Iteration 25, loss = 0.22307362\n","Iteration 26, loss = 0.22254912\n","Iteration 27, loss = 0.22203995\n","Iteration 28, loss = 0.22156297\n","Iteration 29, loss = 0.22107722\n","Iteration 30, loss = 0.22064722\n","Iteration 31, loss = 0.22023566\n","Iteration 32, loss = 0.21980570\n","Iteration 33, loss = 0.21943129\n","Iteration 34, loss = 0.21905476\n","Iteration 35, loss = 0.21871340\n","Iteration 36, loss = 0.21836350\n","Iteration 37, loss = 0.21800729\n","Iteration 38, loss = 0.21769079\n","Iteration 39, loss = 0.21734079\n","Iteration 40, loss = 0.21703353\n","Iteration 41, loss = 0.21675670\n","Iteration 42, loss = 0.21642589\n","Iteration 43, loss = 0.21617384\n","Iteration 44, loss = 0.21587931\n","Iteration 45, loss = 0.21560110\n","Iteration 46, loss = 0.21533597\n","Iteration 47, loss = 0.21508220\n","Iteration 48, loss = 0.21482457\n","Iteration 49, loss = 0.21457004\n","Iteration 50, loss = 0.21432066\n","Iteration 51, loss = 0.21409517\n","Iteration 52, loss = 0.21388673\n","Iteration 53, loss = 0.21362725\n","Iteration 54, loss = 0.21337226\n","Iteration 55, loss = 0.21315209\n","Iteration 56, loss = 0.21294094\n","Iteration 57, loss = 0.21272186\n","Iteration 58, loss = 0.21255899\n","Iteration 59, loss = 0.21229395\n","Iteration 60, loss = 0.21207312\n","Iteration 61, loss = 0.21189480\n","Iteration 62, loss = 0.21171162\n","Iteration 63, loss = 0.21144814\n","Iteration 64, loss = 0.21125761\n","Iteration 65, loss = 0.21106907\n","Iteration 66, loss = 0.21089482\n","Iteration 67, loss = 0.21068259\n","Iteration 68, loss = 0.21051444\n","Iteration 69, loss = 0.21035609\n","Iteration 70, loss = 0.21015848\n","Iteration 71, loss = 0.20993872\n","Iteration 72, loss = 0.20979914\n","Iteration 73, loss = 0.20958091\n","Iteration 74, loss = 0.20942518\n","Iteration 75, loss = 0.20923315\n","Iteration 76, loss = 0.20907053\n","Iteration 77, loss = 0.20891252\n","Iteration 78, loss = 0.20868378\n","Iteration 79, loss = 0.20859305\n","Iteration 80, loss = 0.20844145\n","Iteration 81, loss = 0.20823504\n","Iteration 82, loss = 0.20809390\n","Iteration 83, loss = 0.20792628\n","Iteration 84, loss = 0.20781267\n","Iteration 85, loss = 0.20762542\n","Iteration 86, loss = 0.20744538\n","Iteration 87, loss = 0.20732236\n","Iteration 88, loss = 0.20712604\n","Iteration 89, loss = 0.20693881\n","Iteration 90, loss = 0.20681115\n","Iteration 91, loss = 0.20665545\n","Iteration 92, loss = 0.20646549\n","Iteration 93, loss = 0.20636301\n","Iteration 94, loss = 0.20615692\n","Iteration 95, loss = 0.20604370\n","Iteration 96, loss = 0.20586691\n","Iteration 97, loss = 0.20571718\n","Iteration 98, loss = 0.20558750\n","Iteration 99, loss = 0.20543264\n","Iteration 100, loss = 0.20526537\n","Iteration 101, loss = 0.20519531\n","Iteration 102, loss = 0.20498333\n","Iteration 103, loss = 0.20487601\n","Iteration 104, loss = 0.20466848\n","Iteration 105, loss = 0.20458099\n","Iteration 106, loss = 0.20445626\n","Iteration 107, loss = 0.20429359\n","Iteration 108, loss = 0.20418960\n","Iteration 109, loss = 0.20399380\n","Iteration 110, loss = 0.20389253\n","Iteration 111, loss = 0.20373983\n","Iteration 112, loss = 0.20355526\n","Iteration 113, loss = 0.20339997\n","Iteration 114, loss = 0.20330084\n","Iteration 115, loss = 0.20315110\n","Iteration 116, loss = 0.20301712\n","Iteration 117, loss = 0.20291412\n","Iteration 118, loss = 0.20278059\n","Iteration 119, loss = 0.20263171\n","Iteration 120, loss = 0.20247194\n","Iteration 121, loss = 0.20235533\n","Iteration 122, loss = 0.20220780\n","Iteration 123, loss = 0.20208849\n","Iteration 124, loss = 0.20193174\n","Iteration 125, loss = 0.20179970\n","Iteration 126, loss = 0.20164985\n","Iteration 127, loss = 0.20154622\n","Iteration 128, loss = 0.20139024\n","Iteration 129, loss = 0.20126015\n","Iteration 130, loss = 0.20112709\n","Iteration 131, loss = 0.20100614\n","Iteration 132, loss = 0.20092562\n","Iteration 133, loss = 0.20073618\n","Iteration 134, loss = 0.20064748\n","Iteration 135, loss = 0.20045676\n","Iteration 136, loss = 0.20036556\n","Iteration 137, loss = 0.20025447\n","Iteration 138, loss = 0.20010226\n","Iteration 139, loss = 0.19994162\n","Iteration 140, loss = 0.19981382\n","Iteration 141, loss = 0.19967530\n","Iteration 142, loss = 0.19956689\n","Iteration 143, loss = 0.19945854\n","Iteration 144, loss = 0.19936195\n","Iteration 145, loss = 0.19918606\n","Iteration 146, loss = 0.19907222\n","Iteration 147, loss = 0.19891991\n","Iteration 148, loss = 0.19883474\n","Iteration 149, loss = 0.19868633\n","Iteration 150, loss = 0.19854551\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.67682884\n","Iteration 2, loss = 0.46837425\n","Iteration 3, loss = 0.37873513\n","Iteration 4, loss = 0.32991418\n","Iteration 5, loss = 0.29991376\n","Iteration 6, loss = 0.28086063\n","Iteration 7, loss = 0.26789692\n","Iteration 8, loss = 0.25893238\n","Iteration 9, loss = 0.25235528\n","Iteration 10, loss = 0.24739422\n","Iteration 11, loss = 0.24344762\n","Iteration 12, loss = 0.24029492\n","Iteration 13, loss = 0.23767328\n","Iteration 14, loss = 0.23542560\n","Iteration 15, loss = 0.23352422\n","Iteration 16, loss = 0.23182490\n","Iteration 17, loss = 0.23040506\n","Iteration 18, loss = 0.22913702\n","Iteration 19, loss = 0.22796720\n","Iteration 20, loss = 0.22695903\n","Iteration 21, loss = 0.22601753\n","Iteration 22, loss = 0.22519062\n","Iteration 23, loss = 0.22442816\n","Iteration 24, loss = 0.22371029\n","Iteration 25, loss = 0.22307176\n","Iteration 26, loss = 0.22248850\n","Iteration 27, loss = 0.22194479\n","Iteration 28, loss = 0.22145696\n","Iteration 29, loss = 0.22094359\n","Iteration 30, loss = 0.22051829\n","Iteration 31, loss = 0.22004929\n","Iteration 32, loss = 0.21968623\n","Iteration 33, loss = 0.21928792\n","Iteration 34, loss = 0.21889850\n","Iteration 35, loss = 0.21856660\n","Iteration 36, loss = 0.21820590\n","Iteration 37, loss = 0.21792492\n","Iteration 38, loss = 0.21757633\n","Iteration 39, loss = 0.21725562\n","Iteration 40, loss = 0.21696545\n","Iteration 41, loss = 0.21671116\n","Iteration 42, loss = 0.21645321\n","Iteration 43, loss = 0.21615833\n","Iteration 44, loss = 0.21590842\n","Iteration 45, loss = 0.21568559\n","Iteration 46, loss = 0.21541040\n","Iteration 47, loss = 0.21516362\n","Iteration 48, loss = 0.21492642\n","Iteration 49, loss = 0.21473279\n","Iteration 50, loss = 0.21451077\n","Iteration 51, loss = 0.21425823\n","Iteration 52, loss = 0.21403917\n","Iteration 53, loss = 0.21384697\n","Iteration 54, loss = 0.21360745\n","Iteration 55, loss = 0.21345788\n","Iteration 56, loss = 0.21321226\n","Iteration 57, loss = 0.21301365\n","Iteration 58, loss = 0.21281650\n","Iteration 59, loss = 0.21265758\n","Iteration 60, loss = 0.21242502\n","Iteration 61, loss = 0.21222831\n","Iteration 62, loss = 0.21205855\n","Iteration 63, loss = 0.21189223\n","Iteration 64, loss = 0.21167652\n","Iteration 65, loss = 0.21153569\n","Iteration 66, loss = 0.21127854\n","Iteration 67, loss = 0.21114409\n","Iteration 68, loss = 0.21093537\n","Iteration 69, loss = 0.21078681\n","Iteration 70, loss = 0.21058867\n","Iteration 71, loss = 0.21051424\n","Iteration 72, loss = 0.21027596\n","Iteration 73, loss = 0.21009610\n","Iteration 74, loss = 0.20992913\n","Iteration 75, loss = 0.20976969\n","Iteration 76, loss = 0.20959458\n","Iteration 77, loss = 0.20944940\n","Iteration 78, loss = 0.20927564\n","Iteration 79, loss = 0.20913463\n","Iteration 80, loss = 0.20893590\n","Iteration 81, loss = 0.20878804\n","Iteration 82, loss = 0.20863498\n","Iteration 83, loss = 0.20847450\n","Iteration 84, loss = 0.20832305\n","Iteration 85, loss = 0.20817286\n","Iteration 86, loss = 0.20800326\n","Iteration 87, loss = 0.20788394\n","Iteration 88, loss = 0.20769866\n","Iteration 89, loss = 0.20758367\n","Iteration 90, loss = 0.20738883\n","Iteration 91, loss = 0.20726171\n","Iteration 92, loss = 0.20709549\n","Iteration 93, loss = 0.20695022\n","Iteration 94, loss = 0.20681003\n","Iteration 95, loss = 0.20663781\n","Iteration 96, loss = 0.20650502\n","Iteration 97, loss = 0.20638432\n","Iteration 98, loss = 0.20618516\n","Iteration 99, loss = 0.20602722\n","Iteration 100, loss = 0.20593019\n","Iteration 101, loss = 0.20573491\n","Iteration 102, loss = 0.20559362\n","Iteration 103, loss = 0.20546696\n","Iteration 104, loss = 0.20532595\n","Iteration 105, loss = 0.20518376\n","Iteration 106, loss = 0.20501837\n","Iteration 107, loss = 0.20488370\n","Iteration 108, loss = 0.20473738\n","Iteration 109, loss = 0.20467219\n","Iteration 110, loss = 0.20446056\n","Iteration 111, loss = 0.20431934\n","Iteration 112, loss = 0.20416706\n","Iteration 113, loss = 0.20404325\n","Iteration 114, loss = 0.20386403\n","Iteration 115, loss = 0.20372116\n","Iteration 116, loss = 0.20364866\n","Iteration 117, loss = 0.20348652\n","Iteration 118, loss = 0.20337217\n","Iteration 119, loss = 0.20319881\n","Iteration 120, loss = 0.20304308\n","Iteration 121, loss = 0.20293091\n","Iteration 122, loss = 0.20276638\n","Iteration 123, loss = 0.20263232\n","Iteration 124, loss = 0.20260123\n","Iteration 125, loss = 0.20236307\n","Iteration 126, loss = 0.20224235\n","Iteration 127, loss = 0.20213575\n","Iteration 128, loss = 0.20197778\n","Iteration 129, loss = 0.20190752\n","Iteration 130, loss = 0.20170742\n","Iteration 131, loss = 0.20158185\n","Iteration 132, loss = 0.20153132\n","Iteration 133, loss = 0.20127016\n","Iteration 134, loss = 0.20116031\n","Iteration 135, loss = 0.20104052\n","Iteration 136, loss = 0.20092093\n","Iteration 137, loss = 0.20079273\n","Iteration 138, loss = 0.20063163\n","Iteration 139, loss = 0.20052998\n","Iteration 140, loss = 0.20039741\n","Iteration 141, loss = 0.20021338\n","Iteration 142, loss = 0.20015105\n","Iteration 143, loss = 0.19998370\n","Iteration 144, loss = 0.19982412\n","Iteration 145, loss = 0.19976284\n","Iteration 146, loss = 0.19957362\n","Iteration 147, loss = 0.19947015\n","Iteration 148, loss = 0.19930077\n","Iteration 149, loss = 0.19923756\n","Iteration 150, loss = 0.19907553\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.57013900\n","Iteration 2, loss = 0.42340351\n","Iteration 3, loss = 0.35436576\n","Iteration 4, loss = 0.31616839\n","Iteration 5, loss = 0.29221118\n","Iteration 6, loss = 0.27608974\n","Iteration 7, loss = 0.26466018\n","Iteration 8, loss = 0.25627730\n","Iteration 9, loss = 0.24992645\n","Iteration 10, loss = 0.24492388\n","Iteration 11, loss = 0.24091330\n","Iteration 12, loss = 0.23759024\n","Iteration 13, loss = 0.23488278\n","Iteration 14, loss = 0.23251716\n","Iteration 15, loss = 0.23044486\n","Iteration 16, loss = 0.22867028\n","Iteration 17, loss = 0.22712517\n","Iteration 18, loss = 0.22569897\n","Iteration 19, loss = 0.22447636\n","Iteration 20, loss = 0.22338570\n","Iteration 21, loss = 0.22238935\n","Iteration 22, loss = 0.22143580\n","Iteration 23, loss = 0.22059493\n","Iteration 24, loss = 0.21979228\n","Iteration 25, loss = 0.21909474\n","Iteration 26, loss = 0.21845471\n","Iteration 27, loss = 0.21787360\n","Iteration 28, loss = 0.21723846\n","Iteration 29, loss = 0.21671039\n","Iteration 30, loss = 0.21620972\n","Iteration 31, loss = 0.21572136\n","Iteration 32, loss = 0.21527434\n","Iteration 33, loss = 0.21486807\n","Iteration 34, loss = 0.21442981\n","Iteration 35, loss = 0.21410269\n","Iteration 36, loss = 0.21373651\n","Iteration 37, loss = 0.21336360\n","Iteration 38, loss = 0.21306105\n","Iteration 39, loss = 0.21274227\n","Iteration 40, loss = 0.21247313\n","Iteration 41, loss = 0.21216719\n","Iteration 42, loss = 0.21183790\n","Iteration 43, loss = 0.21158315\n","Iteration 44, loss = 0.21131316\n","Iteration 45, loss = 0.21109516\n","Iteration 46, loss = 0.21077516\n","Iteration 47, loss = 0.21057779\n","Iteration 48, loss = 0.21034086\n","Iteration 49, loss = 0.21011450\n","Iteration 50, loss = 0.20987442\n","Iteration 51, loss = 0.20966009\n","Iteration 52, loss = 0.20940294\n","Iteration 53, loss = 0.20920983\n","Iteration 54, loss = 0.20902335\n","Iteration 55, loss = 0.20882253\n","Iteration 56, loss = 0.20858174\n","Iteration 57, loss = 0.20840802\n","Iteration 58, loss = 0.20819143\n","Iteration 59, loss = 0.20800535\n","Iteration 60, loss = 0.20780588\n","Iteration 61, loss = 0.20762046\n","Iteration 62, loss = 0.20742771\n","Iteration 63, loss = 0.20726021\n","Iteration 64, loss = 0.20702987\n","Iteration 65, loss = 0.20685437\n","Iteration 66, loss = 0.20670832\n","Iteration 67, loss = 0.20650181\n","Iteration 68, loss = 0.20633233\n","Iteration 69, loss = 0.20615402\n","Iteration 70, loss = 0.20599351\n","Iteration 71, loss = 0.20583710\n","Iteration 72, loss = 0.20564034\n","Iteration 73, loss = 0.20546016\n","Iteration 74, loss = 0.20534283\n","Iteration 75, loss = 0.20513001\n","Iteration 76, loss = 0.20500581\n","Iteration 77, loss = 0.20485085\n","Iteration 78, loss = 0.20465156\n","Iteration 79, loss = 0.20446537\n","Iteration 80, loss = 0.20434159\n","Iteration 81, loss = 0.20413416\n","Iteration 82, loss = 0.20398086\n","Iteration 83, loss = 0.20388095\n","Iteration 84, loss = 0.20368562\n","Iteration 85, loss = 0.20352138\n","Iteration 86, loss = 0.20336243\n","Iteration 87, loss = 0.20326378\n","Iteration 88, loss = 0.20304825\n","Iteration 89, loss = 0.20292148\n","Iteration 90, loss = 0.20279313\n","Iteration 91, loss = 0.20259445\n","Iteration 92, loss = 0.20244169\n","Iteration 93, loss = 0.20231181\n","Iteration 94, loss = 0.20217122\n","Iteration 95, loss = 0.20197090\n","Iteration 96, loss = 0.20186593\n","Iteration 97, loss = 0.20166903\n","Iteration 98, loss = 0.20153157\n","Iteration 99, loss = 0.20140634\n","Iteration 100, loss = 0.20128255\n","Iteration 101, loss = 0.20115450\n","Iteration 102, loss = 0.20096837\n","Iteration 103, loss = 0.20080041\n","Iteration 104, loss = 0.20065008\n","Iteration 105, loss = 0.20054803\n","Iteration 106, loss = 0.20040516\n","Iteration 107, loss = 0.20023746\n","Iteration 108, loss = 0.20011723\n","Iteration 109, loss = 0.19995675\n","Iteration 110, loss = 0.19978363\n","Iteration 111, loss = 0.19966802\n","Iteration 112, loss = 0.19951389\n","Iteration 113, loss = 0.19938730\n","Iteration 114, loss = 0.19924520\n","Iteration 115, loss = 0.19909514\n","Iteration 116, loss = 0.19891091\n","Iteration 117, loss = 0.19876371\n","Iteration 118, loss = 0.19865065\n","Iteration 119, loss = 0.19852332\n","Iteration 120, loss = 0.19834868\n","Iteration 121, loss = 0.19822326\n","Iteration 122, loss = 0.19804938\n","Iteration 123, loss = 0.19795939\n","Iteration 124, loss = 0.19782677\n","Iteration 125, loss = 0.19766908\n","Iteration 126, loss = 0.19754494\n","Iteration 127, loss = 0.19739135\n","Iteration 128, loss = 0.19727042\n","Iteration 129, loss = 0.19709599\n","Iteration 130, loss = 0.19697298\n","Iteration 131, loss = 0.19684107\n","Iteration 132, loss = 0.19673385\n","Iteration 133, loss = 0.19655998\n","Iteration 134, loss = 0.19643588\n","Iteration 135, loss = 0.19625687\n","Iteration 136, loss = 0.19612573\n","Iteration 137, loss = 0.19598131\n","Iteration 138, loss = 0.19588244\n","Iteration 139, loss = 0.19572681\n","Iteration 140, loss = 0.19559721\n","Iteration 141, loss = 0.19544107\n","Iteration 142, loss = 0.19528042\n","Iteration 143, loss = 0.19516779\n","Iteration 144, loss = 0.19503874\n","Iteration 145, loss = 0.19487997\n","Iteration 146, loss = 0.19477735\n","Iteration 147, loss = 0.19464524\n","Iteration 148, loss = 0.19447248\n","Iteration 149, loss = 0.19435249\n","Iteration 150, loss = 0.19420079\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.73651223\n","Iteration 2, loss = 0.44771686\n","Iteration 3, loss = 0.36022956\n","Iteration 4, loss = 0.31964697\n","Iteration 5, loss = 0.29552530\n","Iteration 6, loss = 0.27974497\n","Iteration 7, loss = 0.26875576\n","Iteration 8, loss = 0.26057919\n","Iteration 9, loss = 0.25437218\n","Iteration 10, loss = 0.24942509\n","Iteration 11, loss = 0.24539844\n","Iteration 12, loss = 0.24200790\n","Iteration 13, loss = 0.23914853\n","Iteration 14, loss = 0.23663198\n","Iteration 15, loss = 0.23441135\n","Iteration 16, loss = 0.23246131\n","Iteration 17, loss = 0.23073092\n","Iteration 18, loss = 0.22909458\n","Iteration 19, loss = 0.22763712\n","Iteration 20, loss = 0.22634811\n","Iteration 21, loss = 0.22515525\n","Iteration 22, loss = 0.22403020\n","Iteration 23, loss = 0.22301868\n","Iteration 24, loss = 0.22210023\n","Iteration 25, loss = 0.22123471\n","Iteration 26, loss = 0.22047346\n","Iteration 27, loss = 0.21972952\n","Iteration 28, loss = 0.21903637\n","Iteration 29, loss = 0.21843262\n","Iteration 30, loss = 0.21784487\n","Iteration 31, loss = 0.21730211\n","Iteration 32, loss = 0.21676734\n","Iteration 33, loss = 0.21623916\n","Iteration 34, loss = 0.21575213\n","Iteration 35, loss = 0.21533408\n","Iteration 36, loss = 0.21488266\n","Iteration 37, loss = 0.21445628\n","Iteration 38, loss = 0.21409753\n","Iteration 39, loss = 0.21368509\n","Iteration 40, loss = 0.21332192\n","Iteration 41, loss = 0.21293576\n","Iteration 42, loss = 0.21259747\n","Iteration 43, loss = 0.21228856\n","Iteration 44, loss = 0.21197420\n","Iteration 45, loss = 0.21162407\n","Iteration 46, loss = 0.21133291\n","Iteration 47, loss = 0.21102898\n","Iteration 48, loss = 0.21079069\n","Iteration 49, loss = 0.21048712\n","Iteration 50, loss = 0.21022500\n","Iteration 51, loss = 0.20994750\n","Iteration 52, loss = 0.20971898\n","Iteration 53, loss = 0.20945712\n","Iteration 54, loss = 0.20919306\n","Iteration 55, loss = 0.20896774\n","Iteration 56, loss = 0.20874851\n","Iteration 57, loss = 0.20848258\n","Iteration 58, loss = 0.20822124\n","Iteration 59, loss = 0.20803499\n","Iteration 60, loss = 0.20779111\n","Iteration 61, loss = 0.20756765\n","Iteration 62, loss = 0.20736833\n","Iteration 63, loss = 0.20715969\n","Iteration 64, loss = 0.20693637\n","Iteration 65, loss = 0.20668755\n","Iteration 66, loss = 0.20649102\n","Iteration 67, loss = 0.20632969\n","Iteration 68, loss = 0.20615168\n","Iteration 69, loss = 0.20593367\n","Iteration 70, loss = 0.20568152\n","Iteration 71, loss = 0.20551571\n","Iteration 72, loss = 0.20532616\n","Iteration 73, loss = 0.20509772\n","Iteration 74, loss = 0.20494100\n","Iteration 75, loss = 0.20470991\n","Iteration 76, loss = 0.20456955\n","Iteration 77, loss = 0.20438201\n","Iteration 78, loss = 0.20419133\n","Iteration 79, loss = 0.20399056\n","Iteration 80, loss = 0.20383047\n","Iteration 81, loss = 0.20365854\n","Iteration 82, loss = 0.20347397\n","Iteration 83, loss = 0.20326901\n","Iteration 84, loss = 0.20313699\n","Iteration 85, loss = 0.20300377\n","Iteration 86, loss = 0.20277140\n","Iteration 87, loss = 0.20263301\n","Iteration 88, loss = 0.20243808\n","Iteration 89, loss = 0.20229308\n","Iteration 90, loss = 0.20208568\n","Iteration 91, loss = 0.20193621\n","Iteration 92, loss = 0.20178640\n","Iteration 93, loss = 0.20153664\n","Iteration 94, loss = 0.20145858\n","Iteration 95, loss = 0.20125294\n","Iteration 96, loss = 0.20109104\n","Iteration 97, loss = 0.20096774\n","Iteration 98, loss = 0.20079629\n","Iteration 99, loss = 0.20057816\n","Iteration 100, loss = 0.20045053\n","Iteration 101, loss = 0.20029068\n","Iteration 102, loss = 0.20008751\n","Iteration 103, loss = 0.19996505\n","Iteration 104, loss = 0.19980861\n","Iteration 105, loss = 0.19965165\n","Iteration 106, loss = 0.19945605\n","Iteration 107, loss = 0.19931077\n","Iteration 108, loss = 0.19919173\n","Iteration 109, loss = 0.19900100\n","Iteration 110, loss = 0.19882162\n","Iteration 111, loss = 0.19864809\n","Iteration 112, loss = 0.19853403\n","Iteration 113, loss = 0.19836496\n","Iteration 114, loss = 0.19824488\n","Iteration 115, loss = 0.19805705\n","Iteration 116, loss = 0.19791177\n","Iteration 117, loss = 0.19773338\n","Iteration 118, loss = 0.19760200\n","Iteration 119, loss = 0.19744893\n","Iteration 120, loss = 0.19724268\n","Iteration 121, loss = 0.19713950\n","Iteration 122, loss = 0.19700391\n","Iteration 123, loss = 0.19684115\n","Iteration 124, loss = 0.19672406\n","Iteration 125, loss = 0.19653500\n","Iteration 126, loss = 0.19636587\n","Iteration 127, loss = 0.19619479\n","Iteration 128, loss = 0.19606479\n","Iteration 129, loss = 0.19590862\n","Iteration 130, loss = 0.19574170\n","Iteration 131, loss = 0.19558429\n","Iteration 132, loss = 0.19546773\n","Iteration 133, loss = 0.19533374\n","Iteration 134, loss = 0.19514848\n","Iteration 135, loss = 0.19497807\n","Iteration 136, loss = 0.19485051\n","Iteration 137, loss = 0.19469856\n","Iteration 138, loss = 0.19454020\n","Iteration 139, loss = 0.19433873\n","Iteration 140, loss = 0.19424361\n","Iteration 141, loss = 0.19406537\n","Iteration 142, loss = 0.19393957\n","Iteration 143, loss = 0.19378801\n","Iteration 144, loss = 0.19365142\n","Iteration 145, loss = 0.19348261\n","Iteration 146, loss = 0.19333225\n","Iteration 147, loss = 0.19317286\n","Iteration 148, loss = 0.19305336\n","Iteration 149, loss = 0.19289364\n","Iteration 150, loss = 0.19273291\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.65875346\n","Iteration 2, loss = 0.42767580\n","Iteration 3, loss = 0.34444530\n","Iteration 4, loss = 0.30284238\n","Iteration 5, loss = 0.27887605\n","Iteration 6, loss = 0.26384272\n","Iteration 7, loss = 0.25393223\n","Iteration 8, loss = 0.24716450\n","Iteration 9, loss = 0.24212468\n","Iteration 10, loss = 0.23832875\n","Iteration 11, loss = 0.23537203\n","Iteration 12, loss = 0.23288238\n","Iteration 13, loss = 0.23083404\n","Iteration 14, loss = 0.22902786\n","Iteration 15, loss = 0.22752532\n","Iteration 16, loss = 0.22620898\n","Iteration 17, loss = 0.22504730\n","Iteration 18, loss = 0.22398036\n","Iteration 19, loss = 0.22308018\n","Iteration 20, loss = 0.22220549\n","Iteration 21, loss = 0.22141621\n","Iteration 22, loss = 0.22069905\n","Iteration 23, loss = 0.22007287\n","Iteration 24, loss = 0.21944804\n","Iteration 25, loss = 0.21889444\n","Iteration 26, loss = 0.21836941\n","Iteration 27, loss = 0.21788682\n","Iteration 28, loss = 0.21743828\n","Iteration 29, loss = 0.21699074\n","Iteration 30, loss = 0.21656125\n","Iteration 31, loss = 0.21613359\n","Iteration 32, loss = 0.21578074\n","Iteration 33, loss = 0.21540186\n","Iteration 34, loss = 0.21504930\n","Iteration 35, loss = 0.21470462\n","Iteration 36, loss = 0.21439676\n","Iteration 37, loss = 0.21405273\n","Iteration 38, loss = 0.21378156\n","Iteration 39, loss = 0.21350252\n","Iteration 40, loss = 0.21318963\n","Iteration 41, loss = 0.21291274\n","Iteration 42, loss = 0.21265628\n","Iteration 43, loss = 0.21238078\n","Iteration 44, loss = 0.21211059\n","Iteration 45, loss = 0.21186021\n","Iteration 46, loss = 0.21160789\n","Iteration 47, loss = 0.21134360\n","Iteration 48, loss = 0.21111603\n","Iteration 49, loss = 0.21087928\n","Iteration 50, loss = 0.21061304\n","Iteration 51, loss = 0.21038249\n","Iteration 52, loss = 0.21017998\n","Iteration 53, loss = 0.20993707\n","Iteration 54, loss = 0.20973135\n","Iteration 55, loss = 0.20950779\n","Iteration 56, loss = 0.20924880\n","Iteration 57, loss = 0.20904565\n","Iteration 58, loss = 0.20880394\n","Iteration 59, loss = 0.20859592\n","Iteration 60, loss = 0.20836702\n","Iteration 61, loss = 0.20815770\n","Iteration 62, loss = 0.20797631\n","Iteration 63, loss = 0.20778630\n","Iteration 64, loss = 0.20758731\n","Iteration 65, loss = 0.20736528\n","Iteration 66, loss = 0.20716349\n","Iteration 67, loss = 0.20694885\n","Iteration 68, loss = 0.20681688\n","Iteration 69, loss = 0.20657499\n","Iteration 70, loss = 0.20638205\n","Iteration 71, loss = 0.20617579\n","Iteration 72, loss = 0.20600462\n","Iteration 73, loss = 0.20580815\n","Iteration 74, loss = 0.20566079\n","Iteration 75, loss = 0.20539636\n","Iteration 76, loss = 0.20521021\n","Iteration 77, loss = 0.20509581\n","Iteration 78, loss = 0.20485614\n","Iteration 79, loss = 0.20470073\n","Iteration 80, loss = 0.20447494\n","Iteration 81, loss = 0.20429459\n","Iteration 82, loss = 0.20410540\n","Iteration 83, loss = 0.20394684\n","Iteration 84, loss = 0.20375824\n","Iteration 85, loss = 0.20358452\n","Iteration 86, loss = 0.20342694\n","Iteration 87, loss = 0.20322580\n","Iteration 88, loss = 0.20310030\n","Iteration 89, loss = 0.20287917\n","Iteration 90, loss = 0.20274434\n","Iteration 91, loss = 0.20258861\n","Iteration 92, loss = 0.20239071\n","Iteration 93, loss = 0.20220223\n","Iteration 94, loss = 0.20205151\n","Iteration 95, loss = 0.20184749\n","Iteration 96, loss = 0.20167499\n","Iteration 97, loss = 0.20152146\n","Iteration 98, loss = 0.20140457\n","Iteration 99, loss = 0.20118939\n","Iteration 100, loss = 0.20100619\n","Iteration 101, loss = 0.20087074\n","Iteration 102, loss = 0.20068937\n","Iteration 103, loss = 0.20051476\n","Iteration 104, loss = 0.20034603\n","Iteration 105, loss = 0.20019423\n","Iteration 106, loss = 0.20003936\n","Iteration 107, loss = 0.19990368\n","Iteration 108, loss = 0.19969620\n","Iteration 109, loss = 0.19954358\n","Iteration 110, loss = 0.19938684\n","Iteration 111, loss = 0.19922919\n","Iteration 112, loss = 0.19905343\n","Iteration 113, loss = 0.19889269\n","Iteration 114, loss = 0.19869680\n","Iteration 115, loss = 0.19854725\n","Iteration 116, loss = 0.19844200\n","Iteration 117, loss = 0.19826517\n","Iteration 118, loss = 0.19804729\n","Iteration 119, loss = 0.19793930\n","Iteration 120, loss = 0.19778123\n","Iteration 121, loss = 0.19759350\n","Iteration 122, loss = 0.19745001\n","Iteration 123, loss = 0.19728896\n","Iteration 124, loss = 0.19714274\n","Iteration 125, loss = 0.19696428\n","Iteration 126, loss = 0.19683703\n","Iteration 127, loss = 0.19666179\n","Iteration 128, loss = 0.19650510\n","Iteration 129, loss = 0.19635267\n","Iteration 130, loss = 0.19618973\n","Iteration 131, loss = 0.19605749\n","Iteration 132, loss = 0.19586224\n","Iteration 133, loss = 0.19575851\n","Iteration 134, loss = 0.19561261\n","Iteration 135, loss = 0.19540208\n","Iteration 136, loss = 0.19524836\n","Iteration 137, loss = 0.19515943\n","Iteration 138, loss = 0.19495413\n","Iteration 139, loss = 0.19481110\n","Iteration 140, loss = 0.19466941\n","Iteration 141, loss = 0.19451533\n","Iteration 142, loss = 0.19436517\n","Iteration 143, loss = 0.19419129\n","Iteration 144, loss = 0.19406887\n","Iteration 145, loss = 0.19392808\n","Iteration 146, loss = 0.19380412\n","Iteration 147, loss = 0.19360200\n","Iteration 148, loss = 0.19346037\n","Iteration 149, loss = 0.19330918\n","Iteration 150, loss = 0.19316929\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.34419809\n","Iteration 2, loss = 0.22939694\n","Iteration 3, loss = 0.22091139\n","Iteration 4, loss = 0.21584872\n","Iteration 5, loss = 0.21108292\n","Iteration 6, loss = 0.20877548\n","Iteration 7, loss = 0.20491433\n","Iteration 8, loss = 0.20212523\n","Iteration 9, loss = 0.19946437\n","Iteration 10, loss = 0.19657972\n","Iteration 11, loss = 0.19460731\n","Iteration 12, loss = 0.19074867\n","Iteration 13, loss = 0.18799715\n","Iteration 14, loss = 0.18583626\n","Iteration 15, loss = 0.18328770\n","Iteration 16, loss = 0.18035203\n","Iteration 17, loss = 0.17809805\n","Iteration 18, loss = 0.17611039\n","Iteration 19, loss = 0.17223606\n","Iteration 20, loss = 0.16917485\n","Iteration 21, loss = 0.16506893\n","Iteration 22, loss = 0.16441446\n","Iteration 23, loss = 0.16008013\n","Iteration 24, loss = 0.15542016\n","Iteration 25, loss = 0.15419701\n","Iteration 26, loss = 0.14886723\n","Iteration 27, loss = 0.14671968\n","Iteration 28, loss = 0.14272214\n","Iteration 29, loss = 0.14018001\n","Iteration 30, loss = 0.13736384\n","Iteration 31, loss = 0.13346128\n","Iteration 32, loss = 0.12879999\n","Iteration 33, loss = 0.12738030\n","Iteration 34, loss = 0.12376508\n","Iteration 35, loss = 0.11874934\n","Iteration 36, loss = 0.11543760\n","Iteration 37, loss = 0.11357112\n","Iteration 38, loss = 0.10986156\n","Iteration 39, loss = 0.10611819\n","Iteration 40, loss = 0.10149517\n","Iteration 41, loss = 0.10016254\n","Iteration 42, loss = 0.09639020\n","Iteration 43, loss = 0.09277142\n","Iteration 44, loss = 0.09093920\n","Iteration 45, loss = 0.08733577\n","Iteration 46, loss = 0.08489868\n","Iteration 47, loss = 0.08242270\n","Iteration 48, loss = 0.07935688\n","Iteration 49, loss = 0.07485342\n","Iteration 50, loss = 0.07341491\n","Iteration 51, loss = 0.06995888\n","Iteration 52, loss = 0.06706342\n","Iteration 53, loss = 0.06609372\n","Iteration 54, loss = 0.06318897\n","Iteration 55, loss = 0.05875829\n","Iteration 56, loss = 0.05711531\n","Iteration 57, loss = 0.05571814\n","Iteration 58, loss = 0.05245001\n","Iteration 59, loss = 0.05028934\n","Iteration 60, loss = 0.04929433\n","Iteration 61, loss = 0.04732882\n","Iteration 62, loss = 0.04472982\n","Iteration 63, loss = 0.04199933\n","Iteration 64, loss = 0.04014649\n","Iteration 65, loss = 0.03774190\n","Iteration 66, loss = 0.03670391\n","Iteration 67, loss = 0.03467926\n","Iteration 68, loss = 0.03423987\n","Iteration 69, loss = 0.03222800\n","Iteration 70, loss = 0.03005609\n","Iteration 71, loss = 0.02815146\n","Iteration 72, loss = 0.02724736\n","Iteration 73, loss = 0.02538806\n","Iteration 74, loss = 0.02476307\n","Iteration 75, loss = 0.02267107\n","Iteration 76, loss = 0.02158679\n","Iteration 77, loss = 0.02080214\n","Iteration 78, loss = 0.01990160\n","Iteration 79, loss = 0.01911511\n","Iteration 80, loss = 0.01741583\n","Iteration 81, loss = 0.01719401\n","Iteration 82, loss = 0.01537587\n","Iteration 83, loss = 0.01476932\n","Iteration 84, loss = 0.01461942\n","Iteration 85, loss = 0.01355816\n","Iteration 86, loss = 0.01259425\n","Iteration 87, loss = 0.01181966\n","Iteration 88, loss = 0.01119353\n","Iteration 89, loss = 0.01043278\n","Iteration 90, loss = 0.01037956\n","Iteration 91, loss = 0.00962262\n","Iteration 92, loss = 0.00924096\n","Iteration 93, loss = 0.00864994\n","Iteration 94, loss = 0.00850278\n","Iteration 95, loss = 0.00768908\n","Iteration 96, loss = 0.00724729\n","Iteration 97, loss = 0.00700894\n","Iteration 98, loss = 0.00685323\n","Iteration 99, loss = 0.00639732\n","Iteration 100, loss = 0.00603643\n","Iteration 101, loss = 0.00577060\n","Iteration 102, loss = 0.00547389\n","Iteration 103, loss = 0.00518097\n","Iteration 104, loss = 0.00486913\n","Iteration 105, loss = 0.00464541\n","Iteration 106, loss = 0.00457921\n","Iteration 107, loss = 0.00437377\n","Iteration 108, loss = 0.00423328\n","Iteration 109, loss = 0.00398944\n","Iteration 110, loss = 0.00381135\n","Iteration 111, loss = 0.00358522\n","Iteration 112, loss = 0.00347055\n","Iteration 113, loss = 0.00326064\n","Iteration 114, loss = 0.00320206\n","Iteration 115, loss = 0.00302080\n","Iteration 116, loss = 0.00293442\n","Iteration 117, loss = 0.00285687\n","Iteration 118, loss = 0.00275986\n","Iteration 119, loss = 0.00259139\n","Iteration 120, loss = 0.00250599\n","Iteration 121, loss = 0.00237542\n","Iteration 122, loss = 0.00228668\n","Iteration 123, loss = 0.00212784\n","Iteration 124, loss = 0.00222555\n","Iteration 125, loss = 0.00205827\n","Iteration 126, loss = 0.00190175\n","Iteration 127, loss = 0.00183131\n","Iteration 128, loss = 0.00180826\n","Iteration 129, loss = 0.00174370\n","Iteration 130, loss = 0.00167039\n","Iteration 131, loss = 0.00158533\n","Iteration 132, loss = 0.00155850\n","Iteration 133, loss = 0.00149706\n","Iteration 134, loss = 0.00152217\n","Iteration 135, loss = 0.00142970\n","Iteration 136, loss = 0.00136392\n","Iteration 137, loss = 0.00129159\n","Iteration 138, loss = 0.00124517\n","Iteration 139, loss = 0.00122631\n","Iteration 140, loss = 0.00118121\n","Iteration 141, loss = 0.00111819\n","Iteration 142, loss = 0.00110676\n","Iteration 143, loss = 0.00107581\n","Iteration 144, loss = 0.00103746\n","Iteration 145, loss = 0.00097906\n","Iteration 146, loss = 0.00096248\n","Iteration 147, loss = 0.00094455\n","Iteration 148, loss = 0.00090579\n","Iteration 149, loss = 0.00088371\n","Iteration 150, loss = 0.00086822\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.40860042\n","Iteration 2, loss = 0.23905986\n","Iteration 3, loss = 0.22387306\n","Iteration 4, loss = 0.21878756\n","Iteration 5, loss = 0.21430583\n","Iteration 6, loss = 0.21068219\n","Iteration 7, loss = 0.20777541\n","Iteration 8, loss = 0.20483984\n","Iteration 9, loss = 0.20242279\n","Iteration 10, loss = 0.19882007\n","Iteration 11, loss = 0.19668404\n","Iteration 12, loss = 0.19421171\n","Iteration 13, loss = 0.19077682\n","Iteration 14, loss = 0.18863676\n","Iteration 15, loss = 0.18650403\n","Iteration 16, loss = 0.18299337\n","Iteration 17, loss = 0.18014486\n","Iteration 18, loss = 0.17768583\n","Iteration 19, loss = 0.17540691\n","Iteration 20, loss = 0.17213575\n","Iteration 21, loss = 0.16929138\n","Iteration 22, loss = 0.16668171\n","Iteration 23, loss = 0.16239035\n","Iteration 24, loss = 0.16117788\n","Iteration 25, loss = 0.15826416\n","Iteration 26, loss = 0.15390765\n","Iteration 27, loss = 0.15187050\n","Iteration 28, loss = 0.14943391\n","Iteration 29, loss = 0.14547514\n","Iteration 30, loss = 0.14146999\n","Iteration 31, loss = 0.13766219\n","Iteration 32, loss = 0.13582709\n","Iteration 33, loss = 0.13267462\n","Iteration 34, loss = 0.12981508\n","Iteration 35, loss = 0.12589320\n","Iteration 36, loss = 0.12235942\n","Iteration 37, loss = 0.12025226\n","Iteration 38, loss = 0.11705451\n","Iteration 39, loss = 0.11509206\n","Iteration 40, loss = 0.11074624\n","Iteration 41, loss = 0.10895904\n","Iteration 42, loss = 0.10539628\n","Iteration 43, loss = 0.10273661\n","Iteration 44, loss = 0.10037282\n","Iteration 45, loss = 0.09596262\n","Iteration 46, loss = 0.09339252\n","Iteration 47, loss = 0.09002462\n","Iteration 48, loss = 0.08661544\n","Iteration 49, loss = 0.08529299\n","Iteration 50, loss = 0.08367998\n","Iteration 51, loss = 0.07896698\n","Iteration 52, loss = 0.07660273\n","Iteration 53, loss = 0.07642210\n","Iteration 54, loss = 0.07144166\n","Iteration 55, loss = 0.07112246\n","Iteration 56, loss = 0.07026762\n","Iteration 57, loss = 0.06408207\n","Iteration 58, loss = 0.06166040\n","Iteration 59, loss = 0.05839500\n","Iteration 60, loss = 0.05869771\n","Iteration 61, loss = 0.05581069\n","Iteration 62, loss = 0.05308047\n","Iteration 63, loss = 0.05064464\n","Iteration 64, loss = 0.04844722\n","Iteration 65, loss = 0.04577600\n","Iteration 66, loss = 0.04390255\n","Iteration 67, loss = 0.04441370\n","Iteration 68, loss = 0.04158174\n","Iteration 69, loss = 0.03983854\n","Iteration 70, loss = 0.03777617\n","Iteration 71, loss = 0.03583675\n","Iteration 72, loss = 0.03466239\n","Iteration 73, loss = 0.03257511\n","Iteration 74, loss = 0.03090597\n","Iteration 75, loss = 0.02942876\n","Iteration 76, loss = 0.02771225\n","Iteration 77, loss = 0.02665460\n","Iteration 78, loss = 0.02505642\n","Iteration 79, loss = 0.02462937\n","Iteration 80, loss = 0.02352719\n","Iteration 81, loss = 0.02472244\n","Iteration 82, loss = 0.02101045\n","Iteration 83, loss = 0.01987032\n","Iteration 84, loss = 0.01962191\n","Iteration 85, loss = 0.01840952\n","Iteration 86, loss = 0.01749270\n","Iteration 87, loss = 0.01563532\n","Iteration 88, loss = 0.01444954\n","Iteration 89, loss = 0.01385109\n","Iteration 90, loss = 0.01405684\n","Iteration 91, loss = 0.01304873\n","Iteration 92, loss = 0.01271520\n","Iteration 93, loss = 0.01122682\n","Iteration 94, loss = 0.01090620\n","Iteration 95, loss = 0.01056690\n","Iteration 96, loss = 0.00992186\n","Iteration 97, loss = 0.00930676\n","Iteration 98, loss = 0.00875499\n","Iteration 99, loss = 0.00865040\n","Iteration 100, loss = 0.00784787\n","Iteration 101, loss = 0.00768639\n","Iteration 102, loss = 0.00754763\n","Iteration 103, loss = 0.00681969\n","Iteration 104, loss = 0.00626068\n","Iteration 105, loss = 0.00605393\n","Iteration 106, loss = 0.00580065\n","Iteration 107, loss = 0.00550018\n","Iteration 108, loss = 0.00533017\n","Iteration 109, loss = 0.00516830\n","Iteration 110, loss = 0.00490745\n","Iteration 111, loss = 0.00480647\n","Iteration 112, loss = 0.00436851\n","Iteration 113, loss = 0.00413816\n","Iteration 114, loss = 0.00390320\n","Iteration 115, loss = 0.00377243\n","Iteration 116, loss = 0.00371251\n","Iteration 117, loss = 0.00347647\n","Iteration 118, loss = 0.00333224\n","Iteration 119, loss = 0.00317505\n","Iteration 120, loss = 0.00301226\n","Iteration 121, loss = 0.00298593\n","Iteration 122, loss = 0.00281009\n","Iteration 123, loss = 0.00272196\n","Iteration 124, loss = 0.00263173\n","Iteration 125, loss = 0.00247236\n","Iteration 126, loss = 0.00240603\n","Iteration 127, loss = 0.00227479\n","Iteration 128, loss = 0.00219764\n","Iteration 129, loss = 0.00210771\n","Iteration 130, loss = 0.00204235\n","Iteration 131, loss = 0.00194180\n","Iteration 132, loss = 0.00183546\n","Iteration 133, loss = 0.00180812\n","Iteration 134, loss = 0.00172713\n","Iteration 135, loss = 0.00165831\n","Iteration 136, loss = 0.00160697\n","Iteration 137, loss = 0.00157094\n","Iteration 138, loss = 0.00150502\n","Iteration 139, loss = 0.00144032\n","Iteration 140, loss = 0.00140725\n","Iteration 141, loss = 0.00135612\n","Iteration 142, loss = 0.00131220\n","Iteration 143, loss = 0.00124968\n","Iteration 144, loss = 0.00120117\n","Iteration 145, loss = 0.00118583\n","Iteration 146, loss = 0.00113116\n","Iteration 147, loss = 0.00109458\n","Iteration 148, loss = 0.00107044\n","Iteration 149, loss = 0.00101264\n","Iteration 150, loss = 0.00100557\n","Iteration 1, loss = 0.31232433\n","Iteration 2, loss = 0.22228648\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.21309990\n","Iteration 4, loss = 0.20814233\n","Iteration 5, loss = 0.20520772\n","Iteration 6, loss = 0.20078671\n","Iteration 7, loss = 0.19843260\n","Iteration 8, loss = 0.19441948\n","Iteration 9, loss = 0.19187074\n","Iteration 10, loss = 0.18954614\n","Iteration 11, loss = 0.18630226\n","Iteration 12, loss = 0.18395728\n","Iteration 13, loss = 0.18069105\n","Iteration 14, loss = 0.17763593\n","Iteration 15, loss = 0.17499850\n","Iteration 16, loss = 0.17282975\n","Iteration 17, loss = 0.16991892\n","Iteration 18, loss = 0.16688584\n","Iteration 19, loss = 0.16402074\n","Iteration 20, loss = 0.16204823\n","Iteration 21, loss = 0.15829042\n","Iteration 22, loss = 0.15502681\n","Iteration 23, loss = 0.15152002\n","Iteration 24, loss = 0.14946127\n","Iteration 25, loss = 0.14517697\n","Iteration 26, loss = 0.14268020\n","Iteration 27, loss = 0.13844230\n","Iteration 28, loss = 0.13554248\n","Iteration 29, loss = 0.13126473\n","Iteration 30, loss = 0.12753204\n","Iteration 31, loss = 0.12458207\n","Iteration 32, loss = 0.12082356\n","Iteration 33, loss = 0.11816870\n","Iteration 34, loss = 0.11342196\n","Iteration 35, loss = 0.11063644\n","Iteration 36, loss = 0.10638537\n","Iteration 37, loss = 0.10292560\n","Iteration 38, loss = 0.10027654\n","Iteration 39, loss = 0.09794686\n","Iteration 40, loss = 0.09291796\n","Iteration 41, loss = 0.09066368\n","Iteration 42, loss = 0.08688140\n","Iteration 43, loss = 0.08397948\n","Iteration 44, loss = 0.08062402\n","Iteration 45, loss = 0.07792907\n","Iteration 46, loss = 0.07600895\n","Iteration 47, loss = 0.07227648\n","Iteration 48, loss = 0.06916906\n","Iteration 49, loss = 0.06721152\n","Iteration 50, loss = 0.06430732\n","Iteration 51, loss = 0.06069598\n","Iteration 52, loss = 0.05887578\n","Iteration 53, loss = 0.05682075\n","Iteration 54, loss = 0.05369074\n","Iteration 55, loss = 0.05281799\n","Iteration 56, loss = 0.04964397\n","Iteration 57, loss = 0.04738866\n","Iteration 58, loss = 0.04661767\n","Iteration 59, loss = 0.04355827\n","Iteration 60, loss = 0.04168147\n","Iteration 61, loss = 0.04001925\n","Iteration 62, loss = 0.03862567\n","Iteration 63, loss = 0.03660490\n","Iteration 64, loss = 0.03438981\n","Iteration 65, loss = 0.03316054\n","Iteration 66, loss = 0.03250895\n","Iteration 67, loss = 0.03098441\n","Iteration 68, loss = 0.02941622\n","Iteration 69, loss = 0.02786428\n","Iteration 70, loss = 0.02576716\n","Iteration 71, loss = 0.02428109\n","Iteration 72, loss = 0.02254929\n","Iteration 73, loss = 0.02200897\n","Iteration 74, loss = 0.02085842\n","Iteration 75, loss = 0.01960104\n","Iteration 76, loss = 0.01859577\n","Iteration 77, loss = 0.01762810\n","Iteration 78, loss = 0.01674307\n","Iteration 79, loss = 0.01607031\n","Iteration 80, loss = 0.01557945\n","Iteration 81, loss = 0.01423991\n","Iteration 82, loss = 0.01337476\n","Iteration 83, loss = 0.01256853\n","Iteration 84, loss = 0.01144561\n","Iteration 85, loss = 0.01061434\n","Iteration 86, loss = 0.01040817\n","Iteration 87, loss = 0.00970674\n","Iteration 88, loss = 0.00953615\n","Iteration 89, loss = 0.00873488\n","Iteration 90, loss = 0.00841858\n","Iteration 91, loss = 0.00820304\n","Iteration 92, loss = 0.00759406\n","Iteration 93, loss = 0.00716743\n","Iteration 94, loss = 0.00667515\n","Iteration 95, loss = 0.00635175\n","Iteration 96, loss = 0.00611288\n","Iteration 97, loss = 0.00613935\n","Iteration 98, loss = 0.00588686\n","Iteration 99, loss = 0.00554348\n","Iteration 100, loss = 0.00513095\n","Iteration 101, loss = 0.00486751\n","Iteration 102, loss = 0.00456189\n","Iteration 103, loss = 0.00433529\n","Iteration 104, loss = 0.00413437\n","Iteration 105, loss = 0.00396217\n","Iteration 106, loss = 0.00385117\n","Iteration 107, loss = 0.00361428\n","Iteration 108, loss = 0.00340976\n","Iteration 109, loss = 0.00318879\n","Iteration 110, loss = 0.00317331\n","Iteration 111, loss = 0.00305556\n","Iteration 112, loss = 0.00289749\n","Iteration 113, loss = 0.00285171\n","Iteration 114, loss = 0.00269254\n","Iteration 115, loss = 0.00256544\n","Iteration 116, loss = 0.00237885\n","Iteration 117, loss = 0.00230925\n","Iteration 118, loss = 0.00220231\n","Iteration 119, loss = 0.00212095\n","Iteration 120, loss = 0.00207280\n","Iteration 121, loss = 0.00195201\n","Iteration 122, loss = 0.00187099\n","Iteration 123, loss = 0.00181057\n","Iteration 124, loss = 0.00173586\n","Iteration 125, loss = 0.00166786\n","Iteration 126, loss = 0.00161454\n","Iteration 127, loss = 0.00156508\n","Iteration 128, loss = 0.00148622\n","Iteration 129, loss = 0.00143620\n","Iteration 130, loss = 0.00138574\n","Iteration 131, loss = 0.00133007\n","Iteration 132, loss = 0.00131108\n","Iteration 133, loss = 0.00123725\n","Iteration 134, loss = 0.00121687\n","Iteration 135, loss = 0.00117638\n","Iteration 136, loss = 0.00111895\n","Iteration 137, loss = 0.00108975\n","Iteration 138, loss = 0.00105536\n","Iteration 139, loss = 0.00103168\n","Iteration 140, loss = 0.00098789\n","Iteration 141, loss = 0.00095935\n","Iteration 142, loss = 0.00091541\n","Iteration 143, loss = 0.00088890\n","Iteration 144, loss = 0.00086256\n","Iteration 145, loss = 0.00082962\n","Iteration 146, loss = 0.00081107\n","Iteration 147, loss = 0.00079457\n","Training loss did not improve more than tol=0.000050 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.41333756\n","Iteration 2, loss = 0.23967897\n","Iteration 3, loss = 0.22338781\n","Iteration 4, loss = 0.21698791\n","Iteration 5, loss = 0.21312540\n","Iteration 6, loss = 0.20960647\n","Iteration 7, loss = 0.20573552\n","Iteration 8, loss = 0.20245016\n","Iteration 9, loss = 0.20018598\n","Iteration 10, loss = 0.19709012\n","Iteration 11, loss = 0.19421222\n","Iteration 12, loss = 0.19140647\n","Iteration 13, loss = 0.18892089\n","Iteration 14, loss = 0.18723087\n","Iteration 15, loss = 0.18419754\n","Iteration 16, loss = 0.18226846\n","Iteration 17, loss = 0.17900058\n","Iteration 18, loss = 0.17685621\n","Iteration 19, loss = 0.17415882\n","Iteration 20, loss = 0.17107270\n","Iteration 21, loss = 0.16864672\n","Iteration 22, loss = 0.16560661\n","Iteration 23, loss = 0.16342572\n","Iteration 24, loss = 0.16020741\n","Iteration 25, loss = 0.15664446\n","Iteration 26, loss = 0.15394675\n","Iteration 27, loss = 0.15180621\n","Iteration 28, loss = 0.14739724\n","Iteration 29, loss = 0.14447838\n","Iteration 30, loss = 0.14098224\n","Iteration 31, loss = 0.13909992\n","Iteration 32, loss = 0.13466605\n","Iteration 33, loss = 0.13135182\n","Iteration 34, loss = 0.12821402\n","Iteration 35, loss = 0.12450771\n","Iteration 36, loss = 0.12131304\n","Iteration 37, loss = 0.11912860\n","Iteration 38, loss = 0.11585999\n","Iteration 39, loss = 0.11520468\n","Iteration 40, loss = 0.11181287\n","Iteration 41, loss = 0.10657516\n","Iteration 42, loss = 0.10402402\n","Iteration 43, loss = 0.09928311\n","Iteration 44, loss = 0.09622342\n","Iteration 45, loss = 0.09527534\n","Iteration 46, loss = 0.09252648\n","Iteration 47, loss = 0.08807072\n","Iteration 48, loss = 0.08478572\n","Iteration 49, loss = 0.08171322\n","Iteration 50, loss = 0.08003683\n","Iteration 51, loss = 0.07722200\n","Iteration 52, loss = 0.07353944\n","Iteration 53, loss = 0.07212722\n","Iteration 54, loss = 0.06736941\n","Iteration 55, loss = 0.06624866\n","Iteration 56, loss = 0.06504978\n","Iteration 57, loss = 0.06027121\n","Iteration 58, loss = 0.05789214\n","Iteration 59, loss = 0.05633792\n","Iteration 60, loss = 0.05244238\n","Iteration 61, loss = 0.05101372\n","Iteration 62, loss = 0.04886791\n","Iteration 63, loss = 0.04634119\n","Iteration 64, loss = 0.04428300\n","Iteration 65, loss = 0.04293400\n","Iteration 66, loss = 0.04180464\n","Iteration 67, loss = 0.03985628\n","Iteration 68, loss = 0.03759571\n","Iteration 69, loss = 0.03628706\n","Iteration 70, loss = 0.03499258\n","Iteration 71, loss = 0.03176976\n","Iteration 72, loss = 0.03039378\n","Iteration 73, loss = 0.02910147\n","Iteration 74, loss = 0.02846719\n","Iteration 75, loss = 0.02715510\n","Iteration 76, loss = 0.02497005\n","Iteration 77, loss = 0.02425465\n","Iteration 78, loss = 0.02237572\n","Iteration 79, loss = 0.02148657\n","Iteration 80, loss = 0.02067432\n","Iteration 81, loss = 0.01887429\n","Iteration 82, loss = 0.01915777\n","Iteration 83, loss = 0.01777969\n","Iteration 84, loss = 0.01629794\n","Iteration 85, loss = 0.01592241\n","Iteration 86, loss = 0.01461876\n","Iteration 87, loss = 0.01388054\n","Iteration 88, loss = 0.01298943\n","Iteration 89, loss = 0.01190901\n","Iteration 90, loss = 0.01211180\n","Iteration 91, loss = 0.01204365\n","Iteration 92, loss = 0.01087888\n","Iteration 93, loss = 0.01015961\n","Iteration 94, loss = 0.00977524\n","Iteration 95, loss = 0.00931833\n","Iteration 96, loss = 0.00866023\n","Iteration 97, loss = 0.00825340\n","Iteration 98, loss = 0.00785518\n","Iteration 99, loss = 0.00748327\n","Iteration 100, loss = 0.00717732\n","Iteration 101, loss = 0.00690977\n","Iteration 102, loss = 0.00824389\n","Iteration 103, loss = 0.00641807\n","Iteration 104, loss = 0.00598500\n","Iteration 105, loss = 0.00553761\n","Iteration 106, loss = 0.00525342\n","Iteration 107, loss = 0.00508782\n","Iteration 108, loss = 0.00478763\n","Iteration 109, loss = 0.00453892\n","Iteration 110, loss = 0.00452210\n","Iteration 111, loss = 0.00428230\n","Iteration 112, loss = 0.00476749\n","Iteration 113, loss = 0.00372028\n","Iteration 114, loss = 0.00352476\n","Iteration 115, loss = 0.00346857\n","Iteration 116, loss = 0.00328705\n","Iteration 117, loss = 0.00310476\n","Iteration 118, loss = 0.00302773\n","Iteration 119, loss = 0.00287759\n","Iteration 120, loss = 0.00286151\n","Iteration 121, loss = 0.00263513\n","Iteration 122, loss = 0.00248640\n","Iteration 123, loss = 0.00246345\n","Iteration 124, loss = 0.00231884\n","Iteration 125, loss = 0.00223681\n","Iteration 126, loss = 0.00213932\n","Iteration 127, loss = 0.00206453\n","Iteration 128, loss = 0.00197441\n","Iteration 129, loss = 0.00189596\n","Iteration 130, loss = 0.00188675\n","Iteration 131, loss = 0.00175782\n","Iteration 132, loss = 0.00168201\n","Iteration 133, loss = 0.00163945\n","Iteration 134, loss = 0.00156233\n","Iteration 135, loss = 0.00154229\n","Iteration 136, loss = 0.00146311\n","Iteration 137, loss = 0.00142236\n","Iteration 138, loss = 0.00138494\n","Iteration 139, loss = 0.00133913\n","Iteration 140, loss = 0.00131349\n","Iteration 141, loss = 0.00123386\n","Iteration 142, loss = 0.00118598\n","Iteration 143, loss = 0.00115783\n","Iteration 144, loss = 0.00113724\n","Iteration 145, loss = 0.00108742\n","Iteration 146, loss = 0.00103188\n","Iteration 147, loss = 0.00099881\n","Iteration 148, loss = 0.00098653\n","Iteration 149, loss = 0.00096589\n","Iteration 150, loss = 0.00091353\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.38025924\n","Iteration 2, loss = 0.22559302\n","Iteration 3, loss = 0.21345371\n","Iteration 4, loss = 0.20904381\n","Iteration 5, loss = 0.20534747\n","Iteration 6, loss = 0.20260306\n","Iteration 7, loss = 0.20003912\n","Iteration 8, loss = 0.19683726\n","Iteration 9, loss = 0.19560572\n","Iteration 10, loss = 0.19284212\n","Iteration 11, loss = 0.19003309\n","Iteration 12, loss = 0.18763187\n","Iteration 13, loss = 0.18594002\n","Iteration 14, loss = 0.18304410\n","Iteration 15, loss = 0.18097144\n","Iteration 16, loss = 0.17772392\n","Iteration 17, loss = 0.17590755\n","Iteration 18, loss = 0.17423108\n","Iteration 19, loss = 0.17152160\n","Iteration 20, loss = 0.16877523\n","Iteration 21, loss = 0.16628345\n","Iteration 22, loss = 0.16298762\n","Iteration 23, loss = 0.16108705\n","Iteration 24, loss = 0.15733538\n","Iteration 25, loss = 0.15596141\n","Iteration 26, loss = 0.15210125\n","Iteration 27, loss = 0.15000594\n","Iteration 28, loss = 0.14684754\n","Iteration 29, loss = 0.14278042\n","Iteration 30, loss = 0.14003818\n","Iteration 31, loss = 0.13797949\n","Iteration 32, loss = 0.13343123\n","Iteration 33, loss = 0.12991561\n","Iteration 34, loss = 0.12795308\n","Iteration 35, loss = 0.12522171\n","Iteration 36, loss = 0.12005921\n","Iteration 37, loss = 0.11770413\n","Iteration 38, loss = 0.11377990\n","Iteration 39, loss = 0.11118091\n","Iteration 40, loss = 0.10816233\n","Iteration 41, loss = 0.10354477\n","Iteration 42, loss = 0.10046895\n","Iteration 43, loss = 0.09864007\n","Iteration 44, loss = 0.09313939\n","Iteration 45, loss = 0.09136521\n","Iteration 46, loss = 0.08815596\n","Iteration 47, loss = 0.08429498\n","Iteration 48, loss = 0.07995246\n","Iteration 49, loss = 0.07786578\n","Iteration 50, loss = 0.07543220\n","Iteration 51, loss = 0.07195136\n","Iteration 52, loss = 0.06969327\n","Iteration 53, loss = 0.06734263\n","Iteration 54, loss = 0.06592666\n","Iteration 55, loss = 0.06238370\n","Iteration 56, loss = 0.06087389\n","Iteration 57, loss = 0.05703357\n","Iteration 58, loss = 0.05446039\n","Iteration 59, loss = 0.05210562\n","Iteration 60, loss = 0.04858300\n","Iteration 61, loss = 0.04800121\n","Iteration 62, loss = 0.04515576\n","Iteration 63, loss = 0.04360488\n","Iteration 64, loss = 0.04251303\n","Iteration 65, loss = 0.03973718\n","Iteration 66, loss = 0.03704378\n","Iteration 67, loss = 0.03545409\n","Iteration 68, loss = 0.03414778\n","Iteration 69, loss = 0.03359365\n","Iteration 70, loss = 0.03292774\n","Iteration 71, loss = 0.02865741\n","Iteration 72, loss = 0.02803612\n","Iteration 73, loss = 0.02671838\n","Iteration 74, loss = 0.02590773\n","Iteration 75, loss = 0.02483945\n","Iteration 76, loss = 0.02293358\n","Iteration 77, loss = 0.02259676\n","Iteration 78, loss = 0.02098106\n","Iteration 79, loss = 0.01939413\n","Iteration 80, loss = 0.01894928\n","Iteration 81, loss = 0.01783069\n","Iteration 82, loss = 0.01715649\n","Iteration 83, loss = 0.01608749\n","Iteration 84, loss = 0.01498783\n","Iteration 85, loss = 0.01463004\n","Iteration 86, loss = 0.01361129\n","Iteration 87, loss = 0.01281286\n","Iteration 88, loss = 0.01227509\n","Iteration 89, loss = 0.01190998\n","Iteration 90, loss = 0.01102983\n","Iteration 91, loss = 0.01097686\n","Iteration 92, loss = 0.01008014\n","Iteration 93, loss = 0.00933100\n","Iteration 94, loss = 0.00894272\n","Iteration 95, loss = 0.00895695\n","Iteration 96, loss = 0.00819962\n","Iteration 97, loss = 0.00775050\n","Iteration 98, loss = 0.00728914\n","Iteration 99, loss = 0.00689632\n","Iteration 100, loss = 0.00683041\n","Iteration 101, loss = 0.00631046\n","Iteration 102, loss = 0.00593159\n","Iteration 103, loss = 0.00577139\n","Iteration 104, loss = 0.00548206\n","Iteration 105, loss = 0.00518120\n","Iteration 106, loss = 0.00501867\n","Iteration 107, loss = 0.00485536\n","Iteration 108, loss = 0.00463460\n","Iteration 109, loss = 0.00445137\n","Iteration 110, loss = 0.00426045\n","Iteration 111, loss = 0.00399394\n","Iteration 112, loss = 0.00385963\n","Iteration 113, loss = 0.00364376\n","Iteration 114, loss = 0.00353714\n","Iteration 115, loss = 0.00335904\n","Iteration 116, loss = 0.00317905\n","Iteration 117, loss = 0.00308587\n","Iteration 118, loss = 0.00296215\n","Iteration 119, loss = 0.00279240\n","Iteration 120, loss = 0.00284083\n","Iteration 121, loss = 0.00269582\n","Iteration 122, loss = 0.00246025\n","Iteration 123, loss = 0.00235734\n","Iteration 124, loss = 0.00223708\n","Iteration 125, loss = 0.00223947\n","Iteration 126, loss = 0.00211118\n","Iteration 127, loss = 0.00203216\n","Iteration 128, loss = 0.00190881\n","Iteration 129, loss = 0.00186307\n","Iteration 130, loss = 0.00179577\n","Iteration 131, loss = 0.00169781\n","Iteration 132, loss = 0.00165290\n","Iteration 133, loss = 0.00158968\n","Iteration 134, loss = 0.00153350\n","Iteration 135, loss = 0.00146399\n","Iteration 136, loss = 0.00142643\n","Iteration 137, loss = 0.00138542\n","Iteration 138, loss = 0.00134649\n","Iteration 139, loss = 0.00130522\n","Iteration 140, loss = 0.00124026\n","Iteration 141, loss = 0.00121589\n","Iteration 142, loss = 0.00119504\n","Iteration 143, loss = 0.00114156\n","Iteration 144, loss = 0.00109133\n","Iteration 145, loss = 0.00107020\n","Iteration 146, loss = 0.00100938\n","Iteration 147, loss = 0.00099485\n","Iteration 148, loss = 0.00096032\n","Iteration 149, loss = 0.00094933\n","Iteration 150, loss = 0.00089626\n","Iteration 1, loss = 0.67657688\n","Iteration 2, loss = 0.45554604\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.37555178\n","Iteration 4, loss = 0.33808014\n","Iteration 5, loss = 0.31608441\n","Iteration 6, loss = 0.30129792\n","Iteration 7, loss = 0.29063293\n","Iteration 8, loss = 0.28262999\n","Iteration 9, loss = 0.27622965\n","Iteration 10, loss = 0.27104085\n","Iteration 11, loss = 0.26663764\n","Iteration 12, loss = 0.26296002\n","Iteration 13, loss = 0.25979191\n","Iteration 14, loss = 0.25697618\n","Iteration 15, loss = 0.25451619\n","Iteration 16, loss = 0.25233981\n","Iteration 17, loss = 0.25037453\n","Iteration 18, loss = 0.24857919\n","Iteration 19, loss = 0.24694336\n","Iteration 20, loss = 0.24545845\n","Iteration 21, loss = 0.24412056\n","Iteration 22, loss = 0.24286751\n","Iteration 23, loss = 0.24171145\n","Iteration 24, loss = 0.24058038\n","Iteration 25, loss = 0.23960928\n","Iteration 26, loss = 0.23861681\n","Iteration 27, loss = 0.23771232\n","Iteration 28, loss = 0.23689693\n","Iteration 29, loss = 0.23605415\n","Iteration 30, loss = 0.23533566\n","Iteration 31, loss = 0.23460123\n","Iteration 32, loss = 0.23390561\n","Iteration 33, loss = 0.23322191\n","Iteration 34, loss = 0.23256584\n","Iteration 35, loss = 0.23195150\n","Iteration 36, loss = 0.23131455\n","Iteration 37, loss = 0.23077252\n","Iteration 38, loss = 0.23017038\n","Iteration 39, loss = 0.22961895\n","Iteration 40, loss = 0.22910421\n","Iteration 41, loss = 0.22859384\n","Iteration 42, loss = 0.22813242\n","Iteration 43, loss = 0.22764338\n","Iteration 44, loss = 0.22718655\n","Iteration 45, loss = 0.22675605\n","Iteration 46, loss = 0.22632030\n","Iteration 47, loss = 0.22587519\n","Iteration 48, loss = 0.22546378\n","Iteration 49, loss = 0.22500474\n","Iteration 50, loss = 0.22466625\n","Iteration 51, loss = 0.22422929\n","Iteration 52, loss = 0.22386699\n","Iteration 53, loss = 0.22347855\n","Iteration 54, loss = 0.22312150\n","Iteration 55, loss = 0.22276512\n","Iteration 56, loss = 0.22243025\n","Iteration 57, loss = 0.22212615\n","Iteration 58, loss = 0.22180170\n","Iteration 59, loss = 0.22145421\n","Iteration 60, loss = 0.22114007\n","Iteration 61, loss = 0.22078098\n","Iteration 62, loss = 0.22051412\n","Iteration 63, loss = 0.22015430\n","Iteration 64, loss = 0.21987058\n","Iteration 65, loss = 0.21954955\n","Iteration 66, loss = 0.21924257\n","Iteration 67, loss = 0.21896032\n","Iteration 68, loss = 0.21873016\n","Iteration 69, loss = 0.21839776\n","Iteration 70, loss = 0.21814226\n","Iteration 71, loss = 0.21785531\n","Iteration 72, loss = 0.21762724\n","Iteration 73, loss = 0.21733817\n","Iteration 74, loss = 0.21708087\n","Iteration 75, loss = 0.21684481\n","Iteration 76, loss = 0.21659738\n","Iteration 77, loss = 0.21632024\n","Iteration 78, loss = 0.21604358\n","Iteration 79, loss = 0.21584332\n","Iteration 80, loss = 0.21558618\n","Iteration 81, loss = 0.21540347\n","Iteration 82, loss = 0.21513621\n","Iteration 83, loss = 0.21486742\n","Iteration 84, loss = 0.21464743\n","Iteration 85, loss = 0.21443409\n","Iteration 86, loss = 0.21420426\n","Iteration 87, loss = 0.21398344\n","Iteration 88, loss = 0.21374170\n","Iteration 89, loss = 0.21354872\n","Iteration 90, loss = 0.21334110\n","Iteration 91, loss = 0.21308400\n","Iteration 92, loss = 0.21286776\n","Iteration 93, loss = 0.21266701\n","Iteration 94, loss = 0.21248352\n","Iteration 95, loss = 0.21224851\n","Iteration 96, loss = 0.21202207\n","Iteration 97, loss = 0.21182140\n","Iteration 98, loss = 0.21162819\n","Iteration 99, loss = 0.21142323\n","Iteration 100, loss = 0.21121562\n","Iteration 101, loss = 0.21102300\n","Iteration 102, loss = 0.21081696\n","Iteration 103, loss = 0.21066362\n","Iteration 104, loss = 0.21042844\n","Iteration 105, loss = 0.21023829\n","Iteration 106, loss = 0.21003519\n","Iteration 107, loss = 0.20984923\n","Iteration 108, loss = 0.20963436\n","Iteration 109, loss = 0.20948073\n","Iteration 110, loss = 0.20927382\n","Iteration 111, loss = 0.20905580\n","Iteration 112, loss = 0.20890893\n","Iteration 113, loss = 0.20871465\n","Iteration 114, loss = 0.20857770\n","Iteration 115, loss = 0.20836443\n","Iteration 116, loss = 0.20821419\n","Iteration 117, loss = 0.20796644\n","Iteration 118, loss = 0.20779316\n","Iteration 119, loss = 0.20761886\n","Iteration 120, loss = 0.20741203\n","Iteration 121, loss = 0.20725171\n","Iteration 122, loss = 0.20710377\n","Iteration 123, loss = 0.20687651\n","Iteration 124, loss = 0.20676410\n","Iteration 125, loss = 0.20653980\n","Iteration 126, loss = 0.20640795\n","Iteration 127, loss = 0.20620293\n","Iteration 128, loss = 0.20603297\n","Iteration 129, loss = 0.20586549\n","Iteration 130, loss = 0.20571678\n","Iteration 131, loss = 0.20552286\n","Iteration 132, loss = 0.20536592\n","Iteration 133, loss = 0.20519753\n","Iteration 134, loss = 0.20501752\n","Iteration 135, loss = 0.20486001\n","Iteration 136, loss = 0.20469898\n","Iteration 137, loss = 0.20453530\n","Iteration 138, loss = 0.20436465\n","Iteration 139, loss = 0.20422595\n","Iteration 140, loss = 0.20407686\n","Iteration 141, loss = 0.20387072\n","Iteration 142, loss = 0.20370753\n","Iteration 143, loss = 0.20359565\n","Iteration 144, loss = 0.20341576\n","Iteration 145, loss = 0.20324257\n","Iteration 146, loss = 0.20309283\n","Iteration 147, loss = 0.20289525\n","Iteration 148, loss = 0.20273527\n","Iteration 149, loss = 0.20262431\n","Iteration 150, loss = 0.20244409\n","Iteration 1, loss = 0.92532435\n","Iteration 2, loss = 0.50077069\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.37674338\n","Iteration 4, loss = 0.32710719\n","Iteration 5, loss = 0.30030084\n","Iteration 6, loss = 0.28365547\n","Iteration 7, loss = 0.27234737\n","Iteration 8, loss = 0.26425081\n","Iteration 9, loss = 0.25827529\n","Iteration 10, loss = 0.25358251\n","Iteration 11, loss = 0.24978842\n","Iteration 12, loss = 0.24660046\n","Iteration 13, loss = 0.24397462\n","Iteration 14, loss = 0.24161768\n","Iteration 15, loss = 0.23956558\n","Iteration 16, loss = 0.23778415\n","Iteration 17, loss = 0.23616659\n","Iteration 18, loss = 0.23475789\n","Iteration 19, loss = 0.23346819\n","Iteration 20, loss = 0.23236622\n","Iteration 21, loss = 0.23127469\n","Iteration 22, loss = 0.23032126\n","Iteration 23, loss = 0.22939015\n","Iteration 24, loss = 0.22850355\n","Iteration 25, loss = 0.22778149\n","Iteration 26, loss = 0.22705579\n","Iteration 27, loss = 0.22630168\n","Iteration 28, loss = 0.22568610\n","Iteration 29, loss = 0.22505307\n","Iteration 30, loss = 0.22449075\n","Iteration 31, loss = 0.22394118\n","Iteration 32, loss = 0.22340044\n","Iteration 33, loss = 0.22289810\n","Iteration 34, loss = 0.22239812\n","Iteration 35, loss = 0.22193980\n","Iteration 36, loss = 0.22151218\n","Iteration 37, loss = 0.22109457\n","Iteration 38, loss = 0.22068725\n","Iteration 39, loss = 0.22026628\n","Iteration 40, loss = 0.21992677\n","Iteration 41, loss = 0.21950696\n","Iteration 42, loss = 0.21921444\n","Iteration 43, loss = 0.21881046\n","Iteration 44, loss = 0.21846820\n","Iteration 45, loss = 0.21812842\n","Iteration 46, loss = 0.21775790\n","Iteration 47, loss = 0.21745932\n","Iteration 48, loss = 0.21713401\n","Iteration 49, loss = 0.21685964\n","Iteration 50, loss = 0.21655810\n","Iteration 51, loss = 0.21624415\n","Iteration 52, loss = 0.21595126\n","Iteration 53, loss = 0.21567911\n","Iteration 54, loss = 0.21541511\n","Iteration 55, loss = 0.21519148\n","Iteration 56, loss = 0.21488571\n","Iteration 57, loss = 0.21462700\n","Iteration 58, loss = 0.21436347\n","Iteration 59, loss = 0.21410136\n","Iteration 60, loss = 0.21385812\n","Iteration 61, loss = 0.21359175\n","Iteration 62, loss = 0.21335095\n","Iteration 63, loss = 0.21309035\n","Iteration 64, loss = 0.21285351\n","Iteration 65, loss = 0.21262029\n","Iteration 66, loss = 0.21238054\n","Iteration 67, loss = 0.21214735\n","Iteration 68, loss = 0.21190670\n","Iteration 69, loss = 0.21169863\n","Iteration 70, loss = 0.21144438\n","Iteration 71, loss = 0.21127310\n","Iteration 72, loss = 0.21104618\n","Iteration 73, loss = 0.21085893\n","Iteration 74, loss = 0.21055770\n","Iteration 75, loss = 0.21037225\n","Iteration 76, loss = 0.21013747\n","Iteration 77, loss = 0.20995287\n","Iteration 78, loss = 0.20976546\n","Iteration 79, loss = 0.20953246\n","Iteration 80, loss = 0.20933698\n","Iteration 81, loss = 0.20912652\n","Iteration 82, loss = 0.20897362\n","Iteration 83, loss = 0.20873671\n","Iteration 84, loss = 0.20852842\n","Iteration 85, loss = 0.20830670\n","Iteration 86, loss = 0.20810496\n","Iteration 87, loss = 0.20796173\n","Iteration 88, loss = 0.20774191\n","Iteration 89, loss = 0.20756202\n","Iteration 90, loss = 0.20735922\n","Iteration 91, loss = 0.20717286\n","Iteration 92, loss = 0.20701254\n","Iteration 93, loss = 0.20678146\n","Iteration 94, loss = 0.20660250\n","Iteration 95, loss = 0.20641575\n","Iteration 96, loss = 0.20626388\n","Iteration 97, loss = 0.20606444\n","Iteration 98, loss = 0.20584271\n","Iteration 99, loss = 0.20566479\n","Iteration 100, loss = 0.20546337\n","Iteration 101, loss = 0.20530912\n","Iteration 102, loss = 0.20511227\n","Iteration 103, loss = 0.20491648\n","Iteration 104, loss = 0.20474055\n","Iteration 105, loss = 0.20458751\n","Iteration 106, loss = 0.20440798\n","Iteration 107, loss = 0.20420038\n","Iteration 108, loss = 0.20404131\n","Iteration 109, loss = 0.20386485\n","Iteration 110, loss = 0.20367689\n","Iteration 111, loss = 0.20348552\n","Iteration 112, loss = 0.20329286\n","Iteration 113, loss = 0.20315285\n","Iteration 114, loss = 0.20302956\n","Iteration 115, loss = 0.20281616\n","Iteration 116, loss = 0.20265250\n","Iteration 117, loss = 0.20251386\n","Iteration 118, loss = 0.20227774\n","Iteration 119, loss = 0.20214389\n","Iteration 120, loss = 0.20197790\n","Iteration 121, loss = 0.20177667\n","Iteration 122, loss = 0.20162470\n","Iteration 123, loss = 0.20149160\n","Iteration 124, loss = 0.20124087\n","Iteration 125, loss = 0.20109214\n","Iteration 126, loss = 0.20094736\n","Iteration 127, loss = 0.20077818\n","Iteration 128, loss = 0.20060203\n","Iteration 129, loss = 0.20045092\n","Iteration 130, loss = 0.20030315\n","Iteration 131, loss = 0.20012778\n","Iteration 132, loss = 0.19997547\n","Iteration 133, loss = 0.19986157\n","Iteration 134, loss = 0.19964258\n","Iteration 135, loss = 0.19946284\n","Iteration 136, loss = 0.19929432\n","Iteration 137, loss = 0.19916950\n","Iteration 138, loss = 0.19902179\n","Iteration 139, loss = 0.19881615\n","Iteration 140, loss = 0.19870238\n","Iteration 141, loss = 0.19852586\n","Iteration 142, loss = 0.19835456\n","Iteration 143, loss = 0.19822329\n","Iteration 144, loss = 0.19811120\n","Iteration 145, loss = 0.19786286\n","Iteration 146, loss = 0.19765499\n","Iteration 147, loss = 0.19756724\n","Iteration 148, loss = 0.19737694\n","Iteration 149, loss = 0.19724304\n","Iteration 150, loss = 0.19707791\n","Iteration 1, loss = 0.74930243\n","Iteration 2, loss = 0.52314109"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["\n","Iteration 3, loss = 0.42038076\n","Iteration 4, loss = 0.36368431\n","Iteration 5, loss = 0.32779803\n","Iteration 6, loss = 0.30396015\n","Iteration 7, loss = 0.28728824\n","Iteration 8, loss = 0.27523397\n","Iteration 9, loss = 0.26622155\n","Iteration 10, loss = 0.25943291\n","Iteration 11, loss = 0.25399115\n","Iteration 12, loss = 0.24968487\n","Iteration 13, loss = 0.24608084\n","Iteration 14, loss = 0.24301529\n","Iteration 15, loss = 0.24042563\n","Iteration 16, loss = 0.23810725\n","Iteration 17, loss = 0.23610312\n","Iteration 18, loss = 0.23432329\n","Iteration 19, loss = 0.23275256\n","Iteration 20, loss = 0.23133912\n","Iteration 21, loss = 0.23002886\n","Iteration 22, loss = 0.22887903\n","Iteration 23, loss = 0.22780632\n","Iteration 24, loss = 0.22682238\n","Iteration 25, loss = 0.22590505\n","Iteration 26, loss = 0.22506437\n","Iteration 27, loss = 0.22433274\n","Iteration 28, loss = 0.22356041\n","Iteration 29, loss = 0.22287151\n","Iteration 30, loss = 0.22225152\n","Iteration 31, loss = 0.22158882\n","Iteration 32, loss = 0.22101518\n","Iteration 33, loss = 0.22052027\n","Iteration 34, loss = 0.21998997\n","Iteration 35, loss = 0.21950549\n","Iteration 36, loss = 0.21900871\n","Iteration 37, loss = 0.21857062\n","Iteration 38, loss = 0.21815276\n","Iteration 39, loss = 0.21770061\n","Iteration 40, loss = 0.21730184\n","Iteration 41, loss = 0.21693838\n","Iteration 42, loss = 0.21653812\n","Iteration 43, loss = 0.21615925\n","Iteration 44, loss = 0.21578430\n","Iteration 45, loss = 0.21544604\n","Iteration 46, loss = 0.21508202\n","Iteration 47, loss = 0.21477930\n","Iteration 48, loss = 0.21450873\n","Iteration 49, loss = 0.21411067\n","Iteration 50, loss = 0.21377526\n","Iteration 51, loss = 0.21348512\n","Iteration 52, loss = 0.21319694\n","Iteration 53, loss = 0.21286959\n","Iteration 54, loss = 0.21260153\n","Iteration 55, loss = 0.21231991\n","Iteration 56, loss = 0.21204286\n","Iteration 57, loss = 0.21173531\n","Iteration 58, loss = 0.21146031\n","Iteration 59, loss = 0.21118322\n","Iteration 60, loss = 0.21089904\n","Iteration 61, loss = 0.21066222\n","Iteration 62, loss = 0.21038538\n","Iteration 63, loss = 0.21016263\n","Iteration 64, loss = 0.20988051\n","Iteration 65, loss = 0.20968774\n","Iteration 66, loss = 0.20941844\n","Iteration 67, loss = 0.20914284\n","Iteration 68, loss = 0.20890856\n","Iteration 69, loss = 0.20871056\n","Iteration 70, loss = 0.20843795\n","Iteration 71, loss = 0.20824612\n","Iteration 72, loss = 0.20796210\n","Iteration 73, loss = 0.20774588\n","Iteration 74, loss = 0.20756082\n","Iteration 75, loss = 0.20730392\n","Iteration 76, loss = 0.20713983\n","Iteration 77, loss = 0.20689810\n","Iteration 78, loss = 0.20667694\n","Iteration 79, loss = 0.20646536\n","Iteration 80, loss = 0.20627590\n","Iteration 81, loss = 0.20603377\n","Iteration 82, loss = 0.20584381\n","Iteration 83, loss = 0.20564673\n","Iteration 84, loss = 0.20541053\n","Iteration 85, loss = 0.20524290\n","Iteration 86, loss = 0.20497844\n","Iteration 87, loss = 0.20480334\n","Iteration 88, loss = 0.20460499\n","Iteration 89, loss = 0.20438086\n","Iteration 90, loss = 0.20417908\n","Iteration 91, loss = 0.20399785\n","Iteration 92, loss = 0.20380648\n","Iteration 93, loss = 0.20363414\n","Iteration 94, loss = 0.20345390\n","Iteration 95, loss = 0.20323865\n","Iteration 96, loss = 0.20305064\n","Iteration 97, loss = 0.20286047\n","Iteration 98, loss = 0.20266345\n","Iteration 99, loss = 0.20248512\n","Iteration 100, loss = 0.20228201\n","Iteration 101, loss = 0.20209524\n","Iteration 102, loss = 0.20190665\n","Iteration 103, loss = 0.20175930\n","Iteration 104, loss = 0.20160051\n","Iteration 105, loss = 0.20144889\n","Iteration 106, loss = 0.20120459\n","Iteration 107, loss = 0.20106976\n","Iteration 108, loss = 0.20088901\n","Iteration 109, loss = 0.20073276\n","Iteration 110, loss = 0.20052344\n","Iteration 111, loss = 0.20037815\n","Iteration 112, loss = 0.20019193\n","Iteration 113, loss = 0.19999579\n","Iteration 114, loss = 0.19982488\n","Iteration 115, loss = 0.19962803\n","Iteration 116, loss = 0.19951181\n","Iteration 117, loss = 0.19931225\n","Iteration 118, loss = 0.19919415\n","Iteration 119, loss = 0.19901863\n","Iteration 120, loss = 0.19880627\n","Iteration 121, loss = 0.19864911\n","Iteration 122, loss = 0.19852546\n","Iteration 123, loss = 0.19829398\n","Iteration 124, loss = 0.19818202\n","Iteration 125, loss = 0.19792662\n","Iteration 126, loss = 0.19784236\n","Iteration 127, loss = 0.19764124\n","Iteration 128, loss = 0.19748831\n","Iteration 129, loss = 0.19731700\n","Iteration 130, loss = 0.19720720\n","Iteration 131, loss = 0.19705791\n","Iteration 132, loss = 0.19683598\n","Iteration 133, loss = 0.19668561\n","Iteration 134, loss = 0.19649270\n","Iteration 135, loss = 0.19636525\n","Iteration 136, loss = 0.19620235\n","Iteration 137, loss = 0.19608084\n","Iteration 138, loss = 0.19587415\n","Iteration 139, loss = 0.19574559\n","Iteration 140, loss = 0.19563753\n","Iteration 141, loss = 0.19548099\n","Iteration 142, loss = 0.19531475\n","Iteration 143, loss = 0.19512666\n","Iteration 144, loss = 0.19493844\n","Iteration 145, loss = 0.19481357\n","Iteration 146, loss = 0.19462184\n","Iteration 147, loss = 0.19451289\n","Iteration 148, loss = 0.19438888\n","Iteration 149, loss = 0.19426635\n","Iteration 150, loss = 0.19409767\n","Iteration 1, loss = 0.70307944\n","Iteration 2, loss = 0.58499221\n","Iteration 3, loss = 0.50828171\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.45469377\n","Iteration 5, loss = 0.41441179\n","Iteration 6, loss = 0.38285232\n","Iteration 7, loss = 0.35817199\n","Iteration 8, loss = 0.33853983\n","Iteration 9, loss = 0.32276598\n","Iteration 10, loss = 0.31002918\n","Iteration 11, loss = 0.29970958\n","Iteration 12, loss = 0.29117932\n","Iteration 13, loss = 0.28404709\n","Iteration 14, loss = 0.27799779\n","Iteration 15, loss = 0.27285968\n","Iteration 16, loss = 0.26842523\n","Iteration 17, loss = 0.26457611\n","Iteration 18, loss = 0.26121231\n","Iteration 19, loss = 0.25822768\n","Iteration 20, loss = 0.25558874\n","Iteration 21, loss = 0.25324229\n","Iteration 22, loss = 0.25110047\n","Iteration 23, loss = 0.24910584\n","Iteration 24, loss = 0.24734010\n","Iteration 25, loss = 0.24570942\n","Iteration 26, loss = 0.24420607\n","Iteration 27, loss = 0.24283271\n","Iteration 28, loss = 0.24155127\n","Iteration 29, loss = 0.24034858\n","Iteration 30, loss = 0.23924857\n","Iteration 31, loss = 0.23822110\n","Iteration 32, loss = 0.23722672\n","Iteration 33, loss = 0.23633271\n","Iteration 34, loss = 0.23546979\n","Iteration 35, loss = 0.23468452\n","Iteration 36, loss = 0.23390358\n","Iteration 37, loss = 0.23315332\n","Iteration 38, loss = 0.23244819\n","Iteration 39, loss = 0.23176957\n","Iteration 40, loss = 0.23119209\n","Iteration 41, loss = 0.23053482\n","Iteration 42, loss = 0.22995508\n","Iteration 43, loss = 0.22937873\n","Iteration 44, loss = 0.22887676\n","Iteration 45, loss = 0.22829198\n","Iteration 46, loss = 0.22779934\n","Iteration 47, loss = 0.22728221\n","Iteration 48, loss = 0.22682234\n","Iteration 49, loss = 0.22635005\n","Iteration 50, loss = 0.22588871\n","Iteration 51, loss = 0.22542835\n","Iteration 52, loss = 0.22496947\n","Iteration 53, loss = 0.22453611\n","Iteration 54, loss = 0.22414293\n","Iteration 55, loss = 0.22374694\n","Iteration 56, loss = 0.22335575\n","Iteration 57, loss = 0.22300632\n","Iteration 58, loss = 0.22258994\n","Iteration 59, loss = 0.22224195\n","Iteration 60, loss = 0.22185336\n","Iteration 61, loss = 0.22153121\n","Iteration 62, loss = 0.22116932\n","Iteration 63, loss = 0.22080787\n","Iteration 64, loss = 0.22051524\n","Iteration 65, loss = 0.22020676\n","Iteration 66, loss = 0.21987796\n","Iteration 67, loss = 0.21952825\n","Iteration 68, loss = 0.21922190\n","Iteration 69, loss = 0.21894055\n","Iteration 70, loss = 0.21863886\n","Iteration 71, loss = 0.21833853\n","Iteration 72, loss = 0.21802689\n","Iteration 73, loss = 0.21775400\n","Iteration 74, loss = 0.21745275\n","Iteration 75, loss = 0.21717365\n","Iteration 76, loss = 0.21693003\n","Iteration 77, loss = 0.21664468\n","Iteration 78, loss = 0.21636407\n","Iteration 79, loss = 0.21614266\n","Iteration 80, loss = 0.21582425\n","Iteration 81, loss = 0.21557737\n","Iteration 82, loss = 0.21530288\n","Iteration 83, loss = 0.21505488\n","Iteration 84, loss = 0.21482140\n","Iteration 85, loss = 0.21457600\n","Iteration 86, loss = 0.21433862\n","Iteration 87, loss = 0.21412601\n","Iteration 88, loss = 0.21386810\n","Iteration 89, loss = 0.21362101\n","Iteration 90, loss = 0.21336398\n","Iteration 91, loss = 0.21313283\n","Iteration 92, loss = 0.21294409\n","Iteration 93, loss = 0.21274797\n","Iteration 94, loss = 0.21247408\n","Iteration 95, loss = 0.21230385\n","Iteration 96, loss = 0.21208000\n","Iteration 97, loss = 0.21185449\n","Iteration 98, loss = 0.21159253\n","Iteration 99, loss = 0.21141048\n","Iteration 100, loss = 0.21116841\n","Iteration 101, loss = 0.21098922\n","Iteration 102, loss = 0.21074504\n","Iteration 103, loss = 0.21055362\n","Iteration 104, loss = 0.21034498\n","Iteration 105, loss = 0.21016599\n","Iteration 106, loss = 0.20992271\n","Iteration 107, loss = 0.20973280\n","Iteration 108, loss = 0.20954105\n","Iteration 109, loss = 0.20933233\n","Iteration 110, loss = 0.20911724\n","Iteration 111, loss = 0.20894352\n","Iteration 112, loss = 0.20875996\n","Iteration 113, loss = 0.20854956\n","Iteration 114, loss = 0.20839692\n","Iteration 115, loss = 0.20820467\n","Iteration 116, loss = 0.20800964\n","Iteration 117, loss = 0.20779790\n","Iteration 118, loss = 0.20761902\n","Iteration 119, loss = 0.20745854\n","Iteration 120, loss = 0.20725798\n","Iteration 121, loss = 0.20706996\n","Iteration 122, loss = 0.20687548\n","Iteration 123, loss = 0.20674255\n","Iteration 124, loss = 0.20653905\n","Iteration 125, loss = 0.20637075\n","Iteration 126, loss = 0.20617474\n","Iteration 127, loss = 0.20601152\n","Iteration 128, loss = 0.20581790\n","Iteration 129, loss = 0.20567765\n","Iteration 130, loss = 0.20545287\n","Iteration 131, loss = 0.20530480\n","Iteration 132, loss = 0.20515774\n","Iteration 133, loss = 0.20494710\n","Iteration 134, loss = 0.20476077\n","Iteration 135, loss = 0.20461540\n","Iteration 136, loss = 0.20443079\n","Iteration 137, loss = 0.20428373\n","Iteration 138, loss = 0.20411458\n","Iteration 139, loss = 0.20391170\n","Iteration 140, loss = 0.20378686\n","Iteration 141, loss = 0.20363734\n","Iteration 142, loss = 0.20347354\n","Iteration 143, loss = 0.20328996\n","Iteration 144, loss = 0.20311087\n","Iteration 145, loss = 0.20295243\n","Iteration 146, loss = 0.20280373\n","Iteration 147, loss = 0.20264522\n","Iteration 148, loss = 0.20253878\n","Iteration 149, loss = 0.20234380\n","Iteration 150, loss = 0.20215679\n","Iteration 1, loss = 0.50229304\n","Iteration 2, loss = 0.41790990\n","Iteration 3, loss = 0.36303119\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.32730087\n","Iteration 5, loss = 0.30320266\n","Iteration 6, loss = 0.28645971\n","Iteration 7, loss = 0.27453936\n","Iteration 8, loss = 0.26562464\n","Iteration 9, loss = 0.25891288\n","Iteration 10, loss = 0.25363593\n","Iteration 11, loss = 0.24934107\n","Iteration 12, loss = 0.24580710\n","Iteration 13, loss = 0.24281959\n","Iteration 14, loss = 0.24026068\n","Iteration 15, loss = 0.23800139\n","Iteration 16, loss = 0.23599606\n","Iteration 17, loss = 0.23421502\n","Iteration 18, loss = 0.23262245\n","Iteration 19, loss = 0.23116258\n","Iteration 20, loss = 0.22984711\n","Iteration 21, loss = 0.22868288\n","Iteration 22, loss = 0.22759105\n","Iteration 23, loss = 0.22657344\n","Iteration 24, loss = 0.22563030\n","Iteration 25, loss = 0.22479061\n","Iteration 26, loss = 0.22398613\n","Iteration 27, loss = 0.22324072\n","Iteration 28, loss = 0.22254359\n","Iteration 29, loss = 0.22191106\n","Iteration 30, loss = 0.22128380\n","Iteration 31, loss = 0.22072118\n","Iteration 32, loss = 0.22019226\n","Iteration 33, loss = 0.21965131\n","Iteration 34, loss = 0.21915982\n","Iteration 35, loss = 0.21871044\n","Iteration 36, loss = 0.21825229\n","Iteration 37, loss = 0.21779667\n","Iteration 38, loss = 0.21743179\n","Iteration 39, loss = 0.21703196\n","Iteration 40, loss = 0.21666699\n","Iteration 41, loss = 0.21626572\n","Iteration 42, loss = 0.21592666\n","Iteration 43, loss = 0.21556618\n","Iteration 44, loss = 0.21525068\n","Iteration 45, loss = 0.21495818\n","Iteration 46, loss = 0.21461064\n","Iteration 47, loss = 0.21427852\n","Iteration 48, loss = 0.21395894\n","Iteration 49, loss = 0.21368984\n","Iteration 50, loss = 0.21339902\n","Iteration 51, loss = 0.21310293\n","Iteration 52, loss = 0.21287332\n","Iteration 53, loss = 0.21258189\n","Iteration 54, loss = 0.21233819\n","Iteration 55, loss = 0.21211527\n","Iteration 56, loss = 0.21181321\n","Iteration 57, loss = 0.21159025\n","Iteration 58, loss = 0.21132439\n","Iteration 59, loss = 0.21111808\n","Iteration 60, loss = 0.21086023\n","Iteration 61, loss = 0.21066992\n","Iteration 62, loss = 0.21042398\n","Iteration 63, loss = 0.21019533\n","Iteration 64, loss = 0.21002094\n","Iteration 65, loss = 0.20978933\n","Iteration 66, loss = 0.20954478\n","Iteration 67, loss = 0.20935637\n","Iteration 68, loss = 0.20913494\n","Iteration 69, loss = 0.20892521\n","Iteration 70, loss = 0.20877325\n","Iteration 71, loss = 0.20856422\n","Iteration 72, loss = 0.20835696\n","Iteration 73, loss = 0.20816591\n","Iteration 74, loss = 0.20799443\n","Iteration 75, loss = 0.20778929\n","Iteration 76, loss = 0.20757592\n","Iteration 77, loss = 0.20738815\n","Iteration 78, loss = 0.20724614\n","Iteration 79, loss = 0.20705661\n","Iteration 80, loss = 0.20687502\n","Iteration 81, loss = 0.20667691\n","Iteration 82, loss = 0.20655721\n","Iteration 83, loss = 0.20635336\n","Iteration 84, loss = 0.20621136\n","Iteration 85, loss = 0.20603036\n","Iteration 86, loss = 0.20585742\n","Iteration 87, loss = 0.20570090\n","Iteration 88, loss = 0.20552888\n","Iteration 89, loss = 0.20538083\n","Iteration 90, loss = 0.20522578\n","Iteration 91, loss = 0.20503361\n","Iteration 92, loss = 0.20491481\n","Iteration 93, loss = 0.20478163\n","Iteration 94, loss = 0.20461975\n","Iteration 95, loss = 0.20444788\n","Iteration 96, loss = 0.20426632\n","Iteration 97, loss = 0.20413940\n","Iteration 98, loss = 0.20397443\n","Iteration 99, loss = 0.20385575\n","Iteration 100, loss = 0.20368819\n","Iteration 101, loss = 0.20354577\n","Iteration 102, loss = 0.20337683\n","Iteration 103, loss = 0.20325853\n","Iteration 104, loss = 0.20307739\n","Iteration 105, loss = 0.20297098\n","Iteration 106, loss = 0.20286371\n","Iteration 107, loss = 0.20269028\n","Iteration 108, loss = 0.20255795\n","Iteration 109, loss = 0.20241072\n","Iteration 110, loss = 0.20225642\n","Iteration 111, loss = 0.20214177\n","Iteration 112, loss = 0.20198416\n","Iteration 113, loss = 0.20186428\n","Iteration 114, loss = 0.20170936\n","Iteration 115, loss = 0.20157405\n","Iteration 116, loss = 0.20147957\n","Iteration 117, loss = 0.20129003\n","Iteration 118, loss = 0.20119455\n","Iteration 119, loss = 0.20104864\n","Iteration 120, loss = 0.20093477\n","Iteration 121, loss = 0.20079976\n","Iteration 122, loss = 0.20068047\n","Iteration 123, loss = 0.20050554\n","Iteration 124, loss = 0.20035976\n","Iteration 125, loss = 0.20025651\n","Iteration 126, loss = 0.20014279\n","Iteration 127, loss = 0.20000895\n","Iteration 128, loss = 0.19986069\n","Iteration 129, loss = 0.19974446\n","Iteration 130, loss = 0.19965175\n","Iteration 131, loss = 0.19950113\n","Iteration 132, loss = 0.19936320\n","Iteration 133, loss = 0.19924660\n","Iteration 134, loss = 0.19911334\n","Iteration 135, loss = 0.19898441\n","Iteration 136, loss = 0.19888558\n","Iteration 137, loss = 0.19873428\n","Iteration 138, loss = 0.19864831\n","Iteration 139, loss = 0.19851162\n","Iteration 140, loss = 0.19841046\n","Iteration 141, loss = 0.19828179\n","Iteration 142, loss = 0.19812796\n","Iteration 143, loss = 0.19806770\n","Iteration 144, loss = 0.19791037\n","Iteration 145, loss = 0.19779731\n","Iteration 146, loss = 0.19766296\n","Iteration 147, loss = 0.19758133\n","Iteration 148, loss = 0.19746628\n","Iteration 149, loss = 0.19731677\n","Iteration 150, loss = 0.19723445\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.63943129\n","Iteration 2, loss = 0.30092120\n","Iteration 3, loss = 0.25235234\n","Iteration 4, loss = 0.23785378\n","Iteration 5, loss = 0.22989101\n","Iteration 6, loss = 0.22470835\n","Iteration 7, loss = 0.22082277\n","Iteration 8, loss = 0.21746873\n","Iteration 9, loss = 0.21450568\n","Iteration 10, loss = 0.21220505\n","Iteration 11, loss = 0.20970283\n","Iteration 12, loss = 0.20771130\n","Iteration 13, loss = 0.20527010\n","Iteration 14, loss = 0.20357099\n","Iteration 15, loss = 0.20150490\n","Iteration 16, loss = 0.19945951\n","Iteration 17, loss = 0.19759831\n","Iteration 18, loss = 0.19570273\n","Iteration 19, loss = 0.19399081\n","Iteration 20, loss = 0.19255293\n","Iteration 21, loss = 0.19076896\n","Iteration 22, loss = 0.18926023\n","Iteration 23, loss = 0.18732931\n","Iteration 24, loss = 0.18581910\n","Iteration 25, loss = 0.18411998\n","Iteration 26, loss = 0.18326088\n","Iteration 27, loss = 0.18041675\n","Iteration 28, loss = 0.17876027\n","Iteration 29, loss = 0.17828414\n","Iteration 30, loss = 0.17532619\n","Iteration 31, loss = 0.17376420\n","Iteration 32, loss = 0.17178393\n","Iteration 33, loss = 0.17081261\n","Iteration 34, loss = 0.16897475\n","Iteration 35, loss = 0.16710079\n","Iteration 36, loss = 0.16565322\n","Iteration 37, loss = 0.16298106\n","Iteration 38, loss = 0.16133512\n","Iteration 39, loss = 0.15911560\n","Iteration 40, loss = 0.15776095\n","Iteration 41, loss = 0.15629055\n","Iteration 42, loss = 0.15477474\n","Iteration 43, loss = 0.15203029\n","Iteration 44, loss = 0.15025998\n","Iteration 45, loss = 0.14899598\n","Iteration 46, loss = 0.14680020\n","Iteration 47, loss = 0.14538058\n","Iteration 48, loss = 0.14347746\n","Iteration 49, loss = 0.14109100\n","Iteration 50, loss = 0.14014616\n","Iteration 51, loss = 0.13721355\n","Iteration 52, loss = 0.13641890\n","Iteration 53, loss = 0.13390648\n","Iteration 54, loss = 0.13170230\n","Iteration 55, loss = 0.12978790\n","Iteration 56, loss = 0.12806467\n","Iteration 57, loss = 0.12737208\n","Iteration 58, loss = 0.12515449\n","Iteration 59, loss = 0.12364110\n","Iteration 60, loss = 0.12281463\n","Iteration 61, loss = 0.12053092\n","Iteration 62, loss = 0.11970985\n","Iteration 63, loss = 0.11646674\n","Iteration 64, loss = 0.11531115\n","Iteration 65, loss = 0.11381118\n","Iteration 66, loss = 0.11239830\n","Iteration 67, loss = 0.11079842\n","Iteration 68, loss = 0.11082552\n","Iteration 69, loss = 0.10717207\n","Iteration 70, loss = 0.10588275\n","Iteration 71, loss = 0.10479812\n","Iteration 72, loss = 0.10369909\n","Iteration 73, loss = 0.10229291\n","Iteration 74, loss = 0.10098566\n","Iteration 75, loss = 0.09919604\n","Iteration 76, loss = 0.09746089\n","Iteration 77, loss = 0.09621249\n","Iteration 78, loss = 0.09508237\n","Iteration 79, loss = 0.09390946\n","Iteration 80, loss = 0.09172041\n","Iteration 81, loss = 0.09140357\n","Iteration 82, loss = 0.09043530\n","Iteration 83, loss = 0.08811801\n","Iteration 84, loss = 0.08649050\n","Iteration 85, loss = 0.08555450\n","Iteration 86, loss = 0.08551305\n","Iteration 87, loss = 0.08368701\n","Iteration 88, loss = 0.08226858\n","Iteration 89, loss = 0.08090083\n","Iteration 90, loss = 0.07940116\n","Iteration 91, loss = 0.07843437\n","Iteration 92, loss = 0.07771166\n","Iteration 93, loss = 0.07711800\n","Iteration 94, loss = 0.07481937\n","Iteration 95, loss = 0.07269294\n","Iteration 96, loss = 0.07285468\n","Iteration 97, loss = 0.07035994\n","Iteration 98, loss = 0.06961861\n","Iteration 99, loss = 0.06995110\n","Iteration 100, loss = 0.06787207\n","Iteration 101, loss = 0.06750831\n","Iteration 102, loss = 0.06606382\n","Iteration 103, loss = 0.06476451\n","Iteration 104, loss = 0.06344855\n","Iteration 105, loss = 0.06370259\n","Iteration 106, loss = 0.06160905\n","Iteration 107, loss = 0.06121654\n","Iteration 108, loss = 0.06044158\n","Iteration 109, loss = 0.05963776\n","Iteration 110, loss = 0.05781447\n","Iteration 111, loss = 0.05635770\n","Iteration 112, loss = 0.05598298\n","Iteration 113, loss = 0.05472628\n","Iteration 114, loss = 0.05573781\n","Iteration 115, loss = 0.05323295\n","Iteration 116, loss = 0.05250897\n","Iteration 117, loss = 0.05211117\n","Iteration 118, loss = 0.05051416\n","Iteration 119, loss = 0.04983672\n","Iteration 120, loss = 0.04894156\n","Iteration 121, loss = 0.04811111\n","Iteration 122, loss = 0.04718650\n","Iteration 123, loss = 0.04641881\n","Iteration 124, loss = 0.04622044\n","Iteration 125, loss = 0.04426597\n","Iteration 126, loss = 0.04450445\n","Iteration 127, loss = 0.04337307\n","Iteration 128, loss = 0.04331229\n","Iteration 129, loss = 0.04230784\n","Iteration 130, loss = 0.04062101\n","Iteration 131, loss = 0.03914661\n","Iteration 132, loss = 0.03960356\n","Iteration 133, loss = 0.03901234\n","Iteration 134, loss = 0.03804139\n","Iteration 135, loss = 0.03672585\n","Iteration 136, loss = 0.03610796\n","Iteration 137, loss = 0.03630290\n","Iteration 138, loss = 0.03596408\n","Iteration 139, loss = 0.03470947\n","Iteration 140, loss = 0.03590057\n","Iteration 141, loss = 0.03498501\n","Iteration 142, loss = 0.03326619\n","Iteration 143, loss = 0.03079189\n","Iteration 144, loss = 0.03098872\n","Iteration 145, loss = 0.03161376\n","Iteration 146, loss = 0.02970086\n","Iteration 147, loss = 0.02862068\n","Iteration 148, loss = 0.02894509\n","Iteration 149, loss = 0.02800299\n","Iteration 150, loss = 0.02756042\n","Iteration 1, loss = 0.38326920\n","Iteration 2, loss = 0.26308149\n","Iteration 3, loss = 0.24046545\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.22938988\n","Iteration 5, loss = 0.22310038\n","Iteration 6, loss = 0.21821471\n","Iteration 7, loss = 0.21427163\n","Iteration 8, loss = 0.21147610\n","Iteration 9, loss = 0.20873712\n","Iteration 10, loss = 0.20579456\n","Iteration 11, loss = 0.20402026\n","Iteration 12, loss = 0.20092683\n","Iteration 13, loss = 0.19937935\n","Iteration 14, loss = 0.19706690\n","Iteration 15, loss = 0.19501758\n","Iteration 16, loss = 0.19342396\n","Iteration 17, loss = 0.19173652\n","Iteration 18, loss = 0.18921248\n","Iteration 19, loss = 0.18723402\n","Iteration 20, loss = 0.18542597\n","Iteration 21, loss = 0.18341468\n","Iteration 22, loss = 0.18177817\n","Iteration 23, loss = 0.18005634\n","Iteration 24, loss = 0.17787617\n","Iteration 25, loss = 0.17638748\n","Iteration 26, loss = 0.17459318\n","Iteration 27, loss = 0.17271033\n","Iteration 28, loss = 0.17180805\n","Iteration 29, loss = 0.16935289\n","Iteration 30, loss = 0.16768362\n","Iteration 31, loss = 0.16528237\n","Iteration 32, loss = 0.16458744\n","Iteration 33, loss = 0.16187353\n","Iteration 34, loss = 0.16050335\n","Iteration 35, loss = 0.15805440\n","Iteration 36, loss = 0.15629052\n","Iteration 37, loss = 0.15479486\n","Iteration 38, loss = 0.15354378\n","Iteration 39, loss = 0.15113443\n","Iteration 40, loss = 0.15006965\n","Iteration 41, loss = 0.14815556\n","Iteration 42, loss = 0.14718107\n","Iteration 43, loss = 0.14442729\n","Iteration 44, loss = 0.14281636\n","Iteration 45, loss = 0.14160992\n","Iteration 46, loss = 0.14080125\n","Iteration 47, loss = 0.13794649\n","Iteration 48, loss = 0.13675282\n","Iteration 49, loss = 0.13595320\n","Iteration 50, loss = 0.13252461\n","Iteration 51, loss = 0.13145191\n","Iteration 52, loss = 0.12989837\n","Iteration 53, loss = 0.12867339\n","Iteration 54, loss = 0.12709499\n","Iteration 55, loss = 0.12476536\n","Iteration 56, loss = 0.12367245\n","Iteration 57, loss = 0.12237400\n","Iteration 58, loss = 0.12165573\n","Iteration 59, loss = 0.12033653\n","Iteration 60, loss = 0.11817540\n","Iteration 61, loss = 0.11575155\n","Iteration 62, loss = 0.11522315\n","Iteration 63, loss = 0.11324494\n","Iteration 64, loss = 0.11277214\n","Iteration 65, loss = 0.11057863\n","Iteration 66, loss = 0.10936602\n","Iteration 67, loss = 0.10738830\n","Iteration 68, loss = 0.10684565\n","Iteration 69, loss = 0.10433408\n","Iteration 70, loss = 0.10274649\n","Iteration 71, loss = 0.10313129\n","Iteration 72, loss = 0.10067646\n","Iteration 73, loss = 0.09964713\n","Iteration 74, loss = 0.09721439\n","Iteration 75, loss = 0.09652673\n","Iteration 76, loss = 0.09522528\n","Iteration 77, loss = 0.09383453\n","Iteration 78, loss = 0.09413126\n","Iteration 79, loss = 0.09086248\n","Iteration 80, loss = 0.08963468\n","Iteration 81, loss = 0.08932183\n","Iteration 82, loss = 0.08766366\n","Iteration 83, loss = 0.08577514\n","Iteration 84, loss = 0.08432944\n","Iteration 85, loss = 0.08419940\n","Iteration 86, loss = 0.08251415\n","Iteration 87, loss = 0.08149363\n","Iteration 88, loss = 0.07899116\n","Iteration 89, loss = 0.07844922\n","Iteration 90, loss = 0.07818986\n","Iteration 91, loss = 0.07683556\n","Iteration 92, loss = 0.07536260\n","Iteration 93, loss = 0.07434884\n","Iteration 94, loss = 0.07288049\n","Iteration 95, loss = 0.07155138\n","Iteration 96, loss = 0.07114321\n","Iteration 97, loss = 0.07010526\n","Iteration 98, loss = 0.06906975\n","Iteration 99, loss = 0.06840102\n","Iteration 100, loss = 0.06682728\n","Iteration 101, loss = 0.06601064\n","Iteration 102, loss = 0.06490718\n","Iteration 103, loss = 0.06388342\n","Iteration 104, loss = 0.06273961\n","Iteration 105, loss = 0.06175390\n","Iteration 106, loss = 0.06173957\n","Iteration 107, loss = 0.06008799\n","Iteration 108, loss = 0.05883002\n","Iteration 109, loss = 0.05948205\n","Iteration 110, loss = 0.05787600\n","Iteration 111, loss = 0.05636562\n","Iteration 112, loss = 0.05625065\n","Iteration 113, loss = 0.05449170\n","Iteration 114, loss = 0.05345510\n","Iteration 115, loss = 0.05358331\n","Iteration 116, loss = 0.05387832\n","Iteration 117, loss = 0.05193534\n","Iteration 118, loss = 0.05248312\n","Iteration 119, loss = 0.05046072\n","Iteration 120, loss = 0.05006491\n","Iteration 121, loss = 0.04814595\n","Iteration 122, loss = 0.04703926\n","Iteration 123, loss = 0.04681648\n","Iteration 124, loss = 0.04570251\n","Iteration 125, loss = 0.04485903\n","Iteration 126, loss = 0.04442068\n","Iteration 127, loss = 0.04464939\n","Iteration 128, loss = 0.04347535\n","Iteration 129, loss = 0.04264088\n","Iteration 130, loss = 0.04220339\n","Iteration 131, loss = 0.04123486\n","Iteration 132, loss = 0.04055254\n","Iteration 133, loss = 0.03979420\n","Iteration 134, loss = 0.03967159\n","Iteration 135, loss = 0.03827126\n","Iteration 136, loss = 0.03753434\n","Iteration 137, loss = 0.03675293\n","Iteration 138, loss = 0.03689787\n","Iteration 139, loss = 0.03502759\n","Iteration 140, loss = 0.03553435\n","Iteration 141, loss = 0.03506543\n","Iteration 142, loss = 0.03433577\n","Iteration 143, loss = 0.03556273\n","Iteration 144, loss = 0.03342381\n","Iteration 145, loss = 0.03339623\n","Iteration 146, loss = 0.03231685\n","Iteration 147, loss = 0.03152189\n","Iteration 148, loss = 0.03077882\n","Iteration 149, loss = 0.02973457\n","Iteration 150, loss = 0.02968994\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.44706207\n","Iteration 2, loss = 0.26654131\n","Iteration 3, loss = 0.23679142\n","Iteration 4, loss = 0.22544286\n","Iteration 5, loss = 0.21863874\n","Iteration 6, loss = 0.21364204\n","Iteration 7, loss = 0.20976291\n","Iteration 8, loss = 0.20671703\n","Iteration 9, loss = 0.20441130\n","Iteration 10, loss = 0.20146203\n","Iteration 11, loss = 0.19903733\n","Iteration 12, loss = 0.19687385\n","Iteration 13, loss = 0.19396321\n","Iteration 14, loss = 0.19231699\n","Iteration 15, loss = 0.18986480\n","Iteration 16, loss = 0.18844821\n","Iteration 17, loss = 0.18606665\n","Iteration 18, loss = 0.18462953\n","Iteration 19, loss = 0.18304541\n","Iteration 20, loss = 0.18047789\n","Iteration 21, loss = 0.17889097\n","Iteration 22, loss = 0.17780600\n","Iteration 23, loss = 0.17572110\n","Iteration 24, loss = 0.17415408\n","Iteration 25, loss = 0.17263885\n","Iteration 26, loss = 0.17125878\n","Iteration 27, loss = 0.16910778\n","Iteration 28, loss = 0.16746957\n","Iteration 29, loss = 0.16571445\n","Iteration 30, loss = 0.16409037\n","Iteration 31, loss = 0.16229730\n","Iteration 32, loss = 0.16072584\n","Iteration 33, loss = 0.15873291\n","Iteration 34, loss = 0.15772325\n","Iteration 35, loss = 0.15623036\n","Iteration 36, loss = 0.15447282\n","Iteration 37, loss = 0.15266249\n","Iteration 38, loss = 0.15074821\n","Iteration 39, loss = 0.14947577\n","Iteration 40, loss = 0.14768784\n","Iteration 41, loss = 0.14569245\n","Iteration 42, loss = 0.14473303\n","Iteration 43, loss = 0.14284118\n","Iteration 44, loss = 0.14122473\n","Iteration 45, loss = 0.14044248\n","Iteration 46, loss = 0.13761046\n","Iteration 47, loss = 0.13661473\n","Iteration 48, loss = 0.13546424\n","Iteration 49, loss = 0.13325102\n","Iteration 50, loss = 0.13153886\n","Iteration 51, loss = 0.13064605\n","Iteration 52, loss = 0.12802643\n","Iteration 53, loss = 0.12670765\n","Iteration 54, loss = 0.12495168\n","Iteration 55, loss = 0.12413638\n","Iteration 56, loss = 0.12189380\n","Iteration 57, loss = 0.12038026\n","Iteration 58, loss = 0.11814072\n","Iteration 59, loss = 0.11722666\n","Iteration 60, loss = 0.11521945\n","Iteration 61, loss = 0.11396028\n","Iteration 62, loss = 0.11219388\n","Iteration 63, loss = 0.11163638\n","Iteration 64, loss = 0.10968008\n","Iteration 65, loss = 0.10808802\n","Iteration 66, loss = 0.10697466\n","Iteration 67, loss = 0.10572643\n","Iteration 68, loss = 0.10329832\n","Iteration 69, loss = 0.10193640\n","Iteration 70, loss = 0.10052645\n","Iteration 71, loss = 0.09861429\n","Iteration 72, loss = 0.09833974\n","Iteration 73, loss = 0.09592718\n","Iteration 74, loss = 0.09517410\n","Iteration 75, loss = 0.09510645\n","Iteration 76, loss = 0.09242310\n","Iteration 77, loss = 0.09099544\n","Iteration 78, loss = 0.08937315\n","Iteration 79, loss = 0.08855871\n","Iteration 80, loss = 0.08671773\n","Iteration 81, loss = 0.08692414\n","Iteration 82, loss = 0.08469117\n","Iteration 83, loss = 0.08308837\n","Iteration 84, loss = 0.08176726\n","Iteration 85, loss = 0.08093237\n","Iteration 86, loss = 0.07918740\n","Iteration 87, loss = 0.07890382\n","Iteration 88, loss = 0.07686895\n","Iteration 89, loss = 0.07546674\n","Iteration 90, loss = 0.07399772\n","Iteration 91, loss = 0.07232264\n","Iteration 92, loss = 0.07201489\n","Iteration 93, loss = 0.07103536\n","Iteration 94, loss = 0.06958349\n","Iteration 95, loss = 0.06847116\n","Iteration 96, loss = 0.06718346\n","Iteration 97, loss = 0.06556119\n","Iteration 98, loss = 0.06498713\n","Iteration 99, loss = 0.06336163\n","Iteration 100, loss = 0.06255204\n","Iteration 101, loss = 0.06196718\n","Iteration 102, loss = 0.06021507\n","Iteration 103, loss = 0.05914173\n","Iteration 104, loss = 0.05758026\n","Iteration 105, loss = 0.05705796\n","Iteration 106, loss = 0.05626537\n","Iteration 107, loss = 0.05438904\n","Iteration 108, loss = 0.05379049\n","Iteration 109, loss = 0.05342234\n","Iteration 110, loss = 0.05173077\n","Iteration 111, loss = 0.05085683\n","Iteration 112, loss = 0.05050130\n","Iteration 113, loss = 0.04972078\n","Iteration 114, loss = 0.04836271\n","Iteration 115, loss = 0.04723436\n","Iteration 116, loss = 0.04627339\n","Iteration 117, loss = 0.04528660\n","Iteration 118, loss = 0.04490402\n","Iteration 119, loss = 0.04377498\n","Iteration 120, loss = 0.04307696\n","Iteration 121, loss = 0.04298261\n","Iteration 122, loss = 0.04181130\n","Iteration 123, loss = 0.04095368\n","Iteration 124, loss = 0.03995684\n","Iteration 125, loss = 0.03957498\n","Iteration 126, loss = 0.03923750\n","Iteration 127, loss = 0.03832381\n","Iteration 128, loss = 0.03662746\n","Iteration 129, loss = 0.03721574\n","Iteration 130, loss = 0.03584010\n","Iteration 131, loss = 0.03488797\n","Iteration 132, loss = 0.03580644\n","Iteration 133, loss = 0.03401029\n","Iteration 134, loss = 0.03367279\n","Iteration 135, loss = 0.03233482\n","Iteration 136, loss = 0.03201843\n","Iteration 137, loss = 0.03223623\n","Iteration 138, loss = 0.03089346\n","Iteration 139, loss = 0.03049984\n","Iteration 140, loss = 0.02969940\n","Iteration 141, loss = 0.02936036\n","Iteration 142, loss = 0.02883137\n","Iteration 143, loss = 0.02771585\n","Iteration 144, loss = 0.02686811\n","Iteration 145, loss = 0.02710943\n","Iteration 146, loss = 0.02666675\n","Iteration 147, loss = 0.02544358\n","Iteration 148, loss = 0.02571045\n","Iteration 149, loss = 0.02498388\n","Iteration 150, loss = 0.02430109\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.44857653\n","Iteration 2, loss = 0.26670606\n","Iteration 3, loss = 0.23875116\n","Iteration 4, loss = 0.22727629\n","Iteration 5, loss = 0.22110485\n","Iteration 6, loss = 0.21678182\n","Iteration 7, loss = 0.21330632\n","Iteration 8, loss = 0.20977058\n","Iteration 9, loss = 0.20701866\n","Iteration 10, loss = 0.20477017\n","Iteration 11, loss = 0.20210120\n","Iteration 12, loss = 0.19999752\n","Iteration 13, loss = 0.19819839\n","Iteration 14, loss = 0.19631578\n","Iteration 15, loss = 0.19379679\n","Iteration 16, loss = 0.19218120\n","Iteration 17, loss = 0.19059270\n","Iteration 18, loss = 0.18833093\n","Iteration 19, loss = 0.18720011\n","Iteration 20, loss = 0.18557103\n","Iteration 21, loss = 0.18289952\n","Iteration 22, loss = 0.18211394\n","Iteration 23, loss = 0.17993032\n","Iteration 24, loss = 0.17821757\n","Iteration 25, loss = 0.17695304\n","Iteration 26, loss = 0.17469405\n","Iteration 27, loss = 0.17236059\n","Iteration 28, loss = 0.17187974\n","Iteration 29, loss = 0.16960389\n","Iteration 30, loss = 0.16755307\n","Iteration 31, loss = 0.16681226\n","Iteration 32, loss = 0.16376413\n","Iteration 33, loss = 0.16220613\n","Iteration 34, loss = 0.16053403\n","Iteration 35, loss = 0.15917387\n","Iteration 36, loss = 0.15731049\n","Iteration 37, loss = 0.15540944\n","Iteration 38, loss = 0.15420819\n","Iteration 39, loss = 0.15203878\n","Iteration 40, loss = 0.15073520\n","Iteration 41, loss = 0.14851884\n","Iteration 42, loss = 0.14673509\n","Iteration 43, loss = 0.14543994\n","Iteration 44, loss = 0.14399744\n","Iteration 45, loss = 0.14155344\n","Iteration 46, loss = 0.13954053\n","Iteration 47, loss = 0.13795084\n","Iteration 48, loss = 0.13644011\n","Iteration 49, loss = 0.13398014\n","Iteration 50, loss = 0.13259258\n","Iteration 51, loss = 0.13215689\n","Iteration 52, loss = 0.12972711\n","Iteration 53, loss = 0.12777770\n","Iteration 54, loss = 0.12576178\n","Iteration 55, loss = 0.12425528\n","Iteration 56, loss = 0.12271716\n","Iteration 57, loss = 0.12189107\n","Iteration 58, loss = 0.12031721\n","Iteration 59, loss = 0.11889759\n","Iteration 60, loss = 0.11734372\n","Iteration 61, loss = 0.11653009\n","Iteration 62, loss = 0.11425472\n","Iteration 63, loss = 0.11352873\n","Iteration 64, loss = 0.11121440\n","Iteration 65, loss = 0.11076781\n","Iteration 66, loss = 0.10874808\n","Iteration 67, loss = 0.10771661\n","Iteration 68, loss = 0.10557718\n","Iteration 69, loss = 0.10472761\n","Iteration 70, loss = 0.10499247\n","Iteration 71, loss = 0.10278983\n","Iteration 72, loss = 0.10198326\n","Iteration 73, loss = 0.10038168\n","Iteration 74, loss = 0.09875191\n","Iteration 75, loss = 0.09811967\n","Iteration 76, loss = 0.09664218\n","Iteration 77, loss = 0.09642834\n","Iteration 78, loss = 0.09501437\n","Iteration 79, loss = 0.09426009\n","Iteration 80, loss = 0.09231857\n","Iteration 81, loss = 0.09136294\n","Iteration 82, loss = 0.08929442\n","Iteration 83, loss = 0.08903245\n","Iteration 84, loss = 0.08746439\n","Iteration 85, loss = 0.08677917\n","Iteration 86, loss = 0.08496685\n","Iteration 87, loss = 0.08464551\n","Iteration 88, loss = 0.08405160\n","Iteration 89, loss = 0.08214952\n","Iteration 90, loss = 0.08133990\n","Iteration 91, loss = 0.07998560\n","Iteration 92, loss = 0.07872988\n","Iteration 93, loss = 0.07792310\n","Iteration 94, loss = 0.07758506\n","Iteration 95, loss = 0.07594920\n","Iteration 96, loss = 0.07628899\n","Iteration 97, loss = 0.07467290\n","Iteration 98, loss = 0.07368904\n","Iteration 99, loss = 0.07189825\n","Iteration 100, loss = 0.07211925\n","Iteration 101, loss = 0.07164436\n","Iteration 102, loss = 0.06956579\n","Iteration 103, loss = 0.06920416\n","Iteration 104, loss = 0.06785757\n","Iteration 105, loss = 0.06683677\n","Iteration 106, loss = 0.06667194\n","Iteration 107, loss = 0.06545717\n","Iteration 108, loss = 0.06399756\n","Iteration 109, loss = 0.06369121\n","Iteration 110, loss = 0.06218471\n","Iteration 111, loss = 0.06220334\n","Iteration 112, loss = 0.06041614\n","Iteration 113, loss = 0.06147495\n","Iteration 114, loss = 0.05932184\n","Iteration 115, loss = 0.05850921\n","Iteration 116, loss = 0.05761302\n","Iteration 117, loss = 0.05714724\n","Iteration 118, loss = 0.05837150\n","Iteration 119, loss = 0.05516366\n","Iteration 120, loss = 0.05464934\n","Iteration 121, loss = 0.05429569\n","Iteration 122, loss = 0.05305990\n","Iteration 123, loss = 0.05419246\n","Iteration 124, loss = 0.05261313\n","Iteration 125, loss = 0.05076003\n","Iteration 126, loss = 0.05072749\n","Iteration 127, loss = 0.04986101\n","Iteration 128, loss = 0.04887340\n","Iteration 129, loss = 0.04833567\n","Iteration 130, loss = 0.04834126\n","Iteration 131, loss = 0.04655579\n","Iteration 132, loss = 0.04691846\n","Iteration 133, loss = 0.04619545\n","Iteration 134, loss = 0.04528135\n","Iteration 135, loss = 0.04462178\n","Iteration 136, loss = 0.04459535\n","Iteration 137, loss = 0.04305425\n","Iteration 138, loss = 0.04281972\n","Iteration 139, loss = 0.04207313\n","Iteration 140, loss = 0.04090927\n","Iteration 141, loss = 0.04048704\n","Iteration 142, loss = 0.04018263\n","Iteration 143, loss = 0.04084784\n","Iteration 144, loss = 0.03981204\n","Iteration 145, loss = 0.03851754\n","Iteration 146, loss = 0.03853981\n","Iteration 147, loss = 0.03822087\n","Iteration 148, loss = 0.03718563\n","Iteration 149, loss = 0.03686996\n","Iteration 150, loss = 0.03557001\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.52912304\n","Iteration 2, loss = 0.31950678\n","Iteration 3, loss = 0.26857284\n","Iteration 4, loss = 0.24729257\n","Iteration 5, loss = 0.23597635\n","Iteration 6, loss = 0.22865656\n","Iteration 7, loss = 0.22240541\n","Iteration 8, loss = 0.21772466\n","Iteration 9, loss = 0.21371202\n","Iteration 10, loss = 0.20996544\n","Iteration 11, loss = 0.20696376\n","Iteration 12, loss = 0.20404041\n","Iteration 13, loss = 0.20102063\n","Iteration 14, loss = 0.19874282\n","Iteration 15, loss = 0.19586638\n","Iteration 16, loss = 0.19383427\n","Iteration 17, loss = 0.19150186\n","Iteration 18, loss = 0.18974836\n","Iteration 19, loss = 0.18784772\n","Iteration 20, loss = 0.18592766\n","Iteration 21, loss = 0.18365816\n","Iteration 22, loss = 0.18203905\n","Iteration 23, loss = 0.17997343\n","Iteration 24, loss = 0.17789666\n","Iteration 25, loss = 0.17640428\n","Iteration 26, loss = 0.17389193\n","Iteration 27, loss = 0.17206808\n","Iteration 28, loss = 0.17128611\n","Iteration 29, loss = 0.16842311\n","Iteration 30, loss = 0.16657838\n","Iteration 31, loss = 0.16537589\n","Iteration 32, loss = 0.16274407\n","Iteration 33, loss = 0.16106681\n","Iteration 34, loss = 0.15874801\n","Iteration 35, loss = 0.15662608\n","Iteration 36, loss = 0.15493978\n","Iteration 37, loss = 0.15331763\n","Iteration 38, loss = 0.15141486\n","Iteration 39, loss = 0.15021508\n","Iteration 40, loss = 0.14880586\n","Iteration 41, loss = 0.14638509\n","Iteration 42, loss = 0.14463476\n","Iteration 43, loss = 0.14258140\n","Iteration 44, loss = 0.14174380\n","Iteration 45, loss = 0.13927468\n","Iteration 46, loss = 0.13745578\n","Iteration 47, loss = 0.13590528\n","Iteration 48, loss = 0.13390794\n","Iteration 49, loss = 0.13253288\n","Iteration 50, loss = 0.13134782\n","Iteration 51, loss = 0.12902748\n","Iteration 52, loss = 0.12804749\n","Iteration 53, loss = 0.12648612\n","Iteration 54, loss = 0.12388256\n","Iteration 55, loss = 0.12311443\n","Iteration 56, loss = 0.12112241\n","Iteration 57, loss = 0.11927168\n","Iteration 58, loss = 0.11819661\n","Iteration 59, loss = 0.11629366\n","Iteration 60, loss = 0.11454219\n","Iteration 61, loss = 0.11324081\n","Iteration 62, loss = 0.11213847\n","Iteration 63, loss = 0.10991698\n","Iteration 64, loss = 0.10904239\n","Iteration 65, loss = 0.10713023\n","Iteration 66, loss = 0.10659588\n","Iteration 67, loss = 0.10511462\n","Iteration 68, loss = 0.10263860\n","Iteration 69, loss = 0.10242546\n","Iteration 70, loss = 0.10071178\n","Iteration 71, loss = 0.09827457\n","Iteration 72, loss = 0.09723124\n","Iteration 73, loss = 0.09595542\n","Iteration 74, loss = 0.09526281\n","Iteration 75, loss = 0.09384487\n","Iteration 76, loss = 0.09125242\n","Iteration 77, loss = 0.08931916\n","Iteration 78, loss = 0.09078332\n","Iteration 79, loss = 0.08744129\n","Iteration 80, loss = 0.08732097\n","Iteration 81, loss = 0.08461435\n","Iteration 82, loss = 0.08413808\n","Iteration 83, loss = 0.08398635\n","Iteration 84, loss = 0.08056250\n","Iteration 85, loss = 0.07939600\n","Iteration 86, loss = 0.07858803\n","Iteration 87, loss = 0.07807606\n","Iteration 88, loss = 0.07565891\n","Iteration 89, loss = 0.07504389\n","Iteration 90, loss = 0.07429997\n","Iteration 91, loss = 0.07204281\n","Iteration 92, loss = 0.07115528\n","Iteration 93, loss = 0.07087558\n","Iteration 94, loss = 0.06989592\n","Iteration 95, loss = 0.06922633\n","Iteration 96, loss = 0.06708024\n","Iteration 97, loss = 0.06631796\n","Iteration 98, loss = 0.06554943\n","Iteration 99, loss = 0.06544286\n","Iteration 100, loss = 0.06486762\n","Iteration 101, loss = 0.06128970\n","Iteration 102, loss = 0.06077498\n","Iteration 103, loss = 0.06008792\n","Iteration 104, loss = 0.06106705\n","Iteration 105, loss = 0.05821712\n","Iteration 106, loss = 0.05822779\n","Iteration 107, loss = 0.05689310\n","Iteration 108, loss = 0.05525878\n","Iteration 109, loss = 0.05467937\n","Iteration 110, loss = 0.05376010\n","Iteration 111, loss = 0.05346201\n","Iteration 112, loss = 0.05257893\n","Iteration 113, loss = 0.05087217\n","Iteration 114, loss = 0.05049224\n","Iteration 115, loss = 0.04887918\n","Iteration 116, loss = 0.04818860\n","Iteration 117, loss = 0.04870463\n","Iteration 118, loss = 0.04798668\n","Iteration 119, loss = 0.04666206\n","Iteration 120, loss = 0.04609617\n","Iteration 121, loss = 0.04556664\n","Iteration 122, loss = 0.04416028\n","Iteration 123, loss = 0.04285135\n","Iteration 124, loss = 0.04238232\n","Iteration 125, loss = 0.04204302\n","Iteration 126, loss = 0.04161674\n","Iteration 127, loss = 0.04046349\n","Iteration 128, loss = 0.04005371\n","Iteration 129, loss = 0.03852454\n","Iteration 130, loss = 0.03791273\n","Iteration 131, loss = 0.03790012\n","Iteration 132, loss = 0.03678520\n","Iteration 133, loss = 0.03657614\n","Iteration 134, loss = 0.03603506\n","Iteration 135, loss = 0.03488668\n","Iteration 136, loss = 0.03542038\n","Iteration 137, loss = 0.03536543\n","Iteration 138, loss = 0.03402485\n","Iteration 139, loss = 0.03251144\n","Iteration 140, loss = 0.03174363\n","Iteration 141, loss = 0.03161430\n","Iteration 142, loss = 0.03079171\n","Iteration 143, loss = 0.02985884\n","Iteration 144, loss = 0.02885590\n","Iteration 145, loss = 0.02821342\n","Iteration 146, loss = 0.02751094\n","Iteration 147, loss = 0.02688116\n","Iteration 148, loss = 0.02663781\n","Iteration 149, loss = 0.02653957\n","Iteration 150, loss = 0.02526893\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.59202871\n","Iteration 2, loss = 0.43801365\n","Iteration 3, loss = 0.36289752\n","Iteration 4, loss = 0.32136583\n","Iteration 5, loss = 0.29594784\n","Iteration 6, loss = 0.27925524\n","Iteration 7, loss = 0.26757740\n","Iteration 8, loss = 0.25926380\n","Iteration 9, loss = 0.25304321\n","Iteration 10, loss = 0.24821108\n","Iteration 11, loss = 0.24431892\n","Iteration 12, loss = 0.24108682\n","Iteration 13, loss = 0.23842421\n","Iteration 14, loss = 0.23618400\n","Iteration 15, loss = 0.23421851\n","Iteration 16, loss = 0.23248361\n","Iteration 17, loss = 0.23100283\n","Iteration 18, loss = 0.22963130\n","Iteration 19, loss = 0.22841307\n","Iteration 20, loss = 0.22740860\n","Iteration 21, loss = 0.22640757\n","Iteration 22, loss = 0.22552593\n","Iteration 23, loss = 0.22470272\n","Iteration 24, loss = 0.22395529\n","Iteration 25, loss = 0.22326580\n","Iteration 26, loss = 0.22266938\n","Iteration 27, loss = 0.22207286\n","Iteration 28, loss = 0.22150691\n","Iteration 29, loss = 0.22097506\n","Iteration 30, loss = 0.22053615\n","Iteration 31, loss = 0.22005657\n","Iteration 32, loss = 0.21956641\n","Iteration 33, loss = 0.21914880\n","Iteration 34, loss = 0.21882292\n","Iteration 35, loss = 0.21838197\n","Iteration 36, loss = 0.21806658\n","Iteration 37, loss = 0.21769181\n","Iteration 38, loss = 0.21731538\n","Iteration 39, loss = 0.21699556\n","Iteration 40, loss = 0.21664775\n","Iteration 41, loss = 0.21635001\n","Iteration 42, loss = 0.21603710\n","Iteration 43, loss = 0.21571339\n","Iteration 44, loss = 0.21541510\n","Iteration 45, loss = 0.21512216\n","Iteration 46, loss = 0.21486874\n","Iteration 47, loss = 0.21459803\n","Iteration 48, loss = 0.21429599\n","Iteration 49, loss = 0.21406667\n","Iteration 50, loss = 0.21380260\n","Iteration 51, loss = 0.21352554\n","Iteration 52, loss = 0.21334451\n","Iteration 53, loss = 0.21308636\n","Iteration 54, loss = 0.21285315\n","Iteration 55, loss = 0.21257762\n","Iteration 56, loss = 0.21227890\n","Iteration 57, loss = 0.21213626\n","Iteration 58, loss = 0.21188300\n","Iteration 59, loss = 0.21166950\n","Iteration 60, loss = 0.21139416\n","Iteration 61, loss = 0.21114935\n","Iteration 62, loss = 0.21095534\n","Iteration 63, loss = 0.21069685\n","Iteration 64, loss = 0.21054945\n","Iteration 65, loss = 0.21028918\n","Iteration 66, loss = 0.21017222\n","Iteration 67, loss = 0.20991090\n","Iteration 68, loss = 0.20969186\n","Iteration 69, loss = 0.20951119\n","Iteration 70, loss = 0.20932452\n","Iteration 71, loss = 0.20912173\n","Iteration 72, loss = 0.20891411\n","Iteration 73, loss = 0.20866352\n","Iteration 74, loss = 0.20848913\n","Iteration 75, loss = 0.20831415\n","Iteration 76, loss = 0.20813374\n","Iteration 77, loss = 0.20794425\n","Iteration 78, loss = 0.20773304\n","Iteration 79, loss = 0.20756429\n","Iteration 80, loss = 0.20733224\n","Iteration 81, loss = 0.20718855\n","Iteration 82, loss = 0.20701789\n","Iteration 83, loss = 0.20681156\n","Iteration 84, loss = 0.20666579\n","Iteration 85, loss = 0.20642763\n","Iteration 86, loss = 0.20624581\n","Iteration 87, loss = 0.20609060\n","Iteration 88, loss = 0.20591995\n","Iteration 89, loss = 0.20571580\n","Iteration 90, loss = 0.20556386\n","Iteration 91, loss = 0.20535771\n","Iteration 92, loss = 0.20518105\n","Iteration 93, loss = 0.20503194\n","Iteration 94, loss = 0.20485782\n","Iteration 95, loss = 0.20468636\n","Iteration 96, loss = 0.20449765\n","Iteration 97, loss = 0.20431783\n","Iteration 98, loss = 0.20419366\n","Iteration 99, loss = 0.20396812\n","Iteration 100, loss = 0.20382589\n","Iteration 101, loss = 0.20372580\n","Iteration 102, loss = 0.20349034\n","Iteration 103, loss = 0.20333931\n","Iteration 104, loss = 0.20313410\n","Iteration 105, loss = 0.20299057\n","Iteration 106, loss = 0.20280551\n","Iteration 107, loss = 0.20265102\n","Iteration 108, loss = 0.20250781\n","Iteration 109, loss = 0.20236614\n","Iteration 110, loss = 0.20217059\n","Iteration 111, loss = 0.20202484\n","Iteration 112, loss = 0.20184599\n","Iteration 113, loss = 0.20170524\n","Iteration 114, loss = 0.20151387\n","Iteration 115, loss = 0.20133375\n","Iteration 116, loss = 0.20121965\n","Iteration 117, loss = 0.20103039\n","Iteration 118, loss = 0.20091776\n","Iteration 119, loss = 0.20074524\n","Iteration 120, loss = 0.20057550\n","Iteration 121, loss = 0.20043362\n","Iteration 122, loss = 0.20025651\n","Iteration 123, loss = 0.20014772\n","Iteration 124, loss = 0.19998564\n","Iteration 125, loss = 0.19982738\n","Iteration 126, loss = 0.19965814\n","Iteration 127, loss = 0.19948841\n","Iteration 128, loss = 0.19931353\n","Iteration 129, loss = 0.19917316\n","Iteration 130, loss = 0.19905357\n","Iteration 131, loss = 0.19885851\n","Iteration 132, loss = 0.19873142\n","Iteration 133, loss = 0.19854102\n","Iteration 134, loss = 0.19842127\n","Iteration 135, loss = 0.19824758\n","Iteration 136, loss = 0.19807763\n","Iteration 137, loss = 0.19796952\n","Iteration 138, loss = 0.19783682\n","Iteration 139, loss = 0.19764058\n","Iteration 140, loss = 0.19747792\n","Iteration 141, loss = 0.19731151\n","Iteration 142, loss = 0.19718135\n","Iteration 143, loss = 0.19705588\n","Iteration 144, loss = 0.19691177\n","Iteration 145, loss = 0.19672385\n","Iteration 146, loss = 0.19664889\n","Iteration 147, loss = 0.19642801\n","Iteration 148, loss = 0.19631001\n","Iteration 149, loss = 0.19612663\n","Iteration 150, loss = 0.19600312\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.76080301\n","Iteration 2, loss = 0.47817928\n","Iteration 3, loss = 0.37746249\n","Iteration 4, loss = 0.32838611\n","Iteration 5, loss = 0.29986331\n","Iteration 6, loss = 0.28189342\n","Iteration 7, loss = 0.26994731\n","Iteration 8, loss = 0.26145252\n","Iteration 9, loss = 0.25521252\n","Iteration 10, loss = 0.25051453\n","Iteration 11, loss = 0.24667147\n","Iteration 12, loss = 0.24358062\n","Iteration 13, loss = 0.24103810\n","Iteration 14, loss = 0.23873017\n","Iteration 15, loss = 0.23687938\n","Iteration 16, loss = 0.23519651\n","Iteration 17, loss = 0.23373236\n","Iteration 18, loss = 0.23244510\n","Iteration 19, loss = 0.23118190\n","Iteration 20, loss = 0.23010901\n","Iteration 21, loss = 0.22909982\n","Iteration 22, loss = 0.22821722\n","Iteration 23, loss = 0.22740343\n","Iteration 24, loss = 0.22661482\n","Iteration 25, loss = 0.22591979\n","Iteration 26, loss = 0.22525415\n","Iteration 27, loss = 0.22466536\n","Iteration 28, loss = 0.22406518\n","Iteration 29, loss = 0.22347351\n","Iteration 30, loss = 0.22293218\n","Iteration 31, loss = 0.22247792\n","Iteration 32, loss = 0.22199878\n","Iteration 33, loss = 0.22150854\n","Iteration 34, loss = 0.22110090\n","Iteration 35, loss = 0.22069708\n","Iteration 36, loss = 0.22024904\n","Iteration 37, loss = 0.21985442\n","Iteration 38, loss = 0.21946601\n","Iteration 39, loss = 0.21910967\n","Iteration 40, loss = 0.21871535\n","Iteration 41, loss = 0.21840303\n","Iteration 42, loss = 0.21806514\n","Iteration 43, loss = 0.21771069\n","Iteration 44, loss = 0.21738273\n","Iteration 45, loss = 0.21707193\n","Iteration 46, loss = 0.21674563\n","Iteration 47, loss = 0.21647003\n","Iteration 48, loss = 0.21615076\n","Iteration 49, loss = 0.21586972\n","Iteration 50, loss = 0.21559504\n","Iteration 51, loss = 0.21530510\n","Iteration 52, loss = 0.21500140\n","Iteration 53, loss = 0.21474084\n","Iteration 54, loss = 0.21449713\n","Iteration 55, loss = 0.21423990\n","Iteration 56, loss = 0.21394933\n","Iteration 57, loss = 0.21370158\n","Iteration 58, loss = 0.21341288\n","Iteration 59, loss = 0.21321654\n","Iteration 60, loss = 0.21295010\n","Iteration 61, loss = 0.21266394\n","Iteration 62, loss = 0.21244757\n","Iteration 63, loss = 0.21219930\n","Iteration 64, loss = 0.21201390\n","Iteration 65, loss = 0.21171710\n","Iteration 66, loss = 0.21153692\n","Iteration 67, loss = 0.21129171\n","Iteration 68, loss = 0.21104486\n","Iteration 69, loss = 0.21083871\n","Iteration 70, loss = 0.21068155\n","Iteration 71, loss = 0.21042346\n","Iteration 72, loss = 0.21018091\n","Iteration 73, loss = 0.21002875\n","Iteration 74, loss = 0.20975548\n","Iteration 75, loss = 0.20953954\n","Iteration 76, loss = 0.20936573\n","Iteration 77, loss = 0.20910081\n","Iteration 78, loss = 0.20891794\n","Iteration 79, loss = 0.20868974\n","Iteration 80, loss = 0.20854688\n","Iteration 81, loss = 0.20829114\n","Iteration 82, loss = 0.20813223\n","Iteration 83, loss = 0.20791186\n","Iteration 84, loss = 0.20773552\n","Iteration 85, loss = 0.20750040\n","Iteration 86, loss = 0.20730739\n","Iteration 87, loss = 0.20714296\n","Iteration 88, loss = 0.20691589\n","Iteration 89, loss = 0.20675227\n","Iteration 90, loss = 0.20652569\n","Iteration 91, loss = 0.20639649\n","Iteration 92, loss = 0.20619501\n","Iteration 93, loss = 0.20595184\n","Iteration 94, loss = 0.20584774\n","Iteration 95, loss = 0.20558658\n","Iteration 96, loss = 0.20542099\n","Iteration 97, loss = 0.20527603\n","Iteration 98, loss = 0.20506584\n","Iteration 99, loss = 0.20485329\n","Iteration 100, loss = 0.20469570\n","Iteration 101, loss = 0.20453534\n","Iteration 102, loss = 0.20433037\n","Iteration 103, loss = 0.20413371\n","Iteration 104, loss = 0.20401765\n","Iteration 105, loss = 0.20382492\n","Iteration 106, loss = 0.20362218\n","Iteration 107, loss = 0.20345002\n","Iteration 108, loss = 0.20328790\n","Iteration 109, loss = 0.20310255\n","Iteration 110, loss = 0.20291860\n","Iteration 111, loss = 0.20273208\n","Iteration 112, loss = 0.20257572\n","Iteration 113, loss = 0.20237913\n","Iteration 114, loss = 0.20226659\n","Iteration 115, loss = 0.20205267\n","Iteration 116, loss = 0.20189638\n","Iteration 117, loss = 0.20170368\n","Iteration 118, loss = 0.20162122\n","Iteration 119, loss = 0.20140549\n","Iteration 120, loss = 0.20121585\n","Iteration 121, loss = 0.20108496\n","Iteration 122, loss = 0.20086834\n","Iteration 123, loss = 0.20071177\n","Iteration 124, loss = 0.20056199\n","Iteration 125, loss = 0.20036308\n","Iteration 126, loss = 0.20020794\n","Iteration 127, loss = 0.20004543\n","Iteration 128, loss = 0.19987362\n","Iteration 129, loss = 0.19968510\n","Iteration 130, loss = 0.19953545\n","Iteration 131, loss = 0.19938962\n","Iteration 132, loss = 0.19920019\n","Iteration 133, loss = 0.19905229\n","Iteration 134, loss = 0.19889492\n","Iteration 135, loss = 0.19873591\n","Iteration 136, loss = 0.19855524\n","Iteration 137, loss = 0.19844906\n","Iteration 138, loss = 0.19823334\n","Iteration 139, loss = 0.19806475\n","Iteration 140, loss = 0.19789043\n","Iteration 141, loss = 0.19773683\n","Iteration 142, loss = 0.19763281\n","Iteration 143, loss = 0.19740188\n","Iteration 144, loss = 0.19724334\n","Iteration 145, loss = 0.19706889\n","Iteration 146, loss = 0.19691350\n","Iteration 147, loss = 0.19679193\n","Iteration 148, loss = 0.19659318\n","Iteration 149, loss = 0.19645128\n","Iteration 150, loss = 0.19627253\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.60000223\n","Iteration 2, loss = 0.35028672\n","Iteration 3, loss = 0.29691271\n","Iteration 4, loss = 0.27344052\n","Iteration 5, loss = 0.25983843\n","Iteration 6, loss = 0.25076162\n","Iteration 7, loss = 0.24438294\n","Iteration 8, loss = 0.23947147\n","Iteration 9, loss = 0.23566633\n","Iteration 10, loss = 0.23246027\n","Iteration 11, loss = 0.22991523\n","Iteration 12, loss = 0.22766028\n","Iteration 13, loss = 0.22576134\n","Iteration 14, loss = 0.22408323\n","Iteration 15, loss = 0.22266499\n","Iteration 16, loss = 0.22145983\n","Iteration 17, loss = 0.22030958\n","Iteration 18, loss = 0.21933375\n","Iteration 19, loss = 0.21839159\n","Iteration 20, loss = 0.21762858\n","Iteration 21, loss = 0.21687737\n","Iteration 22, loss = 0.21623380\n","Iteration 23, loss = 0.21561404\n","Iteration 24, loss = 0.21504889\n","Iteration 25, loss = 0.21453472\n","Iteration 26, loss = 0.21407036\n","Iteration 27, loss = 0.21357229\n","Iteration 28, loss = 0.21312684\n","Iteration 29, loss = 0.21274629\n","Iteration 30, loss = 0.21235765\n","Iteration 31, loss = 0.21202239\n","Iteration 32, loss = 0.21164087\n","Iteration 33, loss = 0.21128094\n","Iteration 34, loss = 0.21094522\n","Iteration 35, loss = 0.21069082\n","Iteration 36, loss = 0.21034726\n","Iteration 37, loss = 0.21003561\n","Iteration 38, loss = 0.20976746\n","Iteration 39, loss = 0.20948760\n","Iteration 40, loss = 0.20924375\n","Iteration 41, loss = 0.20892996\n","Iteration 42, loss = 0.20870597\n","Iteration 43, loss = 0.20841912\n","Iteration 44, loss = 0.20816375\n","Iteration 45, loss = 0.20787048\n","Iteration 46, loss = 0.20769706\n","Iteration 47, loss = 0.20743655\n","Iteration 48, loss = 0.20723789\n","Iteration 49, loss = 0.20703176\n","Iteration 50, loss = 0.20672346\n","Iteration 51, loss = 0.20656054\n","Iteration 52, loss = 0.20636734\n","Iteration 53, loss = 0.20615980\n","Iteration 54, loss = 0.20589759\n","Iteration 55, loss = 0.20567694\n","Iteration 56, loss = 0.20549633\n","Iteration 57, loss = 0.20531644\n","Iteration 58, loss = 0.20506565\n","Iteration 59, loss = 0.20487078\n","Iteration 60, loss = 0.20466927\n","Iteration 61, loss = 0.20447852\n","Iteration 62, loss = 0.20431194\n","Iteration 63, loss = 0.20409969\n","Iteration 64, loss = 0.20392477\n","Iteration 65, loss = 0.20365328\n","Iteration 66, loss = 0.20355491\n","Iteration 67, loss = 0.20331661\n","Iteration 68, loss = 0.20313259\n","Iteration 69, loss = 0.20297845\n","Iteration 70, loss = 0.20279121\n","Iteration 71, loss = 0.20257887\n","Iteration 72, loss = 0.20238395\n","Iteration 73, loss = 0.20225173\n","Iteration 74, loss = 0.20202735\n","Iteration 75, loss = 0.20185762\n","Iteration 76, loss = 0.20165406\n","Iteration 77, loss = 0.20151111\n","Iteration 78, loss = 0.20133009\n","Iteration 79, loss = 0.20117781\n","Iteration 80, loss = 0.20104452\n","Iteration 81, loss = 0.20083328\n","Iteration 82, loss = 0.20063444\n","Iteration 83, loss = 0.20048019\n","Iteration 84, loss = 0.20033965\n","Iteration 85, loss = 0.20009280\n","Iteration 86, loss = 0.19999959\n","Iteration 87, loss = 0.19981986\n","Iteration 88, loss = 0.19963359\n","Iteration 89, loss = 0.19948209\n","Iteration 90, loss = 0.19928517\n","Iteration 91, loss = 0.19912650\n","Iteration 92, loss = 0.19897369\n","Iteration 93, loss = 0.19878681\n","Iteration 94, loss = 0.19861636\n","Iteration 95, loss = 0.19848632\n","Iteration 96, loss = 0.19828908\n","Iteration 97, loss = 0.19815099\n","Iteration 98, loss = 0.19800095\n","Iteration 99, loss = 0.19788719\n","Iteration 100, loss = 0.19768313\n","Iteration 101, loss = 0.19756786\n","Iteration 102, loss = 0.19735581\n","Iteration 103, loss = 0.19721783\n","Iteration 104, loss = 0.19710133\n","Iteration 105, loss = 0.19689194\n","Iteration 106, loss = 0.19676292\n","Iteration 107, loss = 0.19662516\n","Iteration 108, loss = 0.19648174\n","Iteration 109, loss = 0.19630036\n","Iteration 110, loss = 0.19621447\n","Iteration 111, loss = 0.19598776\n","Iteration 112, loss = 0.19582629\n","Iteration 113, loss = 0.19572056\n","Iteration 114, loss = 0.19552861\n","Iteration 115, loss = 0.19535332\n","Iteration 116, loss = 0.19524066\n","Iteration 117, loss = 0.19510107\n","Iteration 118, loss = 0.19498015\n","Iteration 119, loss = 0.19478597\n","Iteration 120, loss = 0.19465363\n","Iteration 121, loss = 0.19447593\n","Iteration 122, loss = 0.19436065\n","Iteration 123, loss = 0.19423353\n","Iteration 124, loss = 0.19409476\n","Iteration 125, loss = 0.19392652\n","Iteration 126, loss = 0.19377490\n","Iteration 127, loss = 0.19359275\n","Iteration 128, loss = 0.19355564\n","Iteration 129, loss = 0.19332643\n","Iteration 130, loss = 0.19317182\n","Iteration 131, loss = 0.19302027\n","Iteration 132, loss = 0.19292038\n","Iteration 133, loss = 0.19277215\n","Iteration 134, loss = 0.19259320\n","Iteration 135, loss = 0.19246149\n","Iteration 136, loss = 0.19228995\n","Iteration 137, loss = 0.19216354\n","Iteration 138, loss = 0.19202115\n","Iteration 139, loss = 0.19190988\n","Iteration 140, loss = 0.19172467\n","Iteration 141, loss = 0.19157577\n","Iteration 142, loss = 0.19145159\n","Iteration 143, loss = 0.19128169\n","Iteration 144, loss = 0.19115874\n","Iteration 145, loss = 0.19102020\n","Iteration 146, loss = 0.19083326\n","Iteration 147, loss = 0.19072736\n","Iteration 148, loss = 0.19054134\n","Iteration 149, loss = 0.19046604\n","Iteration 150, loss = 0.19039967\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.69517463\n","Iteration 2, loss = 0.44901218\n","Iteration 3, loss = 0.36005747\n","Iteration 4, loss = 0.31589758\n","Iteration 5, loss = 0.28954883\n","Iteration 6, loss = 0.27276387\n","Iteration 7, loss = 0.26137085\n","Iteration 8, loss = 0.25343999\n","Iteration 9, loss = 0.24762557\n","Iteration 10, loss = 0.24313239\n","Iteration 11, loss = 0.23959211\n","Iteration 12, loss = 0.23671804\n","Iteration 13, loss = 0.23429296\n","Iteration 14, loss = 0.23224571\n","Iteration 15, loss = 0.23039136\n","Iteration 16, loss = 0.22886098\n","Iteration 17, loss = 0.22751704\n","Iteration 18, loss = 0.22631004\n","Iteration 19, loss = 0.22522512\n","Iteration 20, loss = 0.22426887\n","Iteration 21, loss = 0.22336950\n","Iteration 22, loss = 0.22257200\n","Iteration 23, loss = 0.22185285\n","Iteration 24, loss = 0.22117800\n","Iteration 25, loss = 0.22053252\n","Iteration 26, loss = 0.21997974\n","Iteration 27, loss = 0.21945117\n","Iteration 28, loss = 0.21893551\n","Iteration 29, loss = 0.21846800\n","Iteration 30, loss = 0.21803064\n","Iteration 31, loss = 0.21761797\n","Iteration 32, loss = 0.21723230\n","Iteration 33, loss = 0.21681217\n","Iteration 34, loss = 0.21645650\n","Iteration 35, loss = 0.21612344\n","Iteration 36, loss = 0.21576069\n","Iteration 37, loss = 0.21542584\n","Iteration 38, loss = 0.21509842\n","Iteration 39, loss = 0.21479929\n","Iteration 40, loss = 0.21449478\n","Iteration 41, loss = 0.21423708\n","Iteration 42, loss = 0.21392369\n","Iteration 43, loss = 0.21364428\n","Iteration 44, loss = 0.21337380\n","Iteration 45, loss = 0.21311562\n","Iteration 46, loss = 0.21288619\n","Iteration 47, loss = 0.21263307\n","Iteration 48, loss = 0.21237926\n","Iteration 49, loss = 0.21214211\n","Iteration 50, loss = 0.21189633\n","Iteration 51, loss = 0.21169059\n","Iteration 52, loss = 0.21147634\n","Iteration 53, loss = 0.21127060\n","Iteration 54, loss = 0.21099083\n","Iteration 55, loss = 0.21079651\n","Iteration 56, loss = 0.21057676\n","Iteration 57, loss = 0.21039340\n","Iteration 58, loss = 0.21015574\n","Iteration 59, loss = 0.20995328\n","Iteration 60, loss = 0.20976404\n","Iteration 61, loss = 0.20952391\n","Iteration 62, loss = 0.20942078\n","Iteration 63, loss = 0.20917602\n","Iteration 64, loss = 0.20898769\n","Iteration 65, loss = 0.20878017\n","Iteration 66, loss = 0.20860997\n","Iteration 67, loss = 0.20844706\n","Iteration 68, loss = 0.20820175\n","Iteration 69, loss = 0.20807447\n","Iteration 70, loss = 0.20791015\n","Iteration 71, loss = 0.20771352\n","Iteration 72, loss = 0.20750805\n","Iteration 73, loss = 0.20733871\n","Iteration 74, loss = 0.20720706\n","Iteration 75, loss = 0.20700014\n","Iteration 76, loss = 0.20682685\n","Iteration 77, loss = 0.20669135\n","Iteration 78, loss = 0.20648619\n","Iteration 79, loss = 0.20628879\n","Iteration 80, loss = 0.20620759\n","Iteration 81, loss = 0.20600861\n","Iteration 82, loss = 0.20584833\n","Iteration 83, loss = 0.20569689\n","Iteration 84, loss = 0.20551536\n","Iteration 85, loss = 0.20536131\n","Iteration 86, loss = 0.20520400\n","Iteration 87, loss = 0.20505002\n","Iteration 88, loss = 0.20488443\n","Iteration 89, loss = 0.20477628\n","Iteration 90, loss = 0.20457579\n","Iteration 91, loss = 0.20442690\n","Iteration 92, loss = 0.20425475\n","Iteration 93, loss = 0.20417120\n","Iteration 94, loss = 0.20398728\n","Iteration 95, loss = 0.20383275\n","Iteration 96, loss = 0.20370178\n","Iteration 97, loss = 0.20352921\n","Iteration 98, loss = 0.20337691\n","Iteration 99, loss = 0.20323573\n","Iteration 100, loss = 0.20311741\n","Iteration 101, loss = 0.20295474\n","Iteration 102, loss = 0.20281170\n","Iteration 103, loss = 0.20269763\n","Iteration 104, loss = 0.20252503\n","Iteration 105, loss = 0.20239134\n","Iteration 106, loss = 0.20224927\n","Iteration 107, loss = 0.20209538\n","Iteration 108, loss = 0.20195671\n","Iteration 109, loss = 0.20177993\n","Iteration 110, loss = 0.20168532\n","Iteration 111, loss = 0.20153051\n","Iteration 112, loss = 0.20137334\n","Iteration 113, loss = 0.20124503\n","Iteration 114, loss = 0.20115787\n","Iteration 115, loss = 0.20097448\n","Iteration 116, loss = 0.20083384\n","Iteration 117, loss = 0.20073073\n","Iteration 118, loss = 0.20055035\n","Iteration 119, loss = 0.20041729\n","Iteration 120, loss = 0.20031776\n","Iteration 121, loss = 0.20015721\n","Iteration 122, loss = 0.20000272\n","Iteration 123, loss = 0.19990382\n","Iteration 124, loss = 0.19975722\n","Iteration 125, loss = 0.19963180\n","Iteration 126, loss = 0.19952349\n","Iteration 127, loss = 0.19936187\n","Iteration 128, loss = 0.19919781\n","Iteration 129, loss = 0.19904592\n","Iteration 130, loss = 0.19890415\n","Iteration 131, loss = 0.19878713\n","Iteration 132, loss = 0.19866841\n","Iteration 133, loss = 0.19854139\n","Iteration 134, loss = 0.19844933\n","Iteration 135, loss = 0.19830912\n","Iteration 136, loss = 0.19814011\n","Iteration 137, loss = 0.19801023\n","Iteration 138, loss = 0.19783006\n","Iteration 139, loss = 0.19775542\n","Iteration 140, loss = 0.19758086\n","Iteration 141, loss = 0.19745877\n","Iteration 142, loss = 0.19733584\n","Iteration 143, loss = 0.19720213\n","Iteration 144, loss = 0.19709786\n","Iteration 145, loss = 0.19695226\n","Iteration 146, loss = 0.19684020\n","Iteration 147, loss = 0.19671036\n","Iteration 148, loss = 0.19658062\n","Iteration 149, loss = 0.19638142\n","Iteration 150, loss = 0.19630589\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.70680125\n","Iteration 2, loss = 0.41781894\n","Iteration 3, loss = 0.33029866\n","Iteration 4, loss = 0.29160138\n","Iteration 5, loss = 0.27033632\n","Iteration 6, loss = 0.25740273\n","Iteration 7, loss = 0.24899173\n","Iteration 8, loss = 0.24296610\n","Iteration 9, loss = 0.23855081\n","Iteration 10, loss = 0.23506275\n","Iteration 11, loss = 0.23234718\n","Iteration 12, loss = 0.23010405\n","Iteration 13, loss = 0.22813755\n","Iteration 14, loss = 0.22647440\n","Iteration 15, loss = 0.22500421\n","Iteration 16, loss = 0.22370955\n","Iteration 17, loss = 0.22261807\n","Iteration 18, loss = 0.22160232\n","Iteration 19, loss = 0.22070561\n","Iteration 20, loss = 0.21984729\n","Iteration 21, loss = 0.21909506\n","Iteration 22, loss = 0.21840198\n","Iteration 23, loss = 0.21778342\n","Iteration 24, loss = 0.21717806\n","Iteration 25, loss = 0.21664473\n","Iteration 26, loss = 0.21610283\n","Iteration 27, loss = 0.21561449\n","Iteration 28, loss = 0.21515338\n","Iteration 29, loss = 0.21469873\n","Iteration 30, loss = 0.21429418\n","Iteration 31, loss = 0.21390424\n","Iteration 32, loss = 0.21351721\n","Iteration 33, loss = 0.21318306\n","Iteration 34, loss = 0.21285347\n","Iteration 35, loss = 0.21246430\n","Iteration 36, loss = 0.21214899\n","Iteration 37, loss = 0.21183945\n","Iteration 38, loss = 0.21151933\n","Iteration 39, loss = 0.21125505\n","Iteration 40, loss = 0.21095136\n","Iteration 41, loss = 0.21070484\n","Iteration 42, loss = 0.21047039\n","Iteration 43, loss = 0.21016279\n","Iteration 44, loss = 0.20990790\n","Iteration 45, loss = 0.20970302\n","Iteration 46, loss = 0.20942161\n","Iteration 47, loss = 0.20922064\n","Iteration 48, loss = 0.20898261\n","Iteration 49, loss = 0.20873746\n","Iteration 50, loss = 0.20852569\n","Iteration 51, loss = 0.20830929\n","Iteration 52, loss = 0.20807707\n","Iteration 53, loss = 0.20787014\n","Iteration 54, loss = 0.20764952\n","Iteration 55, loss = 0.20747269\n","Iteration 56, loss = 0.20726995\n","Iteration 57, loss = 0.20703702\n","Iteration 58, loss = 0.20685970\n","Iteration 59, loss = 0.20663890\n","Iteration 60, loss = 0.20648848\n","Iteration 61, loss = 0.20633006\n","Iteration 62, loss = 0.20609515\n","Iteration 63, loss = 0.20596124\n","Iteration 64, loss = 0.20576204\n","Iteration 65, loss = 0.20553423\n","Iteration 66, loss = 0.20541492\n","Iteration 67, loss = 0.20518232\n","Iteration 68, loss = 0.20502651\n","Iteration 69, loss = 0.20485588\n","Iteration 70, loss = 0.20468197\n","Iteration 71, loss = 0.20450986\n","Iteration 72, loss = 0.20432903\n","Iteration 73, loss = 0.20415398\n","Iteration 74, loss = 0.20400481\n","Iteration 75, loss = 0.20382332\n","Iteration 76, loss = 0.20368685\n","Iteration 77, loss = 0.20347935\n","Iteration 78, loss = 0.20334522\n","Iteration 79, loss = 0.20317043\n","Iteration 80, loss = 0.20298119\n","Iteration 81, loss = 0.20282082\n","Iteration 82, loss = 0.20272510\n","Iteration 83, loss = 0.20258148\n","Iteration 84, loss = 0.20236811\n","Iteration 85, loss = 0.20220389\n","Iteration 86, loss = 0.20207085\n","Iteration 87, loss = 0.20192863\n","Iteration 88, loss = 0.20176158\n","Iteration 89, loss = 0.20158353\n","Iteration 90, loss = 0.20141494\n","Iteration 91, loss = 0.20129357\n","Iteration 92, loss = 0.20108311\n","Iteration 93, loss = 0.20096776\n","Iteration 94, loss = 0.20084369\n","Iteration 95, loss = 0.20066196\n","Iteration 96, loss = 0.20052059\n","Iteration 97, loss = 0.20038324\n","Iteration 98, loss = 0.20024598\n","Iteration 99, loss = 0.20005277\n","Iteration 100, loss = 0.19992663\n","Iteration 101, loss = 0.19976031\n","Iteration 102, loss = 0.19961713\n","Iteration 103, loss = 0.19945092\n","Iteration 104, loss = 0.19930111\n","Iteration 105, loss = 0.19926060\n","Iteration 106, loss = 0.19901923\n","Iteration 107, loss = 0.19884964\n","Iteration 108, loss = 0.19872215\n","Iteration 109, loss = 0.19857755\n","Iteration 110, loss = 0.19844190\n","Iteration 111, loss = 0.19826018\n","Iteration 112, loss = 0.19815087\n","Iteration 113, loss = 0.19803121\n","Iteration 114, loss = 0.19784999\n","Iteration 115, loss = 0.19773102\n","Iteration 116, loss = 0.19755905\n","Iteration 117, loss = 0.19744016\n","Iteration 118, loss = 0.19728397\n","Iteration 119, loss = 0.19718993\n","Iteration 120, loss = 0.19702130\n","Iteration 121, loss = 0.19683875\n","Iteration 122, loss = 0.19672296\n","Iteration 123, loss = 0.19656615\n","Iteration 124, loss = 0.19642904\n","Iteration 125, loss = 0.19626472\n","Iteration 126, loss = 0.19615222\n","Iteration 127, loss = 0.19608422\n","Iteration 128, loss = 0.19585945\n","Iteration 129, loss = 0.19576672\n","Iteration 130, loss = 0.19560002\n","Iteration 131, loss = 0.19549553\n","Iteration 132, loss = 0.19524336\n","Iteration 133, loss = 0.19510368\n","Iteration 134, loss = 0.19503345\n","Iteration 135, loss = 0.19486183\n","Iteration 136, loss = 0.19473057\n","Iteration 137, loss = 0.19458338\n","Iteration 138, loss = 0.19449182\n","Iteration 139, loss = 0.19431575\n","Iteration 140, loss = 0.19424131\n","Iteration 141, loss = 0.19399292\n","Iteration 142, loss = 0.19387547\n","Iteration 143, loss = 0.19374524\n","Iteration 144, loss = 0.19363439\n","Iteration 145, loss = 0.19347913\n","Iteration 146, loss = 0.19331577\n","Iteration 147, loss = 0.19315586\n","Iteration 148, loss = 0.19305112\n","Iteration 149, loss = 0.19288082\n","Iteration 150, loss = 0.19277459\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.36988754\n","Iteration 2, loss = 0.22874888\n","Iteration 3, loss = 0.21860683\n","Iteration 4, loss = 0.21257814\n","Iteration 5, loss = 0.20945933\n","Iteration 6, loss = 0.20520115\n","Iteration 7, loss = 0.20201296\n","Iteration 8, loss = 0.19873682\n","Iteration 9, loss = 0.19662472\n","Iteration 10, loss = 0.19309312\n","Iteration 11, loss = 0.19023019\n","Iteration 12, loss = 0.18763799\n","Iteration 13, loss = 0.18539526\n","Iteration 14, loss = 0.18328531\n","Iteration 15, loss = 0.17975949\n","Iteration 16, loss = 0.17684542\n","Iteration 17, loss = 0.17466630\n","Iteration 18, loss = 0.17103818\n","Iteration 19, loss = 0.16860880\n","Iteration 20, loss = 0.16526579\n","Iteration 21, loss = 0.16187058\n","Iteration 22, loss = 0.15917253\n","Iteration 23, loss = 0.15544175\n","Iteration 24, loss = 0.15139200\n","Iteration 25, loss = 0.14895397\n","Iteration 26, loss = 0.14614582\n","Iteration 27, loss = 0.14185418\n","Iteration 28, loss = 0.13803808\n","Iteration 29, loss = 0.13602554\n","Iteration 30, loss = 0.13114035\n","Iteration 31, loss = 0.12822056\n","Iteration 32, loss = 0.12390826\n","Iteration 33, loss = 0.12162779\n","Iteration 34, loss = 0.11622127\n","Iteration 35, loss = 0.11465371\n","Iteration 36, loss = 0.11181063\n","Iteration 37, loss = 0.10681834\n","Iteration 38, loss = 0.10601074\n","Iteration 39, loss = 0.10060005\n","Iteration 40, loss = 0.09647275\n","Iteration 41, loss = 0.09589309\n","Iteration 42, loss = 0.09136213\n","Iteration 43, loss = 0.08918319\n","Iteration 44, loss = 0.08591883\n","Iteration 45, loss = 0.08124110\n","Iteration 46, loss = 0.07872532\n","Iteration 47, loss = 0.07595591\n","Iteration 48, loss = 0.07428822\n","Iteration 49, loss = 0.07155010\n","Iteration 50, loss = 0.06967542\n","Iteration 51, loss = 0.06652042\n","Iteration 52, loss = 0.06366707\n","Iteration 53, loss = 0.06291266\n","Iteration 54, loss = 0.05983441\n","Iteration 55, loss = 0.05522009\n","Iteration 56, loss = 0.05375743\n","Iteration 57, loss = 0.05096868\n","Iteration 58, loss = 0.04889862\n","Iteration 59, loss = 0.04722270\n","Iteration 60, loss = 0.04455729\n","Iteration 61, loss = 0.04255538\n","Iteration 62, loss = 0.04050662\n","Iteration 63, loss = 0.04006333\n","Iteration 64, loss = 0.03708552\n","Iteration 65, loss = 0.03580892\n","Iteration 66, loss = 0.03400179\n","Iteration 67, loss = 0.03187813\n","Iteration 68, loss = 0.03077309\n","Iteration 69, loss = 0.03041414\n","Iteration 70, loss = 0.02860530\n","Iteration 71, loss = 0.02597950\n","Iteration 72, loss = 0.02504495\n","Iteration 73, loss = 0.02373389\n","Iteration 74, loss = 0.02281849\n","Iteration 75, loss = 0.02181583\n","Iteration 76, loss = 0.02045055\n","Iteration 77, loss = 0.01879107\n","Iteration 78, loss = 0.01860636\n","Iteration 79, loss = 0.01719720\n","Iteration 80, loss = 0.01605274\n","Iteration 81, loss = 0.01535647\n","Iteration 82, loss = 0.01446682\n","Iteration 83, loss = 0.01355885\n","Iteration 84, loss = 0.01399218\n","Iteration 85, loss = 0.01266347\n","Iteration 86, loss = 0.01259284\n","Iteration 87, loss = 0.01157996\n","Iteration 88, loss = 0.01094978\n","Iteration 89, loss = 0.01018727\n","Iteration 90, loss = 0.00971145\n","Iteration 91, loss = 0.00929165\n","Iteration 92, loss = 0.00862845\n","Iteration 93, loss = 0.00868827\n","Iteration 94, loss = 0.00793586\n","Iteration 95, loss = 0.00753023\n","Iteration 96, loss = 0.00730561\n","Iteration 97, loss = 0.00678201\n","Iteration 98, loss = 0.00655422\n","Iteration 99, loss = 0.00602817\n","Iteration 100, loss = 0.00581359\n","Iteration 101, loss = 0.00553584\n","Iteration 102, loss = 0.00527309\n","Iteration 103, loss = 0.00500897\n","Iteration 104, loss = 0.00482946\n","Iteration 105, loss = 0.00459438\n","Iteration 106, loss = 0.00460863\n","Iteration 107, loss = 0.00430320\n","Iteration 108, loss = 0.00407143\n","Iteration 109, loss = 0.00396076\n","Iteration 110, loss = 0.00374747\n","Iteration 111, loss = 0.00357248\n","Iteration 112, loss = 0.00346348\n","Iteration 113, loss = 0.00337384\n","Iteration 114, loss = 0.00325898\n","Iteration 115, loss = 0.00307735\n","Iteration 116, loss = 0.00299846\n","Iteration 117, loss = 0.00294312\n","Iteration 118, loss = 0.00285105\n","Iteration 119, loss = 0.00266154\n","Iteration 120, loss = 0.00257439\n","Iteration 121, loss = 0.00248770\n","Iteration 122, loss = 0.00247157\n","Iteration 123, loss = 0.00234147\n","Iteration 124, loss = 0.00225671\n","Iteration 125, loss = 0.00223389\n","Iteration 126, loss = 0.00214422\n","Iteration 127, loss = 0.00207148\n","Iteration 128, loss = 0.00202494\n","Iteration 129, loss = 0.00196762\n","Iteration 130, loss = 0.00185216\n","Iteration 131, loss = 0.00183706\n","Iteration 132, loss = 0.00176889\n","Iteration 133, loss = 0.00170966\n","Iteration 134, loss = 0.00166284\n","Iteration 135, loss = 0.00160806\n","Iteration 136, loss = 0.00157720\n","Iteration 137, loss = 0.00153082\n","Iteration 138, loss = 0.00150592\n","Iteration 139, loss = 0.00147309\n","Iteration 140, loss = 0.00142787\n","Iteration 141, loss = 0.00140390\n","Iteration 142, loss = 0.00134684\n","Iteration 143, loss = 0.00138696\n","Iteration 144, loss = 0.00129605\n","Iteration 145, loss = 0.00124829\n","Iteration 146, loss = 0.00122019\n","Iteration 147, loss = 0.00120549\n","Iteration 148, loss = 0.00117321\n","Iteration 149, loss = 0.00116238\n","Iteration 150, loss = 0.00113850\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.36224978\n","Iteration 2, loss = 0.23390602\n","Iteration 3, loss = 0.22268409\n","Iteration 4, loss = 0.21672374\n","Iteration 5, loss = 0.21298863\n","Iteration 6, loss = 0.20941777\n","Iteration 7, loss = 0.20594590\n","Iteration 8, loss = 0.20229794\n","Iteration 9, loss = 0.19947338\n","Iteration 10, loss = 0.19738955\n","Iteration 11, loss = 0.19460787\n","Iteration 12, loss = 0.19187726\n","Iteration 13, loss = 0.18816566\n","Iteration 14, loss = 0.18590533\n","Iteration 15, loss = 0.18366746\n","Iteration 16, loss = 0.18058195\n","Iteration 17, loss = 0.17729946\n","Iteration 18, loss = 0.17458142\n","Iteration 19, loss = 0.17202881\n","Iteration 20, loss = 0.16715430\n","Iteration 21, loss = 0.16473751\n","Iteration 22, loss = 0.16331397\n","Iteration 23, loss = 0.15986025\n","Iteration 24, loss = 0.15659227\n","Iteration 25, loss = 0.15280999\n","Iteration 26, loss = 0.14820622\n","Iteration 27, loss = 0.14558872\n","Iteration 28, loss = 0.14362418\n","Iteration 29, loss = 0.13906483\n","Iteration 30, loss = 0.13475777\n","Iteration 31, loss = 0.13146428\n","Iteration 32, loss = 0.12841747\n","Iteration 33, loss = 0.12383149\n","Iteration 34, loss = 0.12147998\n","Iteration 35, loss = 0.11862648\n","Iteration 36, loss = 0.11379410\n","Iteration 37, loss = 0.11047504\n","Iteration 38, loss = 0.10551757\n","Iteration 39, loss = 0.10218531\n","Iteration 40, loss = 0.10133602\n","Iteration 41, loss = 0.09828938\n","Iteration 42, loss = 0.09384859\n","Iteration 43, loss = 0.08953846\n","Iteration 44, loss = 0.08712837\n","Iteration 45, loss = 0.08426927\n","Iteration 46, loss = 0.08140012\n","Iteration 47, loss = 0.07748749\n","Iteration 48, loss = 0.07633264\n","Iteration 49, loss = 0.07207398\n","Iteration 50, loss = 0.06841019\n","Iteration 51, loss = 0.06469084\n","Iteration 52, loss = 0.06358396\n","Iteration 53, loss = 0.06008815\n","Iteration 54, loss = 0.05699344\n","Iteration 55, loss = 0.05693322\n","Iteration 56, loss = 0.05454125\n","Iteration 57, loss = 0.05210833\n","Iteration 58, loss = 0.05071902\n","Iteration 59, loss = 0.04777797\n","Iteration 60, loss = 0.04577849\n","Iteration 61, loss = 0.04261425\n","Iteration 62, loss = 0.04027421\n","Iteration 63, loss = 0.03838329\n","Iteration 64, loss = 0.03533733\n","Iteration 65, loss = 0.03533567\n","Iteration 66, loss = 0.03389166\n","Iteration 67, loss = 0.02999980\n","Iteration 68, loss = 0.02988513\n","Iteration 69, loss = 0.02717540\n","Iteration 70, loss = 0.02629010\n","Iteration 71, loss = 0.02481103\n","Iteration 72, loss = 0.02330157\n","Iteration 73, loss = 0.02227791\n","Iteration 74, loss = 0.02141086\n","Iteration 75, loss = 0.02003836\n","Iteration 76, loss = 0.01931138\n","Iteration 77, loss = 0.01774160\n","Iteration 78, loss = 0.01714256\n","Iteration 79, loss = 0.01563370\n","Iteration 80, loss = 0.01529478\n","Iteration 81, loss = 0.01455567\n","Iteration 82, loss = 0.01327886\n","Iteration 83, loss = 0.01223905\n","Iteration 84, loss = 0.01165305\n","Iteration 85, loss = 0.01144332\n","Iteration 86, loss = 0.01105565\n","Iteration 87, loss = 0.00985358\n","Iteration 88, loss = 0.00932849\n","Iteration 89, loss = 0.00894832\n","Iteration 90, loss = 0.00884094\n","Iteration 91, loss = 0.00804627\n","Iteration 92, loss = 0.00768743\n","Iteration 93, loss = 0.00706846\n","Iteration 94, loss = 0.00681451\n","Iteration 95, loss = 0.00641659\n","Iteration 96, loss = 0.00602059\n","Iteration 97, loss = 0.00585354\n","Iteration 98, loss = 0.00559319\n","Iteration 99, loss = 0.00527672\n","Iteration 100, loss = 0.00487743\n","Iteration 101, loss = 0.00460727\n","Iteration 102, loss = 0.00441920\n","Iteration 103, loss = 0.00427391\n","Iteration 104, loss = 0.00413544\n","Iteration 105, loss = 0.00386033\n","Iteration 106, loss = 0.00373410\n","Iteration 107, loss = 0.00355043\n","Iteration 108, loss = 0.00343666\n","Iteration 109, loss = 0.00325193\n","Iteration 110, loss = 0.00310748\n","Iteration 111, loss = 0.00295614\n","Iteration 112, loss = 0.00279795\n","Iteration 113, loss = 0.00288028\n","Iteration 114, loss = 0.00260647\n","Iteration 115, loss = 0.00257815\n","Iteration 116, loss = 0.00238602\n","Iteration 117, loss = 0.00235395\n","Iteration 118, loss = 0.00226595\n","Iteration 119, loss = 0.00216896\n","Iteration 120, loss = 0.00213551\n","Iteration 121, loss = 0.00204029\n","Iteration 122, loss = 0.00198101\n","Iteration 123, loss = 0.00189749\n","Iteration 124, loss = 0.00185796\n","Iteration 125, loss = 0.00179596\n","Iteration 126, loss = 0.00171470\n","Iteration 127, loss = 0.00166425\n","Iteration 128, loss = 0.00162839\n","Iteration 129, loss = 0.00157926\n","Iteration 130, loss = 0.00151964\n","Iteration 131, loss = 0.00147827\n","Iteration 132, loss = 0.00143885\n","Iteration 133, loss = 0.00139585\n","Iteration 134, loss = 0.00136994\n","Iteration 135, loss = 0.00130097\n","Iteration 136, loss = 0.00126979\n","Iteration 137, loss = 0.00124013\n","Iteration 138, loss = 0.00121857\n","Iteration 139, loss = 0.00119758\n","Iteration 140, loss = 0.00116685\n","Iteration 141, loss = 0.00112749\n","Iteration 142, loss = 0.00110337\n","Iteration 143, loss = 0.00109274\n","Iteration 144, loss = 0.00106117\n","Iteration 145, loss = 0.00102637\n","Iteration 146, loss = 0.00101767\n","Training loss did not improve more than tol=0.000050 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.34666591\n","Iteration 2, loss = 0.22463858\n","Iteration 3, loss = 0.21306254\n","Iteration 4, loss = 0.20803141\n","Iteration 5, loss = 0.20446143\n","Iteration 6, loss = 0.20075644\n","Iteration 7, loss = 0.19822232\n","Iteration 8, loss = 0.19617693\n","Iteration 9, loss = 0.19343936\n","Iteration 10, loss = 0.19095127\n","Iteration 11, loss = 0.18729395\n","Iteration 12, loss = 0.18597117\n","Iteration 13, loss = 0.18223440\n","Iteration 14, loss = 0.18007429\n","Iteration 15, loss = 0.17772774\n","Iteration 16, loss = 0.17543062\n","Iteration 17, loss = 0.17292901\n","Iteration 18, loss = 0.17012083\n","Iteration 19, loss = 0.16676451\n","Iteration 20, loss = 0.16415691\n","Iteration 21, loss = 0.16047700\n","Iteration 22, loss = 0.15796517\n","Iteration 23, loss = 0.15602946\n","Iteration 24, loss = 0.15230850\n","Iteration 25, loss = 0.14903398\n","Iteration 26, loss = 0.14560298\n","Iteration 27, loss = 0.14312472\n","Iteration 28, loss = 0.13933763\n","Iteration 29, loss = 0.13591429\n","Iteration 30, loss = 0.13246581\n","Iteration 31, loss = 0.13071683\n","Iteration 32, loss = 0.12653661\n","Iteration 33, loss = 0.12233195\n","Iteration 34, loss = 0.12082203\n","Iteration 35, loss = 0.11818760\n","Iteration 36, loss = 0.11379425\n","Iteration 37, loss = 0.11072818\n","Iteration 38, loss = 0.10878685\n","Iteration 39, loss = 0.10606695\n","Iteration 40, loss = 0.10028122\n","Iteration 41, loss = 0.09936856\n","Iteration 42, loss = 0.09492641\n","Iteration 43, loss = 0.09225041\n","Iteration 44, loss = 0.09050495\n","Iteration 45, loss = 0.08635686\n","Iteration 46, loss = 0.08414316\n","Iteration 47, loss = 0.08089774\n","Iteration 48, loss = 0.07828975\n","Iteration 49, loss = 0.07697718\n","Iteration 50, loss = 0.07449150\n","Iteration 51, loss = 0.07001012\n","Iteration 52, loss = 0.06661159\n","Iteration 53, loss = 0.06446726\n","Iteration 54, loss = 0.06244056\n","Iteration 55, loss = 0.05916578\n","Iteration 56, loss = 0.05755523\n","Iteration 57, loss = 0.05627394\n","Iteration 58, loss = 0.05387087\n","Iteration 59, loss = 0.05008526\n","Iteration 60, loss = 0.04833660\n","Iteration 61, loss = 0.04620118\n","Iteration 62, loss = 0.04420295\n","Iteration 63, loss = 0.04387908\n","Iteration 64, loss = 0.04040579\n","Iteration 65, loss = 0.03974242\n","Iteration 66, loss = 0.03777082\n","Iteration 67, loss = 0.03537323\n","Iteration 68, loss = 0.03441534\n","Iteration 69, loss = 0.03266056\n","Iteration 70, loss = 0.03040735\n","Iteration 71, loss = 0.02885207\n","Iteration 72, loss = 0.02768281\n","Iteration 73, loss = 0.02634637\n","Iteration 74, loss = 0.02717858\n","Iteration 75, loss = 0.02376250\n","Iteration 76, loss = 0.02309172\n","Iteration 77, loss = 0.02157051\n","Iteration 78, loss = 0.02033363\n","Iteration 79, loss = 0.02005702\n","Iteration 80, loss = 0.01874426\n","Iteration 81, loss = 0.01801372\n","Iteration 82, loss = 0.01749036\n","Iteration 83, loss = 0.01619727\n","Iteration 84, loss = 0.01540834\n","Iteration 85, loss = 0.01443411\n","Iteration 86, loss = 0.01365889\n","Iteration 87, loss = 0.01343432\n","Iteration 88, loss = 0.01225285\n","Iteration 89, loss = 0.01188236\n","Iteration 90, loss = 0.01089718\n","Iteration 91, loss = 0.01088969\n","Iteration 92, loss = 0.01016945\n","Iteration 93, loss = 0.00959620\n","Iteration 94, loss = 0.00927981\n","Iteration 95, loss = 0.00869187\n","Iteration 96, loss = 0.00855167\n","Iteration 97, loss = 0.00825301\n","Iteration 98, loss = 0.00778385\n","Iteration 99, loss = 0.00776865\n","Iteration 100, loss = 0.00677407\n","Iteration 101, loss = 0.00655283\n","Iteration 102, loss = 0.00612175\n","Iteration 103, loss = 0.00634140\n","Iteration 104, loss = 0.00585730\n","Iteration 105, loss = 0.00544312\n","Iteration 106, loss = 0.00519608\n","Iteration 107, loss = 0.00502603\n","Iteration 108, loss = 0.00504769\n","Iteration 109, loss = 0.00463774\n","Iteration 110, loss = 0.00450247\n","Iteration 111, loss = 0.00424661\n","Iteration 112, loss = 0.00411347\n","Iteration 113, loss = 0.00403580\n","Iteration 114, loss = 0.00378172\n","Iteration 115, loss = 0.00366701\n","Iteration 116, loss = 0.00343554\n","Iteration 117, loss = 0.00331965\n","Iteration 118, loss = 0.00317590\n","Iteration 119, loss = 0.00311111\n","Iteration 120, loss = 0.00295933\n","Iteration 121, loss = 0.00284480\n","Iteration 122, loss = 0.00279326\n","Iteration 123, loss = 0.00268315\n","Iteration 124, loss = 0.00262785\n","Iteration 125, loss = 0.00249478\n","Iteration 126, loss = 0.00243468\n","Iteration 127, loss = 0.00235038\n","Iteration 128, loss = 0.00226558\n","Iteration 129, loss = 0.00221814\n","Iteration 130, loss = 0.00215698\n","Iteration 131, loss = 0.00206280\n","Iteration 132, loss = 0.00197273\n","Iteration 133, loss = 0.00194573\n","Iteration 134, loss = 0.00189060\n","Iteration 135, loss = 0.00182276\n","Iteration 136, loss = 0.00175108\n","Iteration 137, loss = 0.00172465\n","Iteration 138, loss = 0.00169429\n","Iteration 139, loss = 0.00162793\n","Iteration 140, loss = 0.00156806\n","Iteration 141, loss = 0.00156873\n","Iteration 142, loss = 0.00152272\n","Iteration 143, loss = 0.00149358\n","Iteration 144, loss = 0.00143227\n","Iteration 145, loss = 0.00142061\n","Iteration 146, loss = 0.00135201\n","Iteration 147, loss = 0.00133564\n","Iteration 148, loss = 0.00130206\n","Iteration 149, loss = 0.00126861\n","Iteration 150, loss = 0.00122775\n","Iteration 1, loss = 0.36972488\n","Iteration 2, loss = 0.22884387\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.21951635\n","Iteration 4, loss = 0.21371094\n","Iteration 5, loss = 0.21031056\n","Iteration 6, loss = 0.20604215\n","Iteration 7, loss = 0.20338400\n","Iteration 8, loss = 0.20020915\n","Iteration 9, loss = 0.19730747\n","Iteration 10, loss = 0.19482444\n","Iteration 11, loss = 0.19206345\n","Iteration 12, loss = 0.18928701\n","Iteration 13, loss = 0.18693494\n","Iteration 14, loss = 0.18426824\n","Iteration 15, loss = 0.18238877\n","Iteration 16, loss = 0.17925495\n","Iteration 17, loss = 0.17649975\n","Iteration 18, loss = 0.17383681\n","Iteration 19, loss = 0.17106781\n","Iteration 20, loss = 0.16789861\n","Iteration 21, loss = 0.16568981\n","Iteration 22, loss = 0.16289944\n","Iteration 23, loss = 0.15934475\n","Iteration 24, loss = 0.15531085\n","Iteration 25, loss = 0.15349784\n","Iteration 26, loss = 0.15016281\n","Iteration 27, loss = 0.14611695\n","Iteration 28, loss = 0.14331750\n","Iteration 29, loss = 0.14055039\n","Iteration 30, loss = 0.13547691\n","Iteration 31, loss = 0.13492897\n","Iteration 32, loss = 0.12813232\n","Iteration 33, loss = 0.12436594\n","Iteration 34, loss = 0.12134361\n","Iteration 35, loss = 0.11954549\n","Iteration 36, loss = 0.11580165\n","Iteration 37, loss = 0.11033389\n","Iteration 38, loss = 0.10761304\n","Iteration 39, loss = 0.10495149\n","Iteration 40, loss = 0.10048101\n","Iteration 41, loss = 0.09771783\n","Iteration 42, loss = 0.09450641\n","Iteration 43, loss = 0.09085689\n","Iteration 44, loss = 0.08804916\n","Iteration 45, loss = 0.08464751\n","Iteration 46, loss = 0.08253533\n","Iteration 47, loss = 0.07829499\n","Iteration 48, loss = 0.07408962\n","Iteration 49, loss = 0.07254037\n","Iteration 50, loss = 0.06957790\n","Iteration 51, loss = 0.06751421\n","Iteration 52, loss = 0.06396220\n","Iteration 53, loss = 0.06411647\n","Iteration 54, loss = 0.05960109\n","Iteration 55, loss = 0.05608651\n","Iteration 56, loss = 0.05528363\n","Iteration 57, loss = 0.05176157\n","Iteration 58, loss = 0.04913525\n","Iteration 59, loss = 0.04629411\n","Iteration 60, loss = 0.04482102\n","Iteration 61, loss = 0.04225703\n","Iteration 62, loss = 0.04227356\n","Iteration 63, loss = 0.03867190\n","Iteration 64, loss = 0.03768736\n","Iteration 65, loss = 0.03617915\n","Iteration 66, loss = 0.03302861\n","Iteration 67, loss = 0.03128158\n","Iteration 68, loss = 0.02960471\n","Iteration 69, loss = 0.02891357\n","Iteration 70, loss = 0.02766766\n","Iteration 71, loss = 0.02645436\n","Iteration 72, loss = 0.02431672\n","Iteration 73, loss = 0.02332652\n","Iteration 74, loss = 0.02229536\n","Iteration 75, loss = 0.02084064\n","Iteration 76, loss = 0.01910252\n","Iteration 77, loss = 0.01781606\n","Iteration 78, loss = 0.01652370\n","Iteration 79, loss = 0.01540269\n","Iteration 80, loss = 0.01521553\n","Iteration 81, loss = 0.01494066\n","Iteration 82, loss = 0.01399307\n","Iteration 83, loss = 0.01266507\n","Iteration 84, loss = 0.01250640\n","Iteration 85, loss = 0.01203863\n","Iteration 86, loss = 0.01077951\n","Iteration 87, loss = 0.01063300\n","Iteration 88, loss = 0.00998181\n","Iteration 89, loss = 0.00911947\n","Iteration 90, loss = 0.00829688\n","Iteration 91, loss = 0.00821645\n","Iteration 92, loss = 0.00821726\n","Iteration 93, loss = 0.00749760\n","Iteration 94, loss = 0.00688095\n","Iteration 95, loss = 0.00687132\n","Iteration 96, loss = 0.00641240\n","Iteration 97, loss = 0.00598498\n","Iteration 98, loss = 0.00566599\n","Iteration 99, loss = 0.00537344\n","Iteration 100, loss = 0.00523026\n","Iteration 101, loss = 0.00495309\n","Iteration 102, loss = 0.00466003\n","Iteration 103, loss = 0.00453138\n","Iteration 104, loss = 0.00427732\n","Iteration 105, loss = 0.00409830\n","Iteration 106, loss = 0.00397580\n","Iteration 107, loss = 0.00388103\n","Iteration 108, loss = 0.00362488\n","Iteration 109, loss = 0.00349799\n","Iteration 110, loss = 0.00336165\n","Iteration 111, loss = 0.00317865\n","Iteration 112, loss = 0.00311454\n","Iteration 113, loss = 0.00300281\n","Iteration 114, loss = 0.00287938\n","Iteration 115, loss = 0.00291620\n","Iteration 116, loss = 0.00286887\n","Iteration 117, loss = 0.00268302\n","Iteration 118, loss = 0.00270267\n","Iteration 119, loss = 0.00242122\n","Iteration 120, loss = 0.00236824\n","Iteration 121, loss = 0.00230850\n","Iteration 122, loss = 0.00219722\n","Iteration 123, loss = 0.00209077\n","Iteration 124, loss = 0.00206306\n","Iteration 125, loss = 0.00203419\n","Iteration 126, loss = 0.00191730\n","Iteration 127, loss = 0.00184911\n","Iteration 128, loss = 0.00178381\n","Iteration 129, loss = 0.00171472\n","Iteration 130, loss = 0.00170216\n","Iteration 131, loss = 0.00163112\n","Iteration 132, loss = 0.00159825\n","Iteration 133, loss = 0.00154278\n","Iteration 134, loss = 0.00150598\n","Iteration 135, loss = 0.00147807\n","Iteration 136, loss = 0.00143682\n","Iteration 137, loss = 0.00139902\n","Iteration 138, loss = 0.00136826\n","Iteration 139, loss = 0.00132232\n","Iteration 140, loss = 0.00127526\n","Iteration 141, loss = 0.00127539\n","Iteration 142, loss = 0.00122642\n","Iteration 143, loss = 0.00120248\n","Iteration 144, loss = 0.00117834\n","Training loss did not improve more than tol=0.000050 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.36041019\n","Iteration 2, loss = 0.23033388\n","Iteration 3, loss = 0.21768314\n","Iteration 4, loss = 0.21266371\n","Iteration 5, loss = 0.20914025\n","Iteration 6, loss = 0.20600993\n","Iteration 7, loss = 0.20203286\n","Iteration 8, loss = 0.20019157\n","Iteration 9, loss = 0.19625753\n","Iteration 10, loss = 0.19332499\n","Iteration 11, loss = 0.19111983\n","Iteration 12, loss = 0.18846914\n","Iteration 13, loss = 0.18547490\n","Iteration 14, loss = 0.18335080\n","Iteration 15, loss = 0.18049958\n","Iteration 16, loss = 0.17931227\n","Iteration 17, loss = 0.17483155\n","Iteration 18, loss = 0.17184624\n","Iteration 19, loss = 0.16823936\n","Iteration 20, loss = 0.16531777\n","Iteration 21, loss = 0.16332559\n","Iteration 22, loss = 0.15918521\n","Iteration 23, loss = 0.15651744\n","Iteration 24, loss = 0.15329747\n","Iteration 25, loss = 0.14980210\n","Iteration 26, loss = 0.14647297\n","Iteration 27, loss = 0.14323519\n","Iteration 28, loss = 0.14026908\n","Iteration 29, loss = 0.13594440\n","Iteration 30, loss = 0.13320039\n","Iteration 31, loss = 0.13013779\n","Iteration 32, loss = 0.12679267\n","Iteration 33, loss = 0.12356345\n","Iteration 34, loss = 0.11908551\n","Iteration 35, loss = 0.11826817\n","Iteration 36, loss = 0.11272040\n","Iteration 37, loss = 0.11011296\n","Iteration 38, loss = 0.10664569\n","Iteration 39, loss = 0.10363532\n","Iteration 40, loss = 0.10067614\n","Iteration 41, loss = 0.09665712\n","Iteration 42, loss = 0.09393202\n","Iteration 43, loss = 0.09336039\n","Iteration 44, loss = 0.08889681\n","Iteration 45, loss = 0.08509670\n","Iteration 46, loss = 0.08327098\n","Iteration 47, loss = 0.08044918\n","Iteration 48, loss = 0.07731016\n","Iteration 49, loss = 0.07568246\n","Iteration 50, loss = 0.07224292\n","Iteration 51, loss = 0.06886433\n","Iteration 52, loss = 0.06675992\n","Iteration 53, loss = 0.06520880\n","Iteration 54, loss = 0.06180122\n","Iteration 55, loss = 0.05968160\n","Iteration 56, loss = 0.05712505\n","Iteration 57, loss = 0.05400845\n","Iteration 58, loss = 0.05264347\n","Iteration 59, loss = 0.05096168\n","Iteration 60, loss = 0.04850738\n","Iteration 61, loss = 0.04633531\n","Iteration 62, loss = 0.04325668\n","Iteration 63, loss = 0.04290082\n","Iteration 64, loss = 0.03973978\n","Iteration 65, loss = 0.03859806\n","Iteration 66, loss = 0.03686046\n","Iteration 67, loss = 0.03467211\n","Iteration 68, loss = 0.03429770\n","Iteration 69, loss = 0.03142643\n","Iteration 70, loss = 0.02976959\n","Iteration 71, loss = 0.02970942\n","Iteration 72, loss = 0.02806542\n","Iteration 73, loss = 0.02608239\n","Iteration 74, loss = 0.02536999\n","Iteration 75, loss = 0.02394326\n","Iteration 76, loss = 0.02212378\n","Iteration 77, loss = 0.02082102\n","Iteration 78, loss = 0.01972736\n","Iteration 79, loss = 0.01921388\n","Iteration 80, loss = 0.01802160\n","Iteration 81, loss = 0.01692462\n","Iteration 82, loss = 0.01665241\n","Iteration 83, loss = 0.01500618\n","Iteration 84, loss = 0.01532012\n","Iteration 85, loss = 0.01425666\n","Iteration 86, loss = 0.01276348\n","Iteration 87, loss = 0.01274360\n","Iteration 88, loss = 0.01179972\n","Iteration 89, loss = 0.01100186\n","Iteration 90, loss = 0.01069804\n","Iteration 91, loss = 0.00991003\n","Iteration 92, loss = 0.00972947\n","Iteration 93, loss = 0.00930710\n","Iteration 94, loss = 0.00868123\n","Iteration 95, loss = 0.00827606\n","Iteration 96, loss = 0.00779460\n","Iteration 97, loss = 0.00754351\n","Iteration 98, loss = 0.00716104\n","Iteration 99, loss = 0.00679328\n","Iteration 100, loss = 0.00657171\n","Iteration 101, loss = 0.00629043\n","Iteration 102, loss = 0.00583193\n","Iteration 103, loss = 0.00559600\n","Iteration 104, loss = 0.00549042\n","Iteration 105, loss = 0.00541846\n","Iteration 106, loss = 0.00508889\n","Iteration 107, loss = 0.00478919\n","Iteration 108, loss = 0.00453677\n","Iteration 109, loss = 0.00442238\n","Iteration 110, loss = 0.00416829\n","Iteration 111, loss = 0.00394697\n","Iteration 112, loss = 0.00398909\n","Iteration 113, loss = 0.00373439\n","Iteration 114, loss = 0.00348614\n","Iteration 115, loss = 0.00338790\n","Iteration 116, loss = 0.00327693\n","Iteration 117, loss = 0.00326834\n","Iteration 118, loss = 0.00309899\n","Iteration 119, loss = 0.00287428\n","Iteration 120, loss = 0.00283131\n","Iteration 121, loss = 0.00263810\n","Iteration 122, loss = 0.00260286\n","Iteration 123, loss = 0.00252326\n","Iteration 124, loss = 0.00242839\n","Iteration 125, loss = 0.00231923\n","Iteration 126, loss = 0.00225640\n","Iteration 127, loss = 0.00220223\n","Iteration 128, loss = 0.00213462\n","Iteration 129, loss = 0.00208921\n","Iteration 130, loss = 0.00198930\n","Iteration 131, loss = 0.00193667\n","Iteration 132, loss = 0.00187826\n","Iteration 133, loss = 0.00184195\n","Iteration 134, loss = 0.00174276\n","Iteration 135, loss = 0.00169492\n","Iteration 136, loss = 0.00168586\n","Iteration 137, loss = 0.00163507\n","Iteration 138, loss = 0.00155885\n","Iteration 139, loss = 0.00151556\n","Iteration 140, loss = 0.00150034\n","Iteration 141, loss = 0.00146089\n","Iteration 142, loss = 0.00141245\n","Iteration 143, loss = 0.00137172\n","Iteration 144, loss = 0.00136294\n","Iteration 145, loss = 0.00129726\n","Iteration 146, loss = 0.00126748\n","Iteration 147, loss = 0.00125985\n","Iteration 148, loss = 0.00123796\n","Iteration 149, loss = 0.00121040\n","Iteration 150, loss = 0.00117077\n","Iteration 1, loss = 0.69585075\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.49754183\n","Iteration 3, loss = 0.40292470\n","Iteration 4, loss = 0.35404049\n","Iteration 5, loss = 0.32432562\n","Iteration 6, loss = 0.30460225\n","Iteration 7, loss = 0.29060842\n","Iteration 8, loss = 0.28035011\n","Iteration 9, loss = 0.27243288\n","Iteration 10, loss = 0.26624343\n","Iteration 11, loss = 0.26127683\n","Iteration 12, loss = 0.25720060\n","Iteration 13, loss = 0.25372732\n","Iteration 14, loss = 0.25077928\n","Iteration 15, loss = 0.24817338\n","Iteration 16, loss = 0.24587943\n","Iteration 17, loss = 0.24387390\n","Iteration 18, loss = 0.24207947\n","Iteration 19, loss = 0.24048964\n","Iteration 20, loss = 0.23899066\n","Iteration 21, loss = 0.23769546\n","Iteration 22, loss = 0.23647761\n","Iteration 23, loss = 0.23538501\n","Iteration 24, loss = 0.23441697\n","Iteration 25, loss = 0.23350038\n","Iteration 26, loss = 0.23261816\n","Iteration 27, loss = 0.23183092\n","Iteration 28, loss = 0.23107159\n","Iteration 29, loss = 0.23040395\n","Iteration 30, loss = 0.22971714\n","Iteration 31, loss = 0.22906700\n","Iteration 32, loss = 0.22849738\n","Iteration 33, loss = 0.22790907\n","Iteration 34, loss = 0.22738701\n","Iteration 35, loss = 0.22690142\n","Iteration 36, loss = 0.22640518\n","Iteration 37, loss = 0.22593909\n","Iteration 38, loss = 0.22549957\n","Iteration 39, loss = 0.22507898\n","Iteration 40, loss = 0.22467527\n","Iteration 41, loss = 0.22428167\n","Iteration 42, loss = 0.22388270\n","Iteration 43, loss = 0.22347802\n","Iteration 44, loss = 0.22307863\n","Iteration 45, loss = 0.22274276\n","Iteration 46, loss = 0.22241928\n","Iteration 47, loss = 0.22206207\n","Iteration 48, loss = 0.22171080\n","Iteration 49, loss = 0.22141761\n","Iteration 50, loss = 0.22108819\n","Iteration 51, loss = 0.22077598\n","Iteration 52, loss = 0.22047362\n","Iteration 53, loss = 0.22019233\n","Iteration 54, loss = 0.21989607\n","Iteration 55, loss = 0.21961724\n","Iteration 56, loss = 0.21934569\n","Iteration 57, loss = 0.21902972\n","Iteration 58, loss = 0.21875833\n","Iteration 59, loss = 0.21848591\n","Iteration 60, loss = 0.21819206\n","Iteration 61, loss = 0.21797708\n","Iteration 62, loss = 0.21771207\n","Iteration 63, loss = 0.21745279\n","Iteration 64, loss = 0.21722933\n","Iteration 65, loss = 0.21700776\n","Iteration 66, loss = 0.21675015\n","Iteration 67, loss = 0.21646549\n","Iteration 68, loss = 0.21625359\n","Iteration 69, loss = 0.21601486\n","Iteration 70, loss = 0.21583142\n","Iteration 71, loss = 0.21553851\n","Iteration 72, loss = 0.21530067\n","Iteration 73, loss = 0.21510143\n","Iteration 74, loss = 0.21492198\n","Iteration 75, loss = 0.21469320\n","Iteration 76, loss = 0.21444641\n","Iteration 77, loss = 0.21426107\n","Iteration 78, loss = 0.21405511\n","Iteration 79, loss = 0.21388467\n","Iteration 80, loss = 0.21361941\n","Iteration 81, loss = 0.21342579\n","Iteration 82, loss = 0.21321972\n","Iteration 83, loss = 0.21303403\n","Iteration 84, loss = 0.21279478\n","Iteration 85, loss = 0.21264306\n","Iteration 86, loss = 0.21246192\n","Iteration 87, loss = 0.21224722\n","Iteration 88, loss = 0.21205638\n","Iteration 89, loss = 0.21188722\n","Iteration 90, loss = 0.21167467\n","Iteration 91, loss = 0.21151003\n","Iteration 92, loss = 0.21129755\n","Iteration 93, loss = 0.21116177\n","Iteration 94, loss = 0.21096686\n","Iteration 95, loss = 0.21074596\n","Iteration 96, loss = 0.21060289\n","Iteration 97, loss = 0.21040773\n","Iteration 98, loss = 0.21023595\n","Iteration 99, loss = 0.21007129\n","Iteration 100, loss = 0.20990348\n","Iteration 101, loss = 0.20968885\n","Iteration 102, loss = 0.20960710\n","Iteration 103, loss = 0.20939018\n","Iteration 104, loss = 0.20919774\n","Iteration 105, loss = 0.20904202\n","Iteration 106, loss = 0.20895082\n","Iteration 107, loss = 0.20871839\n","Iteration 108, loss = 0.20856867\n","Iteration 109, loss = 0.20839466\n","Iteration 110, loss = 0.20826966\n","Iteration 111, loss = 0.20810010\n","Iteration 112, loss = 0.20788125\n","Iteration 113, loss = 0.20774089\n","Iteration 114, loss = 0.20761995\n","Iteration 115, loss = 0.20749715\n","Iteration 116, loss = 0.20728056\n","Iteration 117, loss = 0.20712801\n","Iteration 118, loss = 0.20691955\n","Iteration 119, loss = 0.20682158\n","Iteration 120, loss = 0.20665250\n","Iteration 121, loss = 0.20649571\n","Iteration 122, loss = 0.20636145\n","Iteration 123, loss = 0.20621419\n","Iteration 124, loss = 0.20610399\n","Iteration 125, loss = 0.20587036\n","Iteration 126, loss = 0.20573157\n","Iteration 127, loss = 0.20559153\n","Iteration 128, loss = 0.20545061\n","Iteration 129, loss = 0.20534517\n","Iteration 130, loss = 0.20515604\n","Iteration 131, loss = 0.20500815\n","Iteration 132, loss = 0.20488015\n","Iteration 133, loss = 0.20471211\n","Iteration 134, loss = 0.20456942\n","Iteration 135, loss = 0.20442889\n","Iteration 136, loss = 0.20431127\n","Iteration 137, loss = 0.20412245\n","Iteration 138, loss = 0.20400914\n","Iteration 139, loss = 0.20388447\n","Iteration 140, loss = 0.20369422\n","Iteration 141, loss = 0.20358798\n","Iteration 142, loss = 0.20340478\n","Iteration 143, loss = 0.20331241\n","Iteration 144, loss = 0.20311067\n","Iteration 145, loss = 0.20300912\n","Iteration 146, loss = 0.20288053\n","Iteration 147, loss = 0.20269545\n","Iteration 148, loss = 0.20259923\n","Iteration 149, loss = 0.20246626\n","Iteration 150, loss = 0.20230573\n","Iteration 1, loss = 0.63654322\n","Iteration 2, loss = 0.53822525\n","Iteration 3, loss = 0.46503748\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.41408830\n","Iteration 5, loss = 0.37758207\n","Iteration 6, loss = 0.35087343\n","Iteration 7, loss = 0.33053785\n","Iteration 8, loss = 0.31488536\n","Iteration 9, loss = 0.30240676\n","Iteration 10, loss = 0.29230996\n","Iteration 11, loss = 0.28413597\n","Iteration 12, loss = 0.27736400\n","Iteration 13, loss = 0.27167306\n","Iteration 14, loss = 0.26685866\n","Iteration 15, loss = 0.26272469\n","Iteration 16, loss = 0.25921480\n","Iteration 17, loss = 0.25607933\n","Iteration 18, loss = 0.25331661\n","Iteration 19, loss = 0.25086693\n","Iteration 20, loss = 0.24859383\n","Iteration 21, loss = 0.24653972\n","Iteration 22, loss = 0.24468515\n","Iteration 23, loss = 0.24294355\n","Iteration 24, loss = 0.24131679\n","Iteration 25, loss = 0.23983491\n","Iteration 26, loss = 0.23845360\n","Iteration 27, loss = 0.23715971\n","Iteration 28, loss = 0.23594965\n","Iteration 29, loss = 0.23480828\n","Iteration 30, loss = 0.23377360\n","Iteration 31, loss = 0.23276068\n","Iteration 32, loss = 0.23182930\n","Iteration 33, loss = 0.23093866\n","Iteration 34, loss = 0.23011618\n","Iteration 35, loss = 0.22932734\n","Iteration 36, loss = 0.22858294\n","Iteration 37, loss = 0.22781205\n","Iteration 38, loss = 0.22710208\n","Iteration 39, loss = 0.22649059\n","Iteration 40, loss = 0.22584335\n","Iteration 41, loss = 0.22525550\n","Iteration 42, loss = 0.22465148\n","Iteration 43, loss = 0.22414543\n","Iteration 44, loss = 0.22355605\n","Iteration 45, loss = 0.22303880\n","Iteration 46, loss = 0.22255575\n","Iteration 47, loss = 0.22210376\n","Iteration 48, loss = 0.22166773\n","Iteration 49, loss = 0.22119122\n","Iteration 50, loss = 0.22074115\n","Iteration 51, loss = 0.22034818\n","Iteration 52, loss = 0.21991885\n","Iteration 53, loss = 0.21955247\n","Iteration 54, loss = 0.21919311\n","Iteration 55, loss = 0.21883356\n","Iteration 56, loss = 0.21843603\n","Iteration 57, loss = 0.21809345\n","Iteration 58, loss = 0.21775559\n","Iteration 59, loss = 0.21739797\n","Iteration 60, loss = 0.21706580\n","Iteration 61, loss = 0.21674718\n","Iteration 62, loss = 0.21643596\n","Iteration 63, loss = 0.21610466\n","Iteration 64, loss = 0.21579976\n","Iteration 65, loss = 0.21550803\n","Iteration 66, loss = 0.21522871\n","Iteration 67, loss = 0.21491773\n","Iteration 68, loss = 0.21465004\n","Iteration 69, loss = 0.21436553\n","Iteration 70, loss = 0.21411453\n","Iteration 71, loss = 0.21382594\n","Iteration 72, loss = 0.21359747\n","Iteration 73, loss = 0.21329372\n","Iteration 74, loss = 0.21307175\n","Iteration 75, loss = 0.21283663\n","Iteration 76, loss = 0.21254536\n","Iteration 77, loss = 0.21228139\n","Iteration 78, loss = 0.21207689\n","Iteration 79, loss = 0.21180131\n","Iteration 80, loss = 0.21159045\n","Iteration 81, loss = 0.21134661\n","Iteration 82, loss = 0.21111594\n","Iteration 83, loss = 0.21089638\n","Iteration 84, loss = 0.21066821\n","Iteration 85, loss = 0.21044198\n","Iteration 86, loss = 0.21023327\n","Iteration 87, loss = 0.21001141\n","Iteration 88, loss = 0.20978497\n","Iteration 89, loss = 0.20956181\n","Iteration 90, loss = 0.20937192\n","Iteration 91, loss = 0.20916111\n","Iteration 92, loss = 0.20897604\n","Iteration 93, loss = 0.20872010\n","Iteration 94, loss = 0.20850843\n","Iteration 95, loss = 0.20831781\n","Iteration 96, loss = 0.20810218\n","Iteration 97, loss = 0.20790535\n","Iteration 98, loss = 0.20769224\n","Iteration 99, loss = 0.20749329\n","Iteration 100, loss = 0.20727541\n","Iteration 101, loss = 0.20713756\n","Iteration 102, loss = 0.20692281\n","Iteration 103, loss = 0.20672997\n","Iteration 104, loss = 0.20649434\n","Iteration 105, loss = 0.20630337\n","Iteration 106, loss = 0.20612392\n","Iteration 107, loss = 0.20590754\n","Iteration 108, loss = 0.20573546\n","Iteration 109, loss = 0.20560540\n","Iteration 110, loss = 0.20545787\n","Iteration 111, loss = 0.20523861\n","Iteration 112, loss = 0.20497125\n","Iteration 113, loss = 0.20480890\n","Iteration 114, loss = 0.20463774\n","Iteration 115, loss = 0.20444976\n","Iteration 116, loss = 0.20426804\n","Iteration 117, loss = 0.20405732\n","Iteration 118, loss = 0.20390638\n","Iteration 119, loss = 0.20369594\n","Iteration 120, loss = 0.20354189\n","Iteration 121, loss = 0.20338981\n","Iteration 122, loss = 0.20320684\n","Iteration 123, loss = 0.20299443\n","Iteration 124, loss = 0.20281368\n","Iteration 125, loss = 0.20266498\n","Iteration 126, loss = 0.20249116\n","Iteration 127, loss = 0.20234363\n","Iteration 128, loss = 0.20215224\n","Iteration 129, loss = 0.20196112\n","Iteration 130, loss = 0.20178080\n","Iteration 131, loss = 0.20164729\n","Iteration 132, loss = 0.20147063\n","Iteration 133, loss = 0.20129234\n","Iteration 134, loss = 0.20112577\n","Iteration 135, loss = 0.20096805\n","Iteration 136, loss = 0.20077941\n","Iteration 137, loss = 0.20066488\n","Iteration 138, loss = 0.20046348\n","Iteration 139, loss = 0.20024405\n","Iteration 140, loss = 0.20015008\n","Iteration 141, loss = 0.19995441\n","Iteration 142, loss = 0.19979724\n","Iteration 143, loss = 0.19965461\n","Iteration 144, loss = 0.19945756\n","Iteration 145, loss = 0.19931248\n","Iteration 146, loss = 0.19913733\n","Iteration 147, loss = 0.19894505\n","Iteration 148, loss = 0.19883104\n","Iteration 149, loss = 0.19865118\n","Iteration 150, loss = 0.19851533\n","Iteration 1, loss = 0.68294537\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.44208460\n","Iteration 3, loss = 0.36101001\n","Iteration 4, loss = 0.32119336\n","Iteration 5, loss = 0.29716764\n","Iteration 6, loss = 0.28170352\n","Iteration 7, loss = 0.27099679\n","Iteration 8, loss = 0.26320038\n","Iteration 9, loss = 0.25737914\n","Iteration 10, loss = 0.25279311\n","Iteration 11, loss = 0.24914070\n","Iteration 12, loss = 0.24609275\n","Iteration 13, loss = 0.24346368\n","Iteration 14, loss = 0.24119886\n","Iteration 15, loss = 0.23923714\n","Iteration 16, loss = 0.23748706\n","Iteration 17, loss = 0.23588999\n","Iteration 18, loss = 0.23449140\n","Iteration 19, loss = 0.23319040\n","Iteration 20, loss = 0.23199013\n","Iteration 21, loss = 0.23090030\n","Iteration 22, loss = 0.22990211\n","Iteration 23, loss = 0.22896344\n","Iteration 24, loss = 0.22806315\n","Iteration 25, loss = 0.22724227\n","Iteration 26, loss = 0.22650428\n","Iteration 27, loss = 0.22574997\n","Iteration 28, loss = 0.22506895\n","Iteration 29, loss = 0.22440687\n","Iteration 30, loss = 0.22383994\n","Iteration 31, loss = 0.22323953\n","Iteration 32, loss = 0.22263068\n","Iteration 33, loss = 0.22210530\n","Iteration 34, loss = 0.22161064\n","Iteration 35, loss = 0.22110761\n","Iteration 36, loss = 0.22063638\n","Iteration 37, loss = 0.22014301\n","Iteration 38, loss = 0.21972191\n","Iteration 39, loss = 0.21924375\n","Iteration 40, loss = 0.21883305\n","Iteration 41, loss = 0.21840080\n","Iteration 42, loss = 0.21801246\n","Iteration 43, loss = 0.21762479\n","Iteration 44, loss = 0.21727982\n","Iteration 45, loss = 0.21686415\n","Iteration 46, loss = 0.21650769\n","Iteration 47, loss = 0.21615151\n","Iteration 48, loss = 0.21579918\n","Iteration 49, loss = 0.21547064\n","Iteration 50, loss = 0.21512485\n","Iteration 51, loss = 0.21480372\n","Iteration 52, loss = 0.21447681\n","Iteration 53, loss = 0.21420925\n","Iteration 54, loss = 0.21385847\n","Iteration 55, loss = 0.21362136\n","Iteration 56, loss = 0.21327165\n","Iteration 57, loss = 0.21300148\n","Iteration 58, loss = 0.21270728\n","Iteration 59, loss = 0.21244165\n","Iteration 60, loss = 0.21212825\n","Iteration 61, loss = 0.21186291\n","Iteration 62, loss = 0.21159082\n","Iteration 63, loss = 0.21133870\n","Iteration 64, loss = 0.21108117\n","Iteration 65, loss = 0.21079451\n","Iteration 66, loss = 0.21055612\n","Iteration 67, loss = 0.21027499\n","Iteration 68, loss = 0.21004202\n","Iteration 69, loss = 0.20977943\n","Iteration 70, loss = 0.20955268\n","Iteration 71, loss = 0.20936288\n","Iteration 72, loss = 0.20908015\n","Iteration 73, loss = 0.20886185\n","Iteration 74, loss = 0.20865640\n","Iteration 75, loss = 0.20839307\n","Iteration 76, loss = 0.20819135\n","Iteration 77, loss = 0.20796571\n","Iteration 78, loss = 0.20777492\n","Iteration 79, loss = 0.20748555\n","Iteration 80, loss = 0.20724565\n","Iteration 81, loss = 0.20703266\n","Iteration 82, loss = 0.20683347\n","Iteration 83, loss = 0.20660701\n","Iteration 84, loss = 0.20642802\n","Iteration 85, loss = 0.20621337\n","Iteration 86, loss = 0.20601729\n","Iteration 87, loss = 0.20577718\n","Iteration 88, loss = 0.20558623\n","Iteration 89, loss = 0.20539848\n","Iteration 90, loss = 0.20520236\n","Iteration 91, loss = 0.20502021\n","Iteration 92, loss = 0.20481502\n","Iteration 93, loss = 0.20462656\n","Iteration 94, loss = 0.20442314\n","Iteration 95, loss = 0.20424839\n","Iteration 96, loss = 0.20403439\n","Iteration 97, loss = 0.20384212\n","Iteration 98, loss = 0.20369752\n","Iteration 99, loss = 0.20352682\n","Iteration 100, loss = 0.20330273\n","Iteration 101, loss = 0.20313907\n","Iteration 102, loss = 0.20297307\n","Iteration 103, loss = 0.20281098\n","Iteration 104, loss = 0.20259885\n","Iteration 105, loss = 0.20242055\n","Iteration 106, loss = 0.20227712\n","Iteration 107, loss = 0.20211878\n","Iteration 108, loss = 0.20190173\n","Iteration 109, loss = 0.20176583\n","Iteration 110, loss = 0.20155339\n","Iteration 111, loss = 0.20143690\n","Iteration 112, loss = 0.20126192\n","Iteration 113, loss = 0.20107789\n","Iteration 114, loss = 0.20089177\n","Iteration 115, loss = 0.20076753\n","Iteration 116, loss = 0.20056459\n","Iteration 117, loss = 0.20042223\n","Iteration 118, loss = 0.20025119\n","Iteration 119, loss = 0.20009833\n","Iteration 120, loss = 0.19994331\n","Iteration 121, loss = 0.19985092\n","Iteration 122, loss = 0.19964451\n","Iteration 123, loss = 0.19944610\n","Iteration 124, loss = 0.19931132\n","Iteration 125, loss = 0.19915550\n","Iteration 126, loss = 0.19899673\n","Iteration 127, loss = 0.19885876\n","Iteration 128, loss = 0.19870806\n","Iteration 129, loss = 0.19851178\n","Iteration 130, loss = 0.19839565\n","Iteration 131, loss = 0.19825583\n","Iteration 132, loss = 0.19808414\n","Iteration 133, loss = 0.19793078\n","Iteration 134, loss = 0.19781245\n","Iteration 135, loss = 0.19765693\n","Iteration 136, loss = 0.19752656\n","Iteration 137, loss = 0.19734911\n","Iteration 138, loss = 0.19721406\n","Iteration 139, loss = 0.19712999\n","Iteration 140, loss = 0.19693151\n","Iteration 141, loss = 0.19677965\n","Iteration 142, loss = 0.19663534\n","Iteration 143, loss = 0.19652724\n","Iteration 144, loss = 0.19637102\n","Iteration 145, loss = 0.19622465\n","Iteration 146, loss = 0.19611031\n","Iteration 147, loss = 0.19595286\n","Iteration 148, loss = 0.19584966\n","Iteration 149, loss = 0.19572219\n","Iteration 150, loss = 0.19553635\n","Iteration 1, loss = 0.71567146\n","Iteration 2, loss = 0.57784240\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.48663927\n","Iteration 4, loss = 0.42430603\n","Iteration 5, loss = 0.37945871\n","Iteration 6, loss = 0.34717984\n","Iteration 7, loss = 0.32350149\n","Iteration 8, loss = 0.30608195\n","Iteration 9, loss = 0.29299271\n","Iteration 10, loss = 0.28301007\n","Iteration 11, loss = 0.27511295\n","Iteration 12, loss = 0.26875484\n","Iteration 13, loss = 0.26359229\n","Iteration 14, loss = 0.25917686\n","Iteration 15, loss = 0.25545586\n","Iteration 16, loss = 0.25223922\n","Iteration 17, loss = 0.24934438\n","Iteration 18, loss = 0.24679853\n","Iteration 19, loss = 0.24451935\n","Iteration 20, loss = 0.24251673\n","Iteration 21, loss = 0.24066615\n","Iteration 22, loss = 0.23902111\n","Iteration 23, loss = 0.23754172\n","Iteration 24, loss = 0.23608662\n","Iteration 25, loss = 0.23480575\n","Iteration 26, loss = 0.23368602\n","Iteration 27, loss = 0.23259141\n","Iteration 28, loss = 0.23154868\n","Iteration 29, loss = 0.23064963\n","Iteration 30, loss = 0.22974102\n","Iteration 31, loss = 0.22894600\n","Iteration 32, loss = 0.22821884\n","Iteration 33, loss = 0.22748098\n","Iteration 34, loss = 0.22681058\n","Iteration 35, loss = 0.22618778\n","Iteration 36, loss = 0.22558241\n","Iteration 37, loss = 0.22500429\n","Iteration 38, loss = 0.22447428\n","Iteration 39, loss = 0.22393451\n","Iteration 40, loss = 0.22348956\n","Iteration 41, loss = 0.22296109\n","Iteration 42, loss = 0.22250386\n","Iteration 43, loss = 0.22207992\n","Iteration 44, loss = 0.22164714\n","Iteration 45, loss = 0.22121395\n","Iteration 46, loss = 0.22084323\n","Iteration 47, loss = 0.22044595\n","Iteration 48, loss = 0.22006870\n","Iteration 49, loss = 0.21968527\n","Iteration 50, loss = 0.21932085\n","Iteration 51, loss = 0.21898358\n","Iteration 52, loss = 0.21864383\n","Iteration 53, loss = 0.21830551\n","Iteration 54, loss = 0.21798611\n","Iteration 55, loss = 0.21763439\n","Iteration 56, loss = 0.21732992\n","Iteration 57, loss = 0.21704527\n","Iteration 58, loss = 0.21670191\n","Iteration 59, loss = 0.21638711\n","Iteration 60, loss = 0.21612878\n","Iteration 61, loss = 0.21584259\n","Iteration 62, loss = 0.21557547\n","Iteration 63, loss = 0.21523679\n","Iteration 64, loss = 0.21497352\n","Iteration 65, loss = 0.21469013\n","Iteration 66, loss = 0.21446346\n","Iteration 67, loss = 0.21417034\n","Iteration 68, loss = 0.21389582\n","Iteration 69, loss = 0.21370199\n","Iteration 70, loss = 0.21339647\n","Iteration 71, loss = 0.21312993\n","Iteration 72, loss = 0.21284718\n","Iteration 73, loss = 0.21262006\n","Iteration 74, loss = 0.21235314\n","Iteration 75, loss = 0.21210353\n","Iteration 76, loss = 0.21186880\n","Iteration 77, loss = 0.21159663\n","Iteration 78, loss = 0.21138840\n","Iteration 79, loss = 0.21117796\n","Iteration 80, loss = 0.21091784\n","Iteration 81, loss = 0.21069089\n","Iteration 82, loss = 0.21050633\n","Iteration 83, loss = 0.21022602\n","Iteration 84, loss = 0.21000755\n","Iteration 85, loss = 0.20978157\n","Iteration 86, loss = 0.20957219\n","Iteration 87, loss = 0.20936707\n","Iteration 88, loss = 0.20909651\n","Iteration 89, loss = 0.20893427\n","Iteration 90, loss = 0.20870938\n","Iteration 91, loss = 0.20849745\n","Iteration 92, loss = 0.20832484\n","Iteration 93, loss = 0.20802128\n","Iteration 94, loss = 0.20785243\n","Iteration 95, loss = 0.20766411\n","Iteration 96, loss = 0.20742925\n","Iteration 97, loss = 0.20725179\n","Iteration 98, loss = 0.20703060\n","Iteration 99, loss = 0.20684308\n","Iteration 100, loss = 0.20667261\n","Iteration 101, loss = 0.20644677\n","Iteration 102, loss = 0.20622647\n","Iteration 103, loss = 0.20604316\n","Iteration 104, loss = 0.20584248\n","Iteration 105, loss = 0.20569494\n","Iteration 106, loss = 0.20547405\n","Iteration 107, loss = 0.20526575\n","Iteration 108, loss = 0.20506851\n","Iteration 109, loss = 0.20489188\n","Iteration 110, loss = 0.20466633\n","Iteration 111, loss = 0.20450873\n","Iteration 112, loss = 0.20434797\n","Iteration 113, loss = 0.20414177\n","Iteration 114, loss = 0.20399407\n","Iteration 115, loss = 0.20377898\n","Iteration 116, loss = 0.20363501\n","Iteration 117, loss = 0.20341805\n","Iteration 118, loss = 0.20324184\n","Iteration 119, loss = 0.20306866\n","Iteration 120, loss = 0.20291306\n","Iteration 121, loss = 0.20271744\n","Iteration 122, loss = 0.20252196\n","Iteration 123, loss = 0.20238040\n","Iteration 124, loss = 0.20224242\n","Iteration 125, loss = 0.20206891\n","Iteration 126, loss = 0.20189571\n","Iteration 127, loss = 0.20171684\n","Iteration 128, loss = 0.20158224\n","Iteration 129, loss = 0.20136236\n","Iteration 130, loss = 0.20117638\n","Iteration 131, loss = 0.20103953\n","Iteration 132, loss = 0.20086517\n","Iteration 133, loss = 0.20073068\n","Iteration 134, loss = 0.20055931\n","Iteration 135, loss = 0.20035025\n","Iteration 136, loss = 0.20020932\n","Iteration 137, loss = 0.20007977\n","Iteration 138, loss = 0.19993688\n","Iteration 139, loss = 0.19973479\n","Iteration 140, loss = 0.19963174\n","Iteration 141, loss = 0.19943765\n","Iteration 142, loss = 0.19925361\n","Iteration 143, loss = 0.19910522\n","Iteration 144, loss = 0.19897082\n","Iteration 145, loss = 0.19882718\n","Iteration 146, loss = 0.19864707\n","Iteration 147, loss = 0.19846508\n","Iteration 148, loss = 0.19838118\n","Iteration 149, loss = 0.19816797\n","Iteration 150, loss = 0.19803116\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.63888830\n","Iteration 2, loss = 0.45221970\n","Iteration 3, loss = 0.36972420\n","Iteration 4, loss = 0.32696773\n","Iteration 5, loss = 0.30129929\n","Iteration 6, loss = 0.28504297\n","Iteration 7, loss = 0.27398123\n","Iteration 8, loss = 0.26614868\n","Iteration 9, loss = 0.26028136\n","Iteration 10, loss = 0.25561422\n","Iteration 11, loss = 0.25186232\n","Iteration 12, loss = 0.24863127\n","Iteration 13, loss = 0.24586428\n","Iteration 14, loss = 0.24344234\n","Iteration 15, loss = 0.24129787\n","Iteration 16, loss = 0.23933582\n","Iteration 17, loss = 0.23759568\n","Iteration 18, loss = 0.23600381\n","Iteration 19, loss = 0.23452471\n","Iteration 20, loss = 0.23322231\n","Iteration 21, loss = 0.23198826\n","Iteration 22, loss = 0.23086296\n","Iteration 23, loss = 0.22984194\n","Iteration 24, loss = 0.22886476\n","Iteration 25, loss = 0.22796860\n","Iteration 26, loss = 0.22712572\n","Iteration 27, loss = 0.22636876\n","Iteration 28, loss = 0.22562820\n","Iteration 29, loss = 0.22497664\n","Iteration 30, loss = 0.22431156\n","Iteration 31, loss = 0.22370378\n","Iteration 32, loss = 0.22313714\n","Iteration 33, loss = 0.22259924\n","Iteration 34, loss = 0.22211341\n","Iteration 35, loss = 0.22156919\n","Iteration 36, loss = 0.22111860\n","Iteration 37, loss = 0.22064317\n","Iteration 38, loss = 0.22021340\n","Iteration 39, loss = 0.21981030\n","Iteration 40, loss = 0.21940322\n","Iteration 41, loss = 0.21898305\n","Iteration 42, loss = 0.21859378\n","Iteration 43, loss = 0.21821228\n","Iteration 44, loss = 0.21785945\n","Iteration 45, loss = 0.21750950\n","Iteration 46, loss = 0.21713078\n","Iteration 47, loss = 0.21683523\n","Iteration 48, loss = 0.21643686\n","Iteration 49, loss = 0.21610956\n","Iteration 50, loss = 0.21576794\n","Iteration 51, loss = 0.21546505\n","Iteration 52, loss = 0.21518490\n","Iteration 53, loss = 0.21485613\n","Iteration 54, loss = 0.21454630\n","Iteration 55, loss = 0.21421587\n","Iteration 56, loss = 0.21397346\n","Iteration 57, loss = 0.21364374\n","Iteration 58, loss = 0.21336831\n","Iteration 59, loss = 0.21311984\n","Iteration 60, loss = 0.21287226\n","Iteration 61, loss = 0.21253207\n","Iteration 62, loss = 0.21230360\n","Iteration 63, loss = 0.21197106\n","Iteration 64, loss = 0.21178016\n","Iteration 65, loss = 0.21143816\n","Iteration 66, loss = 0.21119206\n","Iteration 67, loss = 0.21094044\n","Iteration 68, loss = 0.21071019\n","Iteration 69, loss = 0.21042531\n","Iteration 70, loss = 0.21020987\n","Iteration 71, loss = 0.20989534\n","Iteration 72, loss = 0.20972332\n","Iteration 73, loss = 0.20940311\n","Iteration 74, loss = 0.20917011\n","Iteration 75, loss = 0.20894043\n","Iteration 76, loss = 0.20874173\n","Iteration 77, loss = 0.20847080\n","Iteration 78, loss = 0.20826034\n","Iteration 79, loss = 0.20803057\n","Iteration 80, loss = 0.20777117\n","Iteration 81, loss = 0.20752874\n","Iteration 82, loss = 0.20733606\n","Iteration 83, loss = 0.20708384\n","Iteration 84, loss = 0.20686239\n","Iteration 85, loss = 0.20664506\n","Iteration 86, loss = 0.20642386\n","Iteration 87, loss = 0.20621250\n","Iteration 88, loss = 0.20598912\n","Iteration 89, loss = 0.20579067\n","Iteration 90, loss = 0.20555649\n","Iteration 91, loss = 0.20531136\n","Iteration 92, loss = 0.20512531\n","Iteration 93, loss = 0.20491986\n","Iteration 94, loss = 0.20469328\n","Iteration 95, loss = 0.20448511\n","Iteration 96, loss = 0.20429615\n","Iteration 97, loss = 0.20408494\n","Iteration 98, loss = 0.20389652\n","Iteration 99, loss = 0.20370922\n","Iteration 100, loss = 0.20352542\n","Iteration 101, loss = 0.20330063\n","Iteration 102, loss = 0.20310987\n","Iteration 103, loss = 0.20288799\n","Iteration 104, loss = 0.20271720\n","Iteration 105, loss = 0.20250766\n","Iteration 106, loss = 0.20228337\n","Iteration 107, loss = 0.20214700\n","Iteration 108, loss = 0.20192020\n","Iteration 109, loss = 0.20173955\n","Iteration 110, loss = 0.20154627\n","Iteration 111, loss = 0.20135455\n","Iteration 112, loss = 0.20120271\n","Iteration 113, loss = 0.20097612\n","Iteration 114, loss = 0.20080985\n","Iteration 115, loss = 0.20062906\n","Iteration 116, loss = 0.20046506\n","Iteration 117, loss = 0.20025384\n","Iteration 118, loss = 0.20011233\n","Iteration 119, loss = 0.19990809\n","Iteration 120, loss = 0.19970577\n","Iteration 121, loss = 0.19954931\n","Iteration 122, loss = 0.19936658\n","Iteration 123, loss = 0.19918329\n","Iteration 124, loss = 0.19900946\n","Iteration 125, loss = 0.19882943\n","Iteration 126, loss = 0.19867728\n","Iteration 127, loss = 0.19850533\n","Iteration 128, loss = 0.19831214\n","Iteration 129, loss = 0.19813822\n","Iteration 130, loss = 0.19794252\n","Iteration 131, loss = 0.19781713\n","Iteration 132, loss = 0.19767764\n","Iteration 133, loss = 0.19746379\n","Iteration 134, loss = 0.19735287\n","Iteration 135, loss = 0.19708166\n","Iteration 136, loss = 0.19690319\n","Iteration 137, loss = 0.19678593\n","Iteration 138, loss = 0.19663143\n","Iteration 139, loss = 0.19650366\n","Iteration 140, loss = 0.19626503\n","Iteration 141, loss = 0.19611634\n","Iteration 142, loss = 0.19598379\n","Iteration 143, loss = 0.19579679\n","Iteration 144, loss = 0.19562785\n","Iteration 145, loss = 0.19547590\n","Iteration 146, loss = 0.19529708\n","Iteration 147, loss = 0.19514554\n","Iteration 148, loss = 0.19497170\n","Iteration 149, loss = 0.19483438\n","Iteration 150, loss = 0.19471479\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.40550508\n","Iteration 2, loss = 0.26122183\n","Iteration 3, loss = 0.23957094\n","Iteration 4, loss = 0.22990081\n","Iteration 5, loss = 0.22401008\n","Iteration 6, loss = 0.21911020\n","Iteration 7, loss = 0.21517114\n","Iteration 8, loss = 0.21170763\n","Iteration 9, loss = 0.20868573\n","Iteration 10, loss = 0.20613651\n","Iteration 11, loss = 0.20379280\n","Iteration 12, loss = 0.20160355\n","Iteration 13, loss = 0.19918806\n","Iteration 14, loss = 0.19705548\n","Iteration 15, loss = 0.19482964\n","Iteration 16, loss = 0.19320749\n","Iteration 17, loss = 0.19112349\n","Iteration 18, loss = 0.18979374\n","Iteration 19, loss = 0.18760027\n","Iteration 20, loss = 0.18591482\n","Iteration 21, loss = 0.18398426\n","Iteration 22, loss = 0.18213148\n","Iteration 23, loss = 0.18004604\n","Iteration 24, loss = 0.17889077\n","Iteration 25, loss = 0.17684961\n","Iteration 26, loss = 0.17530142\n","Iteration 27, loss = 0.17345047\n","Iteration 28, loss = 0.17148070\n","Iteration 29, loss = 0.16967843\n","Iteration 30, loss = 0.16818902\n","Iteration 31, loss = 0.16664131\n","Iteration 32, loss = 0.16485062\n","Iteration 33, loss = 0.16222102\n","Iteration 34, loss = 0.16052755\n","Iteration 35, loss = 0.15972433\n","Iteration 36, loss = 0.15714606\n","Iteration 37, loss = 0.15579291\n","Iteration 38, loss = 0.15395702\n","Iteration 39, loss = 0.15219909\n","Iteration 40, loss = 0.15067863\n","Iteration 41, loss = 0.14855437\n","Iteration 42, loss = 0.14763566\n","Iteration 43, loss = 0.14569564\n","Iteration 44, loss = 0.14434315\n","Iteration 45, loss = 0.14239680\n","Iteration 46, loss = 0.14051783\n","Iteration 47, loss = 0.13839813\n","Iteration 48, loss = 0.13687653\n","Iteration 49, loss = 0.13545415\n","Iteration 50, loss = 0.13418701\n","Iteration 51, loss = 0.13188143\n","Iteration 52, loss = 0.12983151\n","Iteration 53, loss = 0.12856292\n","Iteration 54, loss = 0.12754067\n","Iteration 55, loss = 0.12572384\n","Iteration 56, loss = 0.12382820\n","Iteration 57, loss = 0.12244125\n","Iteration 58, loss = 0.12104888\n","Iteration 59, loss = 0.11850181\n","Iteration 60, loss = 0.11774035\n","Iteration 61, loss = 0.11588577\n","Iteration 62, loss = 0.11503924\n","Iteration 63, loss = 0.11297521\n","Iteration 64, loss = 0.11236774\n","Iteration 65, loss = 0.10956246\n","Iteration 66, loss = 0.10876122\n","Iteration 67, loss = 0.10758645\n","Iteration 68, loss = 0.10519479\n","Iteration 69, loss = 0.10439040\n","Iteration 70, loss = 0.10289364\n","Iteration 71, loss = 0.10091914\n","Iteration 72, loss = 0.10019025\n","Iteration 73, loss = 0.09899404\n","Iteration 74, loss = 0.09692320\n","Iteration 75, loss = 0.09732355\n","Iteration 76, loss = 0.09495892\n","Iteration 77, loss = 0.09378142\n","Iteration 78, loss = 0.09163978\n","Iteration 79, loss = 0.09094221\n","Iteration 80, loss = 0.09051354\n","Iteration 81, loss = 0.08927929\n","Iteration 82, loss = 0.08650477\n","Iteration 83, loss = 0.08582962\n","Iteration 84, loss = 0.08450877\n","Iteration 85, loss = 0.08352328\n","Iteration 86, loss = 0.08314471\n","Iteration 87, loss = 0.08205987\n","Iteration 88, loss = 0.08085366\n","Iteration 89, loss = 0.07907916\n","Iteration 90, loss = 0.07856488\n","Iteration 91, loss = 0.07707256\n","Iteration 92, loss = 0.07585295\n","Iteration 93, loss = 0.07539438\n","Iteration 94, loss = 0.07419285\n","Iteration 95, loss = 0.07324373\n","Iteration 96, loss = 0.07147127\n","Iteration 97, loss = 0.07090942\n","Iteration 98, loss = 0.07058877\n","Iteration 99, loss = 0.06848162\n","Iteration 100, loss = 0.06848937\n","Iteration 101, loss = 0.06808318\n","Iteration 102, loss = 0.06513806\n","Iteration 103, loss = 0.06486195\n","Iteration 104, loss = 0.06349664\n","Iteration 105, loss = 0.06446761\n","Iteration 106, loss = 0.06229318\n","Iteration 107, loss = 0.06177527\n","Iteration 108, loss = 0.06045071\n","Iteration 109, loss = 0.05945521\n","Iteration 110, loss = 0.05968793\n","Iteration 111, loss = 0.05747972\n","Iteration 112, loss = 0.05728379\n","Iteration 113, loss = 0.05628758\n","Iteration 114, loss = 0.05509330\n","Iteration 115, loss = 0.05408900\n","Iteration 116, loss = 0.05322553\n","Iteration 117, loss = 0.05200469\n","Iteration 118, loss = 0.05166374\n","Iteration 119, loss = 0.05181813\n","Iteration 120, loss = 0.05000474\n","Iteration 121, loss = 0.04914468\n","Iteration 122, loss = 0.04822291\n","Iteration 123, loss = 0.04755184\n","Iteration 124, loss = 0.04668945\n","Iteration 125, loss = 0.04537161\n","Iteration 126, loss = 0.04530133\n","Iteration 127, loss = 0.04452029\n","Iteration 128, loss = 0.04351518\n","Iteration 129, loss = 0.04227671\n","Iteration 130, loss = 0.04215596\n","Iteration 131, loss = 0.04112148\n","Iteration 132, loss = 0.04105290\n","Iteration 133, loss = 0.04008895\n","Iteration 134, loss = 0.03885306\n","Iteration 135, loss = 0.03822870\n","Iteration 136, loss = 0.03724475\n","Iteration 137, loss = 0.03746268\n","Iteration 138, loss = 0.03620498\n","Iteration 139, loss = 0.03597993\n","Iteration 140, loss = 0.03486855\n","Iteration 141, loss = 0.03410021\n","Iteration 142, loss = 0.03411035\n","Iteration 143, loss = 0.03307766\n","Iteration 144, loss = 0.03322859\n","Iteration 145, loss = 0.03176259\n","Iteration 146, loss = 0.03177543\n","Iteration 147, loss = 0.03059274\n","Iteration 148, loss = 0.02966678\n","Iteration 149, loss = 0.02988613\n","Iteration 150, loss = 0.02901508\n","Iteration 1, loss = 0.41858802\n","Iteration 2, loss = 0.26999690\n","Iteration 3, loss = 0.24269490\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.23109162\n","Iteration 5, loss = 0.22405386\n","Iteration 6, loss = 0.21928303\n","Iteration 7, loss = 0.21513577\n","Iteration 8, loss = 0.21215070\n","Iteration 9, loss = 0.20881260\n","Iteration 10, loss = 0.20657372\n","Iteration 11, loss = 0.20371792\n","Iteration 12, loss = 0.20100395\n","Iteration 13, loss = 0.19872092\n","Iteration 14, loss = 0.19673525\n","Iteration 15, loss = 0.19462297\n","Iteration 16, loss = 0.19248742\n","Iteration 17, loss = 0.18984845\n","Iteration 18, loss = 0.18845183\n","Iteration 19, loss = 0.18620508\n","Iteration 20, loss = 0.18442139\n","Iteration 21, loss = 0.18248804\n","Iteration 22, loss = 0.18078802\n","Iteration 23, loss = 0.17889822\n","Iteration 24, loss = 0.17702348\n","Iteration 25, loss = 0.17515077\n","Iteration 26, loss = 0.17279980\n","Iteration 27, loss = 0.17132049\n","Iteration 28, loss = 0.16903840\n","Iteration 29, loss = 0.16697949\n","Iteration 30, loss = 0.16433828\n","Iteration 31, loss = 0.16395712\n","Iteration 32, loss = 0.16171031\n","Iteration 33, loss = 0.15951201\n","Iteration 34, loss = 0.15758806\n","Iteration 35, loss = 0.15521530\n","Iteration 36, loss = 0.15377114\n","Iteration 37, loss = 0.15176664\n","Iteration 38, loss = 0.14962300\n","Iteration 39, loss = 0.14740565\n","Iteration 40, loss = 0.14726236\n","Iteration 41, loss = 0.14379558\n","Iteration 42, loss = 0.14182209\n","Iteration 43, loss = 0.14052010\n","Iteration 44, loss = 0.13837441\n","Iteration 45, loss = 0.13642798\n","Iteration 46, loss = 0.13466331\n","Iteration 47, loss = 0.13236097\n","Iteration 48, loss = 0.13076047\n","Iteration 49, loss = 0.12903866\n","Iteration 50, loss = 0.12853242\n","Iteration 51, loss = 0.12507778\n","Iteration 52, loss = 0.12484997\n","Iteration 53, loss = 0.12310384\n","Iteration 54, loss = 0.12000261\n","Iteration 55, loss = 0.11870005\n","Iteration 56, loss = 0.11675127\n","Iteration 57, loss = 0.11541886\n","Iteration 58, loss = 0.11378971\n","Iteration 59, loss = 0.11229308\n","Iteration 60, loss = 0.11148860\n","Iteration 61, loss = 0.10965295\n","Iteration 62, loss = 0.10836439\n","Iteration 63, loss = 0.10644375\n","Iteration 64, loss = 0.10418247\n","Iteration 65, loss = 0.10376506\n","Iteration 66, loss = 0.10321823\n","Iteration 67, loss = 0.10076955\n","Iteration 68, loss = 0.09911653\n","Iteration 69, loss = 0.09977772\n","Iteration 70, loss = 0.09702527\n","Iteration 71, loss = 0.09535081\n","Iteration 72, loss = 0.09515079\n","Iteration 73, loss = 0.09375517\n","Iteration 74, loss = 0.09155908\n","Iteration 75, loss = 0.08998652\n","Iteration 76, loss = 0.08942484\n","Iteration 77, loss = 0.08808230\n","Iteration 78, loss = 0.08668453\n","Iteration 79, loss = 0.08571961\n","Iteration 80, loss = 0.08549552\n","Iteration 81, loss = 0.08332077\n","Iteration 82, loss = 0.08270807\n","Iteration 83, loss = 0.08181696\n","Iteration 84, loss = 0.08067222\n","Iteration 85, loss = 0.07928546\n","Iteration 86, loss = 0.07876478\n","Iteration 87, loss = 0.07753957\n","Iteration 88, loss = 0.07696866\n","Iteration 89, loss = 0.07516440\n","Iteration 90, loss = 0.07463381\n","Iteration 91, loss = 0.07365333\n","Iteration 92, loss = 0.07177868\n","Iteration 93, loss = 0.07296843\n","Iteration 94, loss = 0.07011714\n","Iteration 95, loss = 0.06953116\n","Iteration 96, loss = 0.06898829\n","Iteration 97, loss = 0.06795117\n","Iteration 98, loss = 0.06695883\n","Iteration 99, loss = 0.06504259\n","Iteration 100, loss = 0.06561565\n","Iteration 101, loss = 0.06467850\n","Iteration 102, loss = 0.06355293\n","Iteration 103, loss = 0.06245488\n","Iteration 104, loss = 0.06226520\n","Iteration 105, loss = 0.06094524\n","Iteration 106, loss = 0.06038218\n","Iteration 107, loss = 0.05951958\n","Iteration 108, loss = 0.05863422\n","Iteration 109, loss = 0.05809528\n","Iteration 110, loss = 0.05701045\n","Iteration 111, loss = 0.05578857\n","Iteration 112, loss = 0.05472798\n","Iteration 113, loss = 0.05457031\n","Iteration 114, loss = 0.05351432\n","Iteration 115, loss = 0.05262383\n","Iteration 116, loss = 0.05300698\n","Iteration 117, loss = 0.05177010\n","Iteration 118, loss = 0.05135287\n","Iteration 119, loss = 0.05033872\n","Iteration 120, loss = 0.05025695\n","Iteration 121, loss = 0.04871634\n","Iteration 122, loss = 0.04811335\n","Iteration 123, loss = 0.04793742\n","Iteration 124, loss = 0.04649392\n","Iteration 125, loss = 0.04599538\n","Iteration 126, loss = 0.04563720\n","Iteration 127, loss = 0.04519080\n","Iteration 128, loss = 0.04368843\n","Iteration 129, loss = 0.04333231\n","Iteration 130, loss = 0.04297827\n","Iteration 131, loss = 0.04307287\n","Iteration 132, loss = 0.04105112\n","Iteration 133, loss = 0.04174729\n","Iteration 134, loss = 0.04088862\n","Iteration 135, loss = 0.03956360\n","Iteration 136, loss = 0.03972102\n","Iteration 137, loss = 0.03937285\n","Iteration 138, loss = 0.03793084\n","Iteration 139, loss = 0.03744210\n","Iteration 140, loss = 0.03689283\n","Iteration 141, loss = 0.03666985\n","Iteration 142, loss = 0.03503678\n","Iteration 143, loss = 0.03534778\n","Iteration 144, loss = 0.03479204\n","Iteration 145, loss = 0.03428970\n","Iteration 146, loss = 0.03444004\n","Iteration 147, loss = 0.03367651\n","Iteration 148, loss = 0.03290255\n","Iteration 149, loss = 0.03217289\n","Iteration 150, loss = 0.03184907\n","Iteration 1, loss = 0.53031386\n","Iteration 2, loss = 0.27241455\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.23469188\n","Iteration 4, loss = 0.22277898\n","Iteration 5, loss = 0.21697043\n","Iteration 6, loss = 0.21295995\n","Iteration 7, loss = 0.20941836\n","Iteration 8, loss = 0.20746134\n","Iteration 9, loss = 0.20432392\n","Iteration 10, loss = 0.20208307\n","Iteration 11, loss = 0.20013085\n","Iteration 12, loss = 0.19802315\n","Iteration 13, loss = 0.19609384\n","Iteration 14, loss = 0.19454694\n","Iteration 15, loss = 0.19237487\n","Iteration 16, loss = 0.19071252\n","Iteration 17, loss = 0.18900146\n","Iteration 18, loss = 0.18679391\n","Iteration 19, loss = 0.18503443\n","Iteration 20, loss = 0.18375039\n","Iteration 21, loss = 0.18189437\n","Iteration 22, loss = 0.17996436\n","Iteration 23, loss = 0.17936724\n","Iteration 24, loss = 0.17718366\n","Iteration 25, loss = 0.17497565\n","Iteration 26, loss = 0.17371755\n","Iteration 27, loss = 0.17175792\n","Iteration 28, loss = 0.17033992\n","Iteration 29, loss = 0.16776807\n","Iteration 30, loss = 0.16645529\n","Iteration 31, loss = 0.16484444\n","Iteration 32, loss = 0.16215720\n","Iteration 33, loss = 0.16051857\n","Iteration 34, loss = 0.15895239\n","Iteration 35, loss = 0.15771073\n","Iteration 36, loss = 0.15544592\n","Iteration 37, loss = 0.15338025\n","Iteration 38, loss = 0.15272448\n","Iteration 39, loss = 0.15052807\n","Iteration 40, loss = 0.14861227\n","Iteration 41, loss = 0.14694136\n","Iteration 42, loss = 0.14549914\n","Iteration 43, loss = 0.14264933\n","Iteration 44, loss = 0.14213338\n","Iteration 45, loss = 0.13921105\n","Iteration 46, loss = 0.13802383\n","Iteration 47, loss = 0.13722219\n","Iteration 48, loss = 0.13422099\n","Iteration 49, loss = 0.13354188\n","Iteration 50, loss = 0.13177301\n","Iteration 51, loss = 0.13104323\n","Iteration 52, loss = 0.12970186\n","Iteration 53, loss = 0.12763707\n","Iteration 54, loss = 0.12529327\n","Iteration 55, loss = 0.12415637\n","Iteration 56, loss = 0.12274058\n","Iteration 57, loss = 0.12146212\n","Iteration 58, loss = 0.11938967\n","Iteration 59, loss = 0.12006651\n","Iteration 60, loss = 0.11824615\n","Iteration 61, loss = 0.11634273\n","Iteration 62, loss = 0.11523517\n","Iteration 63, loss = 0.11259124\n","Iteration 64, loss = 0.11138562\n","Iteration 65, loss = 0.10984736\n","Iteration 66, loss = 0.10961407\n","Iteration 67, loss = 0.10861106\n","Iteration 68, loss = 0.10711051\n","Iteration 69, loss = 0.10487924\n","Iteration 70, loss = 0.10528261\n","Iteration 71, loss = 0.10261630\n","Iteration 72, loss = 0.10137975\n","Iteration 73, loss = 0.09992458\n","Iteration 74, loss = 0.09937122\n","Iteration 75, loss = 0.09749916\n","Iteration 76, loss = 0.09616786\n","Iteration 77, loss = 0.09506770\n","Iteration 78, loss = 0.09436395\n","Iteration 79, loss = 0.09332250\n","Iteration 80, loss = 0.09215436\n","Iteration 81, loss = 0.09151525\n","Iteration 82, loss = 0.09121260\n","Iteration 83, loss = 0.08926749\n","Iteration 84, loss = 0.08910842\n","Iteration 85, loss = 0.08789415\n","Iteration 86, loss = 0.08544601\n","Iteration 87, loss = 0.08454785\n","Iteration 88, loss = 0.08352848\n","Iteration 89, loss = 0.08172967\n","Iteration 90, loss = 0.08196918\n","Iteration 91, loss = 0.08124710\n","Iteration 92, loss = 0.07959612\n","Iteration 93, loss = 0.07876300\n","Iteration 94, loss = 0.07717414\n","Iteration 95, loss = 0.07675610\n","Iteration 96, loss = 0.07476692\n","Iteration 97, loss = 0.07417624\n","Iteration 98, loss = 0.07318528\n","Iteration 99, loss = 0.07137968\n","Iteration 100, loss = 0.07210219\n","Iteration 101, loss = 0.06982478\n","Iteration 102, loss = 0.06944536\n","Iteration 103, loss = 0.06829779\n","Iteration 104, loss = 0.06917394\n","Iteration 105, loss = 0.06758492\n","Iteration 106, loss = 0.06668351\n","Iteration 107, loss = 0.06604017\n","Iteration 108, loss = 0.06448921\n","Iteration 109, loss = 0.06317324\n","Iteration 110, loss = 0.06239755\n","Iteration 111, loss = 0.06234938\n","Iteration 112, loss = 0.06019332\n","Iteration 113, loss = 0.06047614\n","Iteration 114, loss = 0.05947585\n","Iteration 115, loss = 0.05832298\n","Iteration 116, loss = 0.05656681\n","Iteration 117, loss = 0.05708731\n","Iteration 118, loss = 0.05603392\n","Iteration 119, loss = 0.05463154\n","Iteration 120, loss = 0.05438942\n","Iteration 121, loss = 0.05265718\n","Iteration 122, loss = 0.05218341\n","Iteration 123, loss = 0.05192615\n","Iteration 124, loss = 0.05004479\n","Iteration 125, loss = 0.05022086\n","Iteration 126, loss = 0.04930740\n","Iteration 127, loss = 0.04874622\n","Iteration 128, loss = 0.04826731\n","Iteration 129, loss = 0.04660111\n","Iteration 130, loss = 0.04616389\n","Iteration 131, loss = 0.04578299\n","Iteration 132, loss = 0.04473196\n","Iteration 133, loss = 0.04323248\n","Iteration 134, loss = 0.04292978\n","Iteration 135, loss = 0.04260740\n","Iteration 136, loss = 0.04160879\n","Iteration 137, loss = 0.04059537\n","Iteration 138, loss = 0.03939340\n","Iteration 139, loss = 0.03972063\n","Iteration 140, loss = 0.03852789\n","Iteration 141, loss = 0.03697078\n","Iteration 142, loss = 0.03763578\n","Iteration 143, loss = 0.03698947\n","Iteration 144, loss = 0.03648188\n","Iteration 145, loss = 0.03515195\n","Iteration 146, loss = 0.03505061\n","Iteration 147, loss = 0.03448683\n","Iteration 148, loss = 0.03341975\n","Iteration 149, loss = 0.03363643\n","Iteration 150, loss = 0.03255251\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.54127207\n","Iteration 2, loss = 0.29910102\n","Iteration 3, loss = 0.24899113\n","Iteration 4, loss = 0.23533057\n","Iteration 5, loss = 0.22805232\n","Iteration 6, loss = 0.22229964\n","Iteration 7, loss = 0.21793596\n","Iteration 8, loss = 0.21437816\n","Iteration 9, loss = 0.21170952\n","Iteration 10, loss = 0.20864826\n","Iteration 11, loss = 0.20627102\n","Iteration 12, loss = 0.20392278\n","Iteration 13, loss = 0.20175709\n","Iteration 14, loss = 0.19942216\n","Iteration 15, loss = 0.19705951\n","Iteration 16, loss = 0.19474019\n","Iteration 17, loss = 0.19337763\n","Iteration 18, loss = 0.19128089\n","Iteration 19, loss = 0.18927187\n","Iteration 20, loss = 0.18711245\n","Iteration 21, loss = 0.18571820\n","Iteration 22, loss = 0.18350893\n","Iteration 23, loss = 0.18147830\n","Iteration 24, loss = 0.17949685\n","Iteration 25, loss = 0.17764185\n","Iteration 26, loss = 0.17597885\n","Iteration 27, loss = 0.17399863\n","Iteration 28, loss = 0.17167470\n","Iteration 29, loss = 0.17013313\n","Iteration 30, loss = 0.16757120\n","Iteration 31, loss = 0.16611728\n","Iteration 32, loss = 0.16412345\n","Iteration 33, loss = 0.16264145\n","Iteration 34, loss = 0.16157365\n","Iteration 35, loss = 0.15892286\n","Iteration 36, loss = 0.15740841\n","Iteration 37, loss = 0.15568305\n","Iteration 38, loss = 0.15397556\n","Iteration 39, loss = 0.15253279\n","Iteration 40, loss = 0.15075112\n","Iteration 41, loss = 0.14990976\n","Iteration 42, loss = 0.14670829\n","Iteration 43, loss = 0.14528414\n","Iteration 44, loss = 0.14364155\n","Iteration 45, loss = 0.14312836\n","Iteration 46, loss = 0.14033410\n","Iteration 47, loss = 0.13923423\n","Iteration 48, loss = 0.13785164\n","Iteration 49, loss = 0.13563988\n","Iteration 50, loss = 0.13417185\n","Iteration 51, loss = 0.13285031\n","Iteration 52, loss = 0.13156917\n","Iteration 53, loss = 0.13013719\n","Iteration 54, loss = 0.12871276\n","Iteration 55, loss = 0.12649740\n","Iteration 56, loss = 0.12564776\n","Iteration 57, loss = 0.12301616\n","Iteration 58, loss = 0.12194451\n","Iteration 59, loss = 0.12091709\n","Iteration 60, loss = 0.11916531\n","Iteration 61, loss = 0.11824202\n","Iteration 62, loss = 0.11619776\n","Iteration 63, loss = 0.11467712\n","Iteration 64, loss = 0.11319485\n","Iteration 65, loss = 0.11287376\n","Iteration 66, loss = 0.11088987\n","Iteration 67, loss = 0.10975181\n","Iteration 68, loss = 0.10868884\n","Iteration 69, loss = 0.10652212\n","Iteration 70, loss = 0.10514929\n","Iteration 71, loss = 0.10346557\n","Iteration 72, loss = 0.10223043\n","Iteration 73, loss = 0.10055054\n","Iteration 74, loss = 0.10082780\n","Iteration 75, loss = 0.09893129\n","Iteration 76, loss = 0.09827528\n","Iteration 77, loss = 0.09732576\n","Iteration 78, loss = 0.09530975\n","Iteration 79, loss = 0.09422552\n","Iteration 80, loss = 0.09242252\n","Iteration 81, loss = 0.09104508\n","Iteration 82, loss = 0.09055815\n","Iteration 83, loss = 0.08881794\n","Iteration 84, loss = 0.08780027\n","Iteration 85, loss = 0.08652837\n","Iteration 86, loss = 0.08565287\n","Iteration 87, loss = 0.08414186\n","Iteration 88, loss = 0.08401864\n","Iteration 89, loss = 0.08152305\n","Iteration 90, loss = 0.08155194\n","Iteration 91, loss = 0.07999319\n","Iteration 92, loss = 0.07933675\n","Iteration 93, loss = 0.07983972\n","Iteration 94, loss = 0.07621311\n","Iteration 95, loss = 0.07618127\n","Iteration 96, loss = 0.07443084\n","Iteration 97, loss = 0.07476023\n","Iteration 98, loss = 0.07407663\n","Iteration 99, loss = 0.07178054\n","Iteration 100, loss = 0.07010023\n","Iteration 101, loss = 0.06950095\n","Iteration 102, loss = 0.06804540\n","Iteration 103, loss = 0.06664800\n","Iteration 104, loss = 0.06646086\n","Iteration 105, loss = 0.06566662\n","Iteration 106, loss = 0.06498611\n","Iteration 107, loss = 0.06460205\n","Iteration 108, loss = 0.06413520\n","Iteration 109, loss = 0.06187407\n","Iteration 110, loss = 0.06108156\n","Iteration 111, loss = 0.06028292\n","Iteration 112, loss = 0.05907561\n","Iteration 113, loss = 0.05802519\n","Iteration 114, loss = 0.05765690\n","Iteration 115, loss = 0.05668219\n","Iteration 116, loss = 0.05606342\n","Iteration 117, loss = 0.05431262\n","Iteration 118, loss = 0.05413196\n","Iteration 119, loss = 0.05219983\n","Iteration 120, loss = 0.05283313\n","Iteration 121, loss = 0.05167558\n","Iteration 122, loss = 0.05023109\n","Iteration 123, loss = 0.04932776\n","Iteration 124, loss = 0.04929726\n","Iteration 125, loss = 0.04735207\n","Iteration 126, loss = 0.04702483\n","Iteration 127, loss = 0.04650427\n","Iteration 128, loss = 0.04569579\n","Iteration 129, loss = 0.04450277\n","Iteration 130, loss = 0.04400221\n","Iteration 131, loss = 0.04306512\n","Iteration 132, loss = 0.04323759\n","Iteration 133, loss = 0.04151993\n","Iteration 134, loss = 0.04089209\n","Iteration 135, loss = 0.03991081\n","Iteration 136, loss = 0.03933818\n","Iteration 137, loss = 0.03894252\n","Iteration 138, loss = 0.03832278\n","Iteration 139, loss = 0.03764206\n","Iteration 140, loss = 0.03734007\n","Iteration 141, loss = 0.03712337\n","Iteration 142, loss = 0.03639887\n","Iteration 143, loss = 0.03526096\n","Iteration 144, loss = 0.03431373\n","Iteration 145, loss = 0.03420147\n","Iteration 146, loss = 0.03434423\n","Iteration 147, loss = 0.03266805\n","Iteration 148, loss = 0.03228017\n","Iteration 149, loss = 0.03168110\n","Iteration 150, loss = 0.03112510\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.56878909\n","Iteration 2, loss = 0.32311398\n","Iteration 3, loss = 0.26747890\n","Iteration 4, loss = 0.24575621\n","Iteration 5, loss = 0.23377642\n","Iteration 6, loss = 0.22628048\n","Iteration 7, loss = 0.22112728\n","Iteration 8, loss = 0.21688304\n","Iteration 9, loss = 0.21321374\n","Iteration 10, loss = 0.20984637\n","Iteration 11, loss = 0.20689390\n","Iteration 12, loss = 0.20461754\n","Iteration 13, loss = 0.20307554\n","Iteration 14, loss = 0.19972144\n","Iteration 15, loss = 0.19769745\n","Iteration 16, loss = 0.19546023\n","Iteration 17, loss = 0.19292700\n","Iteration 18, loss = 0.19123615\n","Iteration 19, loss = 0.18914943\n","Iteration 20, loss = 0.18685839\n","Iteration 21, loss = 0.18509065\n","Iteration 22, loss = 0.18385552\n","Iteration 23, loss = 0.18201263\n","Iteration 24, loss = 0.18060973\n","Iteration 25, loss = 0.17881720\n","Iteration 26, loss = 0.17734367\n","Iteration 27, loss = 0.17544121\n","Iteration 28, loss = 0.17340897\n","Iteration 29, loss = 0.17174231\n","Iteration 30, loss = 0.17036633\n","Iteration 31, loss = 0.16896649\n","Iteration 32, loss = 0.16666729\n","Iteration 33, loss = 0.16472053\n","Iteration 34, loss = 0.16346890\n","Iteration 35, loss = 0.16124317\n","Iteration 36, loss = 0.16048207\n","Iteration 37, loss = 0.15953461\n","Iteration 38, loss = 0.15633429\n","Iteration 39, loss = 0.15507205\n","Iteration 40, loss = 0.15370916\n","Iteration 41, loss = 0.15196037\n","Iteration 42, loss = 0.14948068\n","Iteration 43, loss = 0.14845383\n","Iteration 44, loss = 0.14637786\n","Iteration 45, loss = 0.14493579\n","Iteration 46, loss = 0.14354608\n","Iteration 47, loss = 0.14176461\n","Iteration 48, loss = 0.13986392\n","Iteration 49, loss = 0.13837639\n","Iteration 50, loss = 0.13695148\n","Iteration 51, loss = 0.13554217\n","Iteration 52, loss = 0.13432023\n","Iteration 53, loss = 0.13277773\n","Iteration 54, loss = 0.13123431\n","Iteration 55, loss = 0.12970583\n","Iteration 56, loss = 0.12772953\n","Iteration 57, loss = 0.12639825\n","Iteration 58, loss = 0.12474220\n","Iteration 59, loss = 0.12319931\n","Iteration 60, loss = 0.12194448\n","Iteration 61, loss = 0.12111166\n","Iteration 62, loss = 0.12049506\n","Iteration 63, loss = 0.11931668\n","Iteration 64, loss = 0.11707305\n","Iteration 65, loss = 0.11509833\n","Iteration 66, loss = 0.11322246\n","Iteration 67, loss = 0.11302652\n","Iteration 68, loss = 0.11212541\n","Iteration 69, loss = 0.11009656\n","Iteration 70, loss = 0.10916013\n","Iteration 71, loss = 0.10812454\n","Iteration 72, loss = 0.10616363\n","Iteration 73, loss = 0.10540524\n","Iteration 74, loss = 0.10483389\n","Iteration 75, loss = 0.10262908\n","Iteration 76, loss = 0.10124548\n","Iteration 77, loss = 0.10021461\n","Iteration 78, loss = 0.09927601\n","Iteration 79, loss = 0.09801739\n","Iteration 80, loss = 0.09663516\n","Iteration 81, loss = 0.09518224\n","Iteration 82, loss = 0.09466532\n","Iteration 83, loss = 0.09378305\n","Iteration 84, loss = 0.09191896\n","Iteration 85, loss = 0.09121340\n","Iteration 86, loss = 0.09027537\n","Iteration 87, loss = 0.08878999\n","Iteration 88, loss = 0.08718843\n","Iteration 89, loss = 0.08805894\n","Iteration 90, loss = 0.08671222\n","Iteration 91, loss = 0.08519345\n","Iteration 92, loss = 0.08376227\n","Iteration 93, loss = 0.08240846\n","Iteration 94, loss = 0.08277391\n","Iteration 95, loss = 0.08074862\n","Iteration 96, loss = 0.07940031\n","Iteration 97, loss = 0.07834269\n","Iteration 98, loss = 0.07758707\n","Iteration 99, loss = 0.07647840\n","Iteration 100, loss = 0.07549680\n","Iteration 101, loss = 0.07483272\n","Iteration 102, loss = 0.07398009\n","Iteration 103, loss = 0.07364816\n","Iteration 104, loss = 0.07249599\n","Iteration 105, loss = 0.07072718\n","Iteration 106, loss = 0.06970657\n","Iteration 107, loss = 0.06995071\n","Iteration 108, loss = 0.06896118\n","Iteration 109, loss = 0.06726525\n","Iteration 110, loss = 0.06667101\n","Iteration 111, loss = 0.06563192\n","Iteration 112, loss = 0.06406038\n","Iteration 113, loss = 0.06505089\n","Iteration 114, loss = 0.06560993\n","Iteration 115, loss = 0.06251911\n","Iteration 116, loss = 0.06186293\n","Iteration 117, loss = 0.06148115\n","Iteration 118, loss = 0.05948588\n","Iteration 119, loss = 0.05962109\n","Iteration 120, loss = 0.05810019\n","Iteration 121, loss = 0.05885075\n","Iteration 122, loss = 0.05692551\n","Iteration 123, loss = 0.05542764\n","Iteration 124, loss = 0.05557622\n","Iteration 125, loss = 0.05394847\n","Iteration 126, loss = 0.05340763\n","Iteration 127, loss = 0.05301788\n","Iteration 128, loss = 0.05224196\n","Iteration 129, loss = 0.05072247\n","Iteration 130, loss = 0.05037094\n","Iteration 131, loss = 0.05026746\n","Iteration 132, loss = 0.04890540\n","Iteration 133, loss = 0.04882849\n","Iteration 134, loss = 0.04781085\n","Iteration 135, loss = 0.04734323\n","Iteration 136, loss = 0.04669737\n","Iteration 137, loss = 0.04663650\n","Iteration 138, loss = 0.04459849\n","Iteration 139, loss = 0.04506354\n","Iteration 140, loss = 0.04410642\n","Iteration 141, loss = 0.04337624\n","Iteration 142, loss = 0.04358159\n","Iteration 143, loss = 0.04249313\n","Iteration 144, loss = 0.04188366\n","Iteration 145, loss = 0.04083546\n","Iteration 146, loss = 0.04055114\n","Iteration 147, loss = 0.03893319\n","Iteration 148, loss = 0.03907871\n","Iteration 149, loss = 0.03775336\n","Iteration 150, loss = 0.03720153\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.67411384\n","Iteration 2, loss = 0.41639724\n","Iteration 3, loss = 0.34064414\n","Iteration 4, loss = 0.30684569\n","Iteration 5, loss = 0.28719417\n","Iteration 6, loss = 0.27439078\n","Iteration 7, loss = 0.26528320\n","Iteration 8, loss = 0.25847412\n","Iteration 9, loss = 0.25318259\n","Iteration 10, loss = 0.24909406\n","Iteration 11, loss = 0.24561814\n","Iteration 12, loss = 0.24275492\n","Iteration 13, loss = 0.24037928\n","Iteration 14, loss = 0.23823685\n","Iteration 15, loss = 0.23641474\n","Iteration 16, loss = 0.23487625\n","Iteration 17, loss = 0.23336531\n","Iteration 18, loss = 0.23209788\n","Iteration 19, loss = 0.23097332\n","Iteration 20, loss = 0.22991125\n","Iteration 21, loss = 0.22896358\n","Iteration 22, loss = 0.22809517\n","Iteration 23, loss = 0.22726684\n","Iteration 24, loss = 0.22653748\n","Iteration 25, loss = 0.22583031\n","Iteration 26, loss = 0.22518445\n","Iteration 27, loss = 0.22458502\n","Iteration 28, loss = 0.22399086\n","Iteration 29, loss = 0.22343224\n","Iteration 30, loss = 0.22292719\n","Iteration 31, loss = 0.22243698\n","Iteration 32, loss = 0.22198950\n","Iteration 33, loss = 0.22152359\n","Iteration 34, loss = 0.22110980\n","Iteration 35, loss = 0.22072615\n","Iteration 36, loss = 0.22029472\n","Iteration 37, loss = 0.21993340\n","Iteration 38, loss = 0.21957993\n","Iteration 39, loss = 0.21919441\n","Iteration 40, loss = 0.21887328\n","Iteration 41, loss = 0.21850070\n","Iteration 42, loss = 0.21822102\n","Iteration 43, loss = 0.21784949\n","Iteration 44, loss = 0.21754904\n","Iteration 45, loss = 0.21724096\n","Iteration 46, loss = 0.21697918\n","Iteration 47, loss = 0.21668334\n","Iteration 48, loss = 0.21635515\n","Iteration 49, loss = 0.21607103\n","Iteration 50, loss = 0.21583888\n","Iteration 51, loss = 0.21558405\n","Iteration 52, loss = 0.21529279\n","Iteration 53, loss = 0.21505989\n","Iteration 54, loss = 0.21480355\n","Iteration 55, loss = 0.21458196\n","Iteration 56, loss = 0.21431341\n","Iteration 57, loss = 0.21409101\n","Iteration 58, loss = 0.21386706\n","Iteration 59, loss = 0.21367944\n","Iteration 60, loss = 0.21340995\n","Iteration 61, loss = 0.21318003\n","Iteration 62, loss = 0.21296666\n","Iteration 63, loss = 0.21271242\n","Iteration 64, loss = 0.21247726\n","Iteration 65, loss = 0.21227464\n","Iteration 66, loss = 0.21207122\n","Iteration 67, loss = 0.21186662\n","Iteration 68, loss = 0.21172397\n","Iteration 69, loss = 0.21145789\n","Iteration 70, loss = 0.21130947\n","Iteration 71, loss = 0.21108270\n","Iteration 72, loss = 0.21090999\n","Iteration 73, loss = 0.21073707\n","Iteration 74, loss = 0.21053442\n","Iteration 75, loss = 0.21034717\n","Iteration 76, loss = 0.21014454\n","Iteration 77, loss = 0.21000340\n","Iteration 78, loss = 0.20982159\n","Iteration 79, loss = 0.20959308\n","Iteration 80, loss = 0.20941273\n","Iteration 81, loss = 0.20928968\n","Iteration 82, loss = 0.20905724\n","Iteration 83, loss = 0.20888694\n","Iteration 84, loss = 0.20872516\n","Iteration 85, loss = 0.20857337\n","Iteration 86, loss = 0.20835719\n","Iteration 87, loss = 0.20819896\n","Iteration 88, loss = 0.20803415\n","Iteration 89, loss = 0.20784111\n","Iteration 90, loss = 0.20772778\n","Iteration 91, loss = 0.20753471\n","Iteration 92, loss = 0.20740817\n","Iteration 93, loss = 0.20721241\n","Iteration 94, loss = 0.20707815\n","Iteration 95, loss = 0.20690538\n","Iteration 96, loss = 0.20674323\n","Iteration 97, loss = 0.20661256\n","Iteration 98, loss = 0.20642115\n","Iteration 99, loss = 0.20625095\n","Iteration 100, loss = 0.20607740\n","Iteration 101, loss = 0.20593318\n","Iteration 102, loss = 0.20576987\n","Iteration 103, loss = 0.20561650\n","Iteration 104, loss = 0.20546133\n","Iteration 105, loss = 0.20533093\n","Iteration 106, loss = 0.20513796\n","Iteration 107, loss = 0.20500164\n","Iteration 108, loss = 0.20486574\n","Iteration 109, loss = 0.20470000\n","Iteration 110, loss = 0.20455480\n","Iteration 111, loss = 0.20441876\n","Iteration 112, loss = 0.20423171\n","Iteration 113, loss = 0.20413548\n","Iteration 114, loss = 0.20393082\n","Iteration 115, loss = 0.20381537\n","Iteration 116, loss = 0.20363851\n","Iteration 117, loss = 0.20356625\n","Iteration 118, loss = 0.20337127\n","Iteration 119, loss = 0.20326118\n","Iteration 120, loss = 0.20303385\n","Iteration 121, loss = 0.20298559\n","Iteration 122, loss = 0.20275184\n","Iteration 123, loss = 0.20259666\n","Iteration 124, loss = 0.20250193\n","Iteration 125, loss = 0.20236968\n","Iteration 126, loss = 0.20221053\n","Iteration 127, loss = 0.20205707\n","Iteration 128, loss = 0.20191379\n","Iteration 129, loss = 0.20175946\n","Iteration 130, loss = 0.20159655\n","Iteration 131, loss = 0.20145600\n","Iteration 132, loss = 0.20135218\n","Iteration 133, loss = 0.20121482\n","Iteration 134, loss = 0.20109421\n","Iteration 135, loss = 0.20093269\n","Iteration 136, loss = 0.20077214\n","Iteration 137, loss = 0.20065207\n","Iteration 138, loss = 0.20049333\n","Iteration 139, loss = 0.20037059\n","Iteration 140, loss = 0.20019945\n","Iteration 141, loss = 0.20012188\n","Iteration 142, loss = 0.19994007\n","Iteration 143, loss = 0.19978031\n","Iteration 144, loss = 0.19965375\n","Iteration 145, loss = 0.19953090\n","Iteration 146, loss = 0.19939755\n","Iteration 147, loss = 0.19927563\n","Iteration 148, loss = 0.19917940\n","Iteration 149, loss = 0.19896016\n","Iteration 150, loss = 0.19882293\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.56223958\n","Iteration 2, loss = 0.41606160\n","Iteration 3, loss = 0.34464470\n","Iteration 4, loss = 0.30564727\n","Iteration 5, loss = 0.28250945\n","Iteration 6, loss = 0.26767135\n","Iteration 7, loss = 0.25777580\n","Iteration 8, loss = 0.25091032\n","Iteration 9, loss = 0.24575839\n","Iteration 10, loss = 0.24186019\n","Iteration 11, loss = 0.23870134\n","Iteration 12, loss = 0.23608725\n","Iteration 13, loss = 0.23393070\n","Iteration 14, loss = 0.23206522\n","Iteration 15, loss = 0.23047450\n","Iteration 16, loss = 0.22904561\n","Iteration 17, loss = 0.22781762\n","Iteration 18, loss = 0.22680760\n","Iteration 19, loss = 0.22591548\n","Iteration 20, loss = 0.22502061\n","Iteration 21, loss = 0.22422750\n","Iteration 22, loss = 0.22351571\n","Iteration 23, loss = 0.22290378\n","Iteration 24, loss = 0.22232379\n","Iteration 25, loss = 0.22184536\n","Iteration 26, loss = 0.22127279\n","Iteration 27, loss = 0.22080514\n","Iteration 28, loss = 0.22037445\n","Iteration 29, loss = 0.21995404\n","Iteration 30, loss = 0.21954854\n","Iteration 31, loss = 0.21917466\n","Iteration 32, loss = 0.21885704\n","Iteration 33, loss = 0.21848643\n","Iteration 34, loss = 0.21815020\n","Iteration 35, loss = 0.21784040\n","Iteration 36, loss = 0.21755201\n","Iteration 37, loss = 0.21736061\n","Iteration 38, loss = 0.21694656\n","Iteration 39, loss = 0.21669368\n","Iteration 40, loss = 0.21640147\n","Iteration 41, loss = 0.21612777\n","Iteration 42, loss = 0.21589667\n","Iteration 43, loss = 0.21562234\n","Iteration 44, loss = 0.21540086\n","Iteration 45, loss = 0.21509827\n","Iteration 46, loss = 0.21492353\n","Iteration 47, loss = 0.21468230\n","Iteration 48, loss = 0.21439936\n","Iteration 49, loss = 0.21424437\n","Iteration 50, loss = 0.21397048\n","Iteration 51, loss = 0.21375845\n","Iteration 52, loss = 0.21352497\n","Iteration 53, loss = 0.21329234\n","Iteration 54, loss = 0.21312276\n","Iteration 55, loss = 0.21290041\n","Iteration 56, loss = 0.21267608\n","Iteration 57, loss = 0.21248430\n","Iteration 58, loss = 0.21236861\n","Iteration 59, loss = 0.21204830\n","Iteration 60, loss = 0.21187925\n","Iteration 61, loss = 0.21166424\n","Iteration 62, loss = 0.21147273\n","Iteration 63, loss = 0.21128671\n","Iteration 64, loss = 0.21108914\n","Iteration 65, loss = 0.21092106\n","Iteration 66, loss = 0.21066880\n","Iteration 67, loss = 0.21047627\n","Iteration 68, loss = 0.21030267\n","Iteration 69, loss = 0.21014159\n","Iteration 70, loss = 0.20998170\n","Iteration 71, loss = 0.20971576\n","Iteration 72, loss = 0.20963030\n","Iteration 73, loss = 0.20942995\n","Iteration 74, loss = 0.20922102\n","Iteration 75, loss = 0.20901204\n","Iteration 76, loss = 0.20880868\n","Iteration 77, loss = 0.20866568\n","Iteration 78, loss = 0.20846445\n","Iteration 79, loss = 0.20833878\n","Iteration 80, loss = 0.20815059\n","Iteration 81, loss = 0.20795039\n","Iteration 82, loss = 0.20779500\n","Iteration 83, loss = 0.20758304\n","Iteration 84, loss = 0.20748085\n","Iteration 85, loss = 0.20729443\n","Iteration 86, loss = 0.20709326\n","Iteration 87, loss = 0.20693357\n","Iteration 88, loss = 0.20674534\n","Iteration 89, loss = 0.20657184\n","Iteration 90, loss = 0.20644449\n","Iteration 91, loss = 0.20623906\n","Iteration 92, loss = 0.20608209\n","Iteration 93, loss = 0.20593631\n","Iteration 94, loss = 0.20574441\n","Iteration 95, loss = 0.20560574\n","Iteration 96, loss = 0.20545309\n","Iteration 97, loss = 0.20524473\n","Iteration 98, loss = 0.20509278\n","Iteration 99, loss = 0.20492941\n","Iteration 100, loss = 0.20474952\n","Iteration 101, loss = 0.20459839\n","Iteration 102, loss = 0.20443896\n","Iteration 103, loss = 0.20429550\n","Iteration 104, loss = 0.20414128\n","Iteration 105, loss = 0.20395214\n","Iteration 106, loss = 0.20383937\n","Iteration 107, loss = 0.20369601\n","Iteration 108, loss = 0.20347736\n","Iteration 109, loss = 0.20336516\n","Iteration 110, loss = 0.20316769\n","Iteration 111, loss = 0.20303248\n","Iteration 112, loss = 0.20286914\n","Iteration 113, loss = 0.20272775\n","Iteration 114, loss = 0.20254908\n","Iteration 115, loss = 0.20238833\n","Iteration 116, loss = 0.20221554\n","Iteration 117, loss = 0.20205360\n","Iteration 118, loss = 0.20189999\n","Iteration 119, loss = 0.20181346\n","Iteration 120, loss = 0.20162227\n","Iteration 121, loss = 0.20146319\n","Iteration 122, loss = 0.20128406\n","Iteration 123, loss = 0.20119116\n","Iteration 124, loss = 0.20100383\n","Iteration 125, loss = 0.20084742\n","Iteration 126, loss = 0.20069154\n","Iteration 127, loss = 0.20053151\n","Iteration 128, loss = 0.20037584\n","Iteration 129, loss = 0.20025752\n","Iteration 130, loss = 0.20008990\n","Iteration 131, loss = 0.19991131\n","Iteration 132, loss = 0.19977218\n","Iteration 133, loss = 0.19961120\n","Iteration 134, loss = 0.19948323\n","Iteration 135, loss = 0.19933215\n","Iteration 136, loss = 0.19917758\n","Iteration 137, loss = 0.19901599\n","Iteration 138, loss = 0.19885141\n","Iteration 139, loss = 0.19871036\n","Iteration 140, loss = 0.19855562\n","Iteration 141, loss = 0.19845389\n","Iteration 142, loss = 0.19829199\n","Iteration 143, loss = 0.19813712\n","Iteration 144, loss = 0.19801820\n","Iteration 145, loss = 0.19784094\n","Iteration 146, loss = 0.19769537\n","Iteration 147, loss = 0.19748898\n","Iteration 148, loss = 0.19734029\n","Iteration 149, loss = 0.19721283\n","Iteration 150, loss = 0.19713088\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.53617026\n","Iteration 2, loss = 0.37781594\n","Iteration 3, loss = 0.31780247\n","Iteration 4, loss = 0.28851160\n","Iteration 5, loss = 0.27119110\n","Iteration 6, loss = 0.25982455\n","Iteration 7, loss = 0.25201525\n","Iteration 8, loss = 0.24622588\n","Iteration 9, loss = 0.24180055\n","Iteration 10, loss = 0.23834569\n","Iteration 11, loss = 0.23547699\n","Iteration 12, loss = 0.23310450\n","Iteration 13, loss = 0.23107895\n","Iteration 14, loss = 0.22932909\n","Iteration 15, loss = 0.22781365\n","Iteration 16, loss = 0.22644194\n","Iteration 17, loss = 0.22520968\n","Iteration 18, loss = 0.22411201\n","Iteration 19, loss = 0.22310338\n","Iteration 20, loss = 0.22218709\n","Iteration 21, loss = 0.22135637\n","Iteration 22, loss = 0.22053689\n","Iteration 23, loss = 0.21981597\n","Iteration 24, loss = 0.21915634\n","Iteration 25, loss = 0.21847888\n","Iteration 26, loss = 0.21784303\n","Iteration 27, loss = 0.21727515\n","Iteration 28, loss = 0.21671113\n","Iteration 29, loss = 0.21621978\n","Iteration 30, loss = 0.21567316\n","Iteration 31, loss = 0.21522566\n","Iteration 32, loss = 0.21473491\n","Iteration 33, loss = 0.21430495\n","Iteration 34, loss = 0.21386173\n","Iteration 35, loss = 0.21346031\n","Iteration 36, loss = 0.21307728\n","Iteration 37, loss = 0.21264027\n","Iteration 38, loss = 0.21228044\n","Iteration 39, loss = 0.21193155\n","Iteration 40, loss = 0.21157761\n","Iteration 41, loss = 0.21120610\n","Iteration 42, loss = 0.21088067\n","Iteration 43, loss = 0.21057540\n","Iteration 44, loss = 0.21024896\n","Iteration 45, loss = 0.20989108\n","Iteration 46, loss = 0.20958725\n","Iteration 47, loss = 0.20929428\n","Iteration 48, loss = 0.20897513\n","Iteration 49, loss = 0.20865091\n","Iteration 50, loss = 0.20838288\n","Iteration 51, loss = 0.20810068\n","Iteration 52, loss = 0.20785066\n","Iteration 53, loss = 0.20756327\n","Iteration 54, loss = 0.20723994\n","Iteration 55, loss = 0.20697879\n","Iteration 56, loss = 0.20671253\n","Iteration 57, loss = 0.20645497\n","Iteration 58, loss = 0.20620003\n","Iteration 59, loss = 0.20594496\n","Iteration 60, loss = 0.20568054\n","Iteration 61, loss = 0.20543634\n","Iteration 62, loss = 0.20514158\n","Iteration 63, loss = 0.20496212\n","Iteration 64, loss = 0.20471916\n","Iteration 65, loss = 0.20445401\n","Iteration 66, loss = 0.20420685\n","Iteration 67, loss = 0.20397607\n","Iteration 68, loss = 0.20375220\n","Iteration 69, loss = 0.20353219\n","Iteration 70, loss = 0.20328051\n","Iteration 71, loss = 0.20307669\n","Iteration 72, loss = 0.20285611\n","Iteration 73, loss = 0.20263604\n","Iteration 74, loss = 0.20236974\n","Iteration 75, loss = 0.20220973\n","Iteration 76, loss = 0.20194572\n","Iteration 77, loss = 0.20170240\n","Iteration 78, loss = 0.20148610\n","Iteration 79, loss = 0.20130158\n","Iteration 80, loss = 0.20102206\n","Iteration 81, loss = 0.20081289\n","Iteration 82, loss = 0.20061063\n","Iteration 83, loss = 0.20041715\n","Iteration 84, loss = 0.20019092\n","Iteration 85, loss = 0.19998295\n","Iteration 86, loss = 0.19979680\n","Iteration 87, loss = 0.19957301\n","Iteration 88, loss = 0.19934790\n","Iteration 89, loss = 0.19916176\n","Iteration 90, loss = 0.19897231\n","Iteration 91, loss = 0.19878811\n","Iteration 92, loss = 0.19858071\n","Iteration 93, loss = 0.19837752\n","Iteration 94, loss = 0.19818497\n","Iteration 95, loss = 0.19797630\n","Iteration 96, loss = 0.19774991\n","Iteration 97, loss = 0.19755920\n","Iteration 98, loss = 0.19735912\n","Iteration 99, loss = 0.19724748\n","Iteration 100, loss = 0.19699314\n","Iteration 101, loss = 0.19681384\n","Iteration 102, loss = 0.19664502\n","Iteration 103, loss = 0.19640413\n","Iteration 104, loss = 0.19619907\n","Iteration 105, loss = 0.19601395\n","Iteration 106, loss = 0.19583513\n","Iteration 107, loss = 0.19567221\n","Iteration 108, loss = 0.19544665\n","Iteration 109, loss = 0.19528473\n","Iteration 110, loss = 0.19507061\n","Iteration 111, loss = 0.19497196\n","Iteration 112, loss = 0.19472393\n","Iteration 113, loss = 0.19455587\n","Iteration 114, loss = 0.19435744\n","Iteration 115, loss = 0.19418860\n","Iteration 116, loss = 0.19401295\n","Iteration 117, loss = 0.19379463\n","Iteration 118, loss = 0.19362052\n","Iteration 119, loss = 0.19344839\n","Iteration 120, loss = 0.19327979\n","Iteration 121, loss = 0.19308629\n","Iteration 122, loss = 0.19290065\n","Iteration 123, loss = 0.19271701\n","Iteration 124, loss = 0.19253769\n","Iteration 125, loss = 0.19234567\n","Iteration 126, loss = 0.19222497\n","Iteration 127, loss = 0.19200978\n","Iteration 128, loss = 0.19192281\n","Iteration 129, loss = 0.19165383\n","Iteration 130, loss = 0.19152747\n","Iteration 131, loss = 0.19130667\n","Iteration 132, loss = 0.19112921\n","Iteration 133, loss = 0.19099731\n","Iteration 134, loss = 0.19081449\n","Iteration 135, loss = 0.19061796\n","Iteration 136, loss = 0.19044062\n","Iteration 137, loss = 0.19029143\n","Iteration 138, loss = 0.19012519\n","Iteration 139, loss = 0.18996764\n","Iteration 140, loss = 0.18982255\n","Iteration 141, loss = 0.18961669\n","Iteration 142, loss = 0.18949533\n","Iteration 143, loss = 0.18933850\n","Iteration 144, loss = 0.18908832\n","Iteration 145, loss = 0.18889777\n","Iteration 146, loss = 0.18875589\n","Iteration 147, loss = 0.18860631\n","Iteration 148, loss = 0.18845465\n","Iteration 149, loss = 0.18826184\n","Iteration 150, loss = 0.18812878\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.65242154\n","Iteration 2, loss = 0.46346930\n","Iteration 3, loss = 0.37498807\n","Iteration 4, loss = 0.32591516\n","Iteration 5, loss = 0.29605592\n","Iteration 6, loss = 0.27688636\n","Iteration 7, loss = 0.26425648\n","Iteration 8, loss = 0.25569347\n","Iteration 9, loss = 0.24951162\n","Iteration 10, loss = 0.24494955\n","Iteration 11, loss = 0.24139442\n","Iteration 12, loss = 0.23845462\n","Iteration 13, loss = 0.23607597\n","Iteration 14, loss = 0.23400743\n","Iteration 15, loss = 0.23223421\n","Iteration 16, loss = 0.23068416\n","Iteration 17, loss = 0.22932927\n","Iteration 18, loss = 0.22808503\n","Iteration 19, loss = 0.22701612\n","Iteration 20, loss = 0.22602412\n","Iteration 21, loss = 0.22515813\n","Iteration 22, loss = 0.22428478\n","Iteration 23, loss = 0.22349639\n","Iteration 24, loss = 0.22282083\n","Iteration 25, loss = 0.22214065\n","Iteration 26, loss = 0.22154812\n","Iteration 27, loss = 0.22101033\n","Iteration 28, loss = 0.22050946\n","Iteration 29, loss = 0.22000826\n","Iteration 30, loss = 0.21947760\n","Iteration 31, loss = 0.21902932\n","Iteration 32, loss = 0.21855475\n","Iteration 33, loss = 0.21817447\n","Iteration 34, loss = 0.21774999\n","Iteration 35, loss = 0.21741796\n","Iteration 36, loss = 0.21704746\n","Iteration 37, loss = 0.21666666\n","Iteration 38, loss = 0.21631227\n","Iteration 39, loss = 0.21603257\n","Iteration 40, loss = 0.21568557\n","Iteration 41, loss = 0.21537465\n","Iteration 42, loss = 0.21506183\n","Iteration 43, loss = 0.21472808\n","Iteration 44, loss = 0.21446508\n","Iteration 45, loss = 0.21415726\n","Iteration 46, loss = 0.21391367\n","Iteration 47, loss = 0.21361879\n","Iteration 48, loss = 0.21335743\n","Iteration 49, loss = 0.21309090\n","Iteration 50, loss = 0.21285351\n","Iteration 51, loss = 0.21259542\n","Iteration 52, loss = 0.21238275\n","Iteration 53, loss = 0.21212173\n","Iteration 54, loss = 0.21185698\n","Iteration 55, loss = 0.21167020\n","Iteration 56, loss = 0.21138576\n","Iteration 57, loss = 0.21123318\n","Iteration 58, loss = 0.21094896\n","Iteration 59, loss = 0.21079116\n","Iteration 60, loss = 0.21053698\n","Iteration 61, loss = 0.21033993\n","Iteration 62, loss = 0.21010945\n","Iteration 63, loss = 0.20990783\n","Iteration 64, loss = 0.20969135\n","Iteration 65, loss = 0.20951330\n","Iteration 66, loss = 0.20930117\n","Iteration 67, loss = 0.20910317\n","Iteration 68, loss = 0.20890995\n","Iteration 69, loss = 0.20869719\n","Iteration 70, loss = 0.20850728\n","Iteration 71, loss = 0.20831743\n","Iteration 72, loss = 0.20811826\n","Iteration 73, loss = 0.20797002\n","Iteration 74, loss = 0.20771877\n","Iteration 75, loss = 0.20754713\n","Iteration 76, loss = 0.20742001\n","Iteration 77, loss = 0.20719108\n","Iteration 78, loss = 0.20699277\n","Iteration 79, loss = 0.20683417\n","Iteration 80, loss = 0.20665482\n","Iteration 81, loss = 0.20649332\n","Iteration 82, loss = 0.20630162\n","Iteration 83, loss = 0.20611746\n","Iteration 84, loss = 0.20599142\n","Iteration 85, loss = 0.20579190\n","Iteration 86, loss = 0.20562172\n","Iteration 87, loss = 0.20548240\n","Iteration 88, loss = 0.20524805\n","Iteration 89, loss = 0.20510370\n","Iteration 90, loss = 0.20495124\n","Iteration 91, loss = 0.20477611\n","Iteration 92, loss = 0.20460206\n","Iteration 93, loss = 0.20443563\n","Iteration 94, loss = 0.20426569\n","Iteration 95, loss = 0.20413358\n","Iteration 96, loss = 0.20393965\n","Iteration 97, loss = 0.20376805\n","Iteration 98, loss = 0.20362962\n","Iteration 99, loss = 0.20346514\n","Iteration 100, loss = 0.20333492\n","Iteration 101, loss = 0.20321313\n","Iteration 102, loss = 0.20302381\n","Iteration 103, loss = 0.20295402\n","Iteration 104, loss = 0.20271784\n","Iteration 105, loss = 0.20256389\n","Iteration 106, loss = 0.20237596\n","Iteration 107, loss = 0.20225107\n","Iteration 108, loss = 0.20214578\n","Iteration 109, loss = 0.20194590\n","Iteration 110, loss = 0.20175478\n","Iteration 111, loss = 0.20159709\n","Iteration 112, loss = 0.20146614\n","Iteration 113, loss = 0.20128957\n","Iteration 114, loss = 0.20124131\n","Iteration 115, loss = 0.20106047\n","Iteration 116, loss = 0.20088531\n","Iteration 117, loss = 0.20071316\n","Iteration 118, loss = 0.20055968\n","Iteration 119, loss = 0.20043727\n","Iteration 120, loss = 0.20029906\n","Iteration 121, loss = 0.20012368\n","Iteration 122, loss = 0.20003896\n","Iteration 123, loss = 0.19989507\n","Iteration 124, loss = 0.19971437\n","Iteration 125, loss = 0.19954670\n","Iteration 126, loss = 0.19940211\n","Iteration 127, loss = 0.19931336\n","Iteration 128, loss = 0.19916495\n","Iteration 129, loss = 0.19903111\n","Iteration 130, loss = 0.19885450\n","Iteration 131, loss = 0.19875429\n","Iteration 132, loss = 0.19856330\n","Iteration 133, loss = 0.19842211\n","Iteration 134, loss = 0.19829923\n","Iteration 135, loss = 0.19813096\n","Iteration 136, loss = 0.19798287\n","Iteration 137, loss = 0.19788104\n","Iteration 138, loss = 0.19769574\n","Iteration 139, loss = 0.19762136\n","Iteration 140, loss = 0.19746709\n","Iteration 141, loss = 0.19731744\n","Iteration 142, loss = 0.19721753\n","Iteration 143, loss = 0.19699687\n","Iteration 144, loss = 0.19696924\n","Iteration 145, loss = 0.19676759\n","Iteration 146, loss = 0.19658879\n","Iteration 147, loss = 0.19650465\n","Iteration 148, loss = 0.19635057\n","Iteration 149, loss = 0.19622035\n","Iteration 150, loss = 0.19611388\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.47899696\n","Iteration 2, loss = 0.36424818\n","Iteration 3, loss = 0.31582899\n","Iteration 4, loss = 0.29049013\n","Iteration 5, loss = 0.27477976\n","Iteration 6, loss = 0.26390768\n","Iteration 7, loss = 0.25602852\n","Iteration 8, loss = 0.24994044\n","Iteration 9, loss = 0.24514679\n","Iteration 10, loss = 0.24124213\n","Iteration 11, loss = 0.23807635\n","Iteration 12, loss = 0.23530106\n","Iteration 13, loss = 0.23294418\n","Iteration 14, loss = 0.23099717\n","Iteration 15, loss = 0.22920151\n","Iteration 16, loss = 0.22771824\n","Iteration 17, loss = 0.22634113\n","Iteration 18, loss = 0.22518162\n","Iteration 19, loss = 0.22407822\n","Iteration 20, loss = 0.22313242\n","Iteration 21, loss = 0.22228734\n","Iteration 22, loss = 0.22145229\n","Iteration 23, loss = 0.22077998\n","Iteration 24, loss = 0.22009530\n","Iteration 25, loss = 0.21944725\n","Iteration 26, loss = 0.21890308\n","Iteration 27, loss = 0.21835284\n","Iteration 28, loss = 0.21784268\n","Iteration 29, loss = 0.21737952\n","Iteration 30, loss = 0.21693644\n","Iteration 31, loss = 0.21652117\n","Iteration 32, loss = 0.21613086\n","Iteration 33, loss = 0.21573599\n","Iteration 34, loss = 0.21536239\n","Iteration 35, loss = 0.21502235\n","Iteration 36, loss = 0.21470827\n","Iteration 37, loss = 0.21441552\n","Iteration 38, loss = 0.21405850\n","Iteration 39, loss = 0.21375120\n","Iteration 40, loss = 0.21346768\n","Iteration 41, loss = 0.21317838\n","Iteration 42, loss = 0.21287855\n","Iteration 43, loss = 0.21260742\n","Iteration 44, loss = 0.21235996\n","Iteration 45, loss = 0.21205633\n","Iteration 46, loss = 0.21185136\n","Iteration 47, loss = 0.21157324\n","Iteration 48, loss = 0.21132765\n","Iteration 49, loss = 0.21108880\n","Iteration 50, loss = 0.21086805\n","Iteration 51, loss = 0.21062941\n","Iteration 52, loss = 0.21039718\n","Iteration 53, loss = 0.21013349\n","Iteration 54, loss = 0.20995951\n","Iteration 55, loss = 0.20971642\n","Iteration 56, loss = 0.20949924\n","Iteration 57, loss = 0.20929357\n","Iteration 58, loss = 0.20906803\n","Iteration 59, loss = 0.20891613\n","Iteration 60, loss = 0.20870920\n","Iteration 61, loss = 0.20849935\n","Iteration 62, loss = 0.20831690\n","Iteration 63, loss = 0.20808416\n","Iteration 64, loss = 0.20788678\n","Iteration 65, loss = 0.20769165\n","Iteration 66, loss = 0.20755198\n","Iteration 67, loss = 0.20733352\n","Iteration 68, loss = 0.20713960\n","Iteration 69, loss = 0.20696782\n","Iteration 70, loss = 0.20675186\n","Iteration 71, loss = 0.20660746\n","Iteration 72, loss = 0.20642470\n","Iteration 73, loss = 0.20621302\n","Iteration 74, loss = 0.20606955\n","Iteration 75, loss = 0.20586028\n","Iteration 76, loss = 0.20567489\n","Iteration 77, loss = 0.20556048\n","Iteration 78, loss = 0.20531157\n","Iteration 79, loss = 0.20522332\n","Iteration 80, loss = 0.20502628\n","Iteration 81, loss = 0.20482487\n","Iteration 82, loss = 0.20469972\n","Iteration 83, loss = 0.20449019\n","Iteration 84, loss = 0.20433013\n","Iteration 85, loss = 0.20416546\n","Iteration 86, loss = 0.20400749\n","Iteration 87, loss = 0.20381017\n","Iteration 88, loss = 0.20369997\n","Iteration 89, loss = 0.20350764\n","Iteration 90, loss = 0.20336090\n","Iteration 91, loss = 0.20316496\n","Iteration 92, loss = 0.20302246\n","Iteration 93, loss = 0.20286120\n","Iteration 94, loss = 0.20273215\n","Iteration 95, loss = 0.20257146\n","Iteration 96, loss = 0.20240028\n","Iteration 97, loss = 0.20224728\n","Iteration 98, loss = 0.20207668\n","Iteration 99, loss = 0.20199608\n","Iteration 100, loss = 0.20178148\n","Iteration 101, loss = 0.20167206\n","Iteration 102, loss = 0.20146181\n","Iteration 103, loss = 0.20136638\n","Iteration 104, loss = 0.20118926\n","Iteration 105, loss = 0.20103815\n","Iteration 106, loss = 0.20086393\n","Iteration 107, loss = 0.20075778\n","Iteration 108, loss = 0.20055052\n","Iteration 109, loss = 0.20041903\n","Iteration 110, loss = 0.20029679\n","Iteration 111, loss = 0.20016061\n","Iteration 112, loss = 0.19999213\n","Iteration 113, loss = 0.19983743\n","Iteration 114, loss = 0.19969120\n","Iteration 115, loss = 0.19956882\n","Iteration 116, loss = 0.19937937\n","Iteration 117, loss = 0.19926826\n","Iteration 118, loss = 0.19910602\n","Iteration 119, loss = 0.19896636\n","Iteration 120, loss = 0.19882626\n","Iteration 121, loss = 0.19866563\n","Iteration 122, loss = 0.19851928\n","Iteration 123, loss = 0.19837501\n","Iteration 124, loss = 0.19821697\n","Iteration 125, loss = 0.19811065\n","Iteration 126, loss = 0.19799249\n","Iteration 127, loss = 0.19778665\n","Iteration 128, loss = 0.19769785\n","Iteration 129, loss = 0.19759176\n","Iteration 130, loss = 0.19741271\n","Iteration 131, loss = 0.19725740\n","Iteration 132, loss = 0.19712596\n","Iteration 133, loss = 0.19702964\n","Iteration 134, loss = 0.19680068\n","Iteration 135, loss = 0.19665961\n","Iteration 136, loss = 0.19655406\n","Iteration 137, loss = 0.19637606\n","Iteration 138, loss = 0.19625834\n","Iteration 139, loss = 0.19613966\n","Iteration 140, loss = 0.19602430\n","Iteration 141, loss = 0.19581800\n","Iteration 142, loss = 0.19569826\n","Iteration 143, loss = 0.19555102\n","Iteration 144, loss = 0.19544424\n","Iteration 145, loss = 0.19530559\n","Iteration 146, loss = 0.19512231\n","Iteration 147, loss = 0.19502707\n","Iteration 148, loss = 0.19486517\n","Iteration 149, loss = 0.19469520\n","Iteration 150, loss = 0.19460158\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.34478717\n","Iteration 2, loss = 0.23657111\n","Iteration 3, loss = 0.22410231\n","Iteration 4, loss = 0.21746014\n","Iteration 5, loss = 0.21351117\n","Iteration 6, loss = 0.21034238\n","Iteration 7, loss = 0.20699508\n","Iteration 8, loss = 0.20401180\n","Iteration 9, loss = 0.20079414\n","Iteration 10, loss = 0.19796711\n","Iteration 11, loss = 0.19515453\n","Iteration 12, loss = 0.19154987\n","Iteration 13, loss = 0.18956347\n","Iteration 14, loss = 0.18733123\n","Iteration 15, loss = 0.18330314\n","Iteration 16, loss = 0.18143798\n","Iteration 17, loss = 0.17816096\n","Iteration 18, loss = 0.17479262\n","Iteration 19, loss = 0.17230592\n","Iteration 20, loss = 0.16938199\n","Iteration 21, loss = 0.16662496\n","Iteration 22, loss = 0.16423944\n","Iteration 23, loss = 0.16047330\n","Iteration 24, loss = 0.15815202\n","Iteration 25, loss = 0.15361613\n","Iteration 26, loss = 0.15126576\n","Iteration 27, loss = 0.14860542\n","Iteration 28, loss = 0.14440951\n","Iteration 29, loss = 0.14096001\n","Iteration 30, loss = 0.13818533\n","Iteration 31, loss = 0.13581354\n","Iteration 32, loss = 0.13299801\n","Iteration 33, loss = 0.12862879\n","Iteration 34, loss = 0.12643444\n","Iteration 35, loss = 0.12296798\n","Iteration 36, loss = 0.11878096\n","Iteration 37, loss = 0.11506171\n","Iteration 38, loss = 0.11240751\n","Iteration 39, loss = 0.10922791\n","Iteration 40, loss = 0.10560103\n","Iteration 41, loss = 0.10229964\n","Iteration 42, loss = 0.09972848\n","Iteration 43, loss = 0.09688653\n","Iteration 44, loss = 0.09317210\n","Iteration 45, loss = 0.09290933\n","Iteration 46, loss = 0.08899249\n","Iteration 47, loss = 0.08497813\n","Iteration 48, loss = 0.08047139\n","Iteration 49, loss = 0.07845824\n","Iteration 50, loss = 0.07487337\n","Iteration 51, loss = 0.07232350\n","Iteration 52, loss = 0.07235781\n","Iteration 53, loss = 0.06743193\n","Iteration 54, loss = 0.06509024\n","Iteration 55, loss = 0.06407736\n","Iteration 56, loss = 0.06002211\n","Iteration 57, loss = 0.05871426\n","Iteration 58, loss = 0.05666572\n","Iteration 59, loss = 0.05450012\n","Iteration 60, loss = 0.05242189\n","Iteration 61, loss = 0.04935835\n","Iteration 62, loss = 0.04771535\n","Iteration 63, loss = 0.04598206\n","Iteration 64, loss = 0.04560947\n","Iteration 65, loss = 0.04292874\n","Iteration 66, loss = 0.03927931\n","Iteration 67, loss = 0.03952879\n","Iteration 68, loss = 0.03914723\n","Iteration 69, loss = 0.03592805\n","Iteration 70, loss = 0.03521286\n","Iteration 71, loss = 0.03404281\n","Iteration 72, loss = 0.03119441\n","Iteration 73, loss = 0.02952952\n","Iteration 74, loss = 0.02881694\n","Iteration 75, loss = 0.02935757\n","Iteration 76, loss = 0.02610995\n","Iteration 77, loss = 0.02623152\n","Iteration 78, loss = 0.02586252\n","Iteration 79, loss = 0.02527829\n","Iteration 80, loss = 0.02174721\n","Iteration 81, loss = 0.02146980\n","Iteration 82, loss = 0.02123755\n","Iteration 83, loss = 0.01971680\n","Iteration 84, loss = 0.01839564\n","Iteration 85, loss = 0.01832271\n","Iteration 86, loss = 0.01707883\n","Iteration 87, loss = 0.01685016\n","Iteration 88, loss = 0.01602119\n","Iteration 89, loss = 0.01624282\n","Iteration 90, loss = 0.01510605\n","Iteration 91, loss = 0.01411148\n","Iteration 92, loss = 0.01354858\n","Iteration 93, loss = 0.01299651\n","Iteration 94, loss = 0.01252583\n","Iteration 95, loss = 0.01215810\n","Iteration 96, loss = 0.01223818\n","Iteration 97, loss = 0.01209767\n","Iteration 98, loss = 0.01113267\n","Iteration 99, loss = 0.01060940\n","Iteration 100, loss = 0.01031641\n","Iteration 101, loss = 0.01001389\n","Iteration 102, loss = 0.00972849\n","Iteration 103, loss = 0.00934204\n","Iteration 104, loss = 0.00917586\n","Iteration 105, loss = 0.00913478\n","Iteration 106, loss = 0.00870576\n","Iteration 107, loss = 0.00843561\n","Iteration 108, loss = 0.00823736\n","Iteration 109, loss = 0.00803200\n","Iteration 110, loss = 0.00789923\n","Iteration 111, loss = 0.00758726\n","Iteration 112, loss = 0.00757929\n","Iteration 113, loss = 0.00730676\n","Iteration 114, loss = 0.00715728\n","Iteration 115, loss = 0.00703964\n","Iteration 116, loss = 0.00693875\n","Iteration 117, loss = 0.00682359\n","Iteration 118, loss = 0.00662922\n","Iteration 119, loss = 0.00646523\n","Iteration 120, loss = 0.00646984\n","Iteration 121, loss = 0.00622755\n","Iteration 122, loss = 0.00614211\n","Iteration 123, loss = 0.00604123\n","Iteration 124, loss = 0.00596972\n","Iteration 125, loss = 0.00592744\n","Iteration 126, loss = 0.00580591\n","Iteration 127, loss = 0.00569950\n","Iteration 128, loss = 0.00562519\n","Iteration 129, loss = 0.00555539\n","Iteration 130, loss = 0.00547038\n","Iteration 131, loss = 0.00542663\n","Iteration 132, loss = 0.00536933\n","Iteration 133, loss = 0.00525576\n","Iteration 134, loss = 0.00521806\n","Iteration 135, loss = 0.00513441\n","Iteration 136, loss = 0.00512581\n","Iteration 137, loss = 0.00505174\n","Iteration 138, loss = 0.00501679\n","Iteration 139, loss = 0.00496445\n","Iteration 140, loss = 0.00492275\n","Iteration 141, loss = 0.00493813\n","Iteration 142, loss = 0.00482482\n","Iteration 143, loss = 0.00479110\n","Iteration 144, loss = 0.00475941\n","Iteration 145, loss = 0.00472753\n","Iteration 146, loss = 0.00469840\n","Iteration 147, loss = 0.00465274\n","Iteration 148, loss = 0.00461947\n","Iteration 149, loss = 0.00458349\n","Iteration 150, loss = 0.00455660\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.42381723\n","Iteration 2, loss = 0.24256235\n","Iteration 3, loss = 0.22600072\n","Iteration 4, loss = 0.22028562\n","Iteration 5, loss = 0.21605668\n","Iteration 6, loss = 0.21224177\n","Iteration 7, loss = 0.21006433\n","Iteration 8, loss = 0.20705479\n","Iteration 9, loss = 0.20289634\n","Iteration 10, loss = 0.20017540\n","Iteration 11, loss = 0.19699242\n","Iteration 12, loss = 0.19522145\n","Iteration 13, loss = 0.19251105\n","Iteration 14, loss = 0.19057967\n","Iteration 15, loss = 0.18794611\n","Iteration 16, loss = 0.18524960\n","Iteration 17, loss = 0.18223392\n","Iteration 18, loss = 0.17969375\n","Iteration 19, loss = 0.17644477\n","Iteration 20, loss = 0.17460505\n","Iteration 21, loss = 0.17148172\n","Iteration 22, loss = 0.16882711\n","Iteration 23, loss = 0.16506926\n","Iteration 24, loss = 0.16425982\n","Iteration 25, loss = 0.16005629\n","Iteration 26, loss = 0.15715508\n","Iteration 27, loss = 0.15400679\n","Iteration 28, loss = 0.15075565\n","Iteration 29, loss = 0.14802114\n","Iteration 30, loss = 0.14404051\n","Iteration 31, loss = 0.14019540\n","Iteration 32, loss = 0.13650289\n","Iteration 33, loss = 0.13351763\n","Iteration 34, loss = 0.13041371\n","Iteration 35, loss = 0.12658813\n","Iteration 36, loss = 0.12359720\n","Iteration 37, loss = 0.11874282\n","Iteration 38, loss = 0.11690806\n","Iteration 39, loss = 0.11273955\n","Iteration 40, loss = 0.10953789\n","Iteration 41, loss = 0.10685292\n","Iteration 42, loss = 0.10200825\n","Iteration 43, loss = 0.09859860\n","Iteration 44, loss = 0.09779502\n","Iteration 45, loss = 0.09589226\n","Iteration 46, loss = 0.09288992\n","Iteration 47, loss = 0.09018865\n","Iteration 48, loss = 0.08581892\n","Iteration 49, loss = 0.08082320\n","Iteration 50, loss = 0.07875166\n","Iteration 51, loss = 0.07747263\n","Iteration 52, loss = 0.07411875\n","Iteration 53, loss = 0.07066829\n","Iteration 54, loss = 0.06759092\n","Iteration 55, loss = 0.06510550\n","Iteration 56, loss = 0.06434656\n","Iteration 57, loss = 0.06164376\n","Iteration 58, loss = 0.05864143\n","Iteration 59, loss = 0.05531642\n","Iteration 60, loss = 0.05323486\n","Iteration 61, loss = 0.05343950\n","Iteration 62, loss = 0.05048955\n","Iteration 63, loss = 0.04885843\n","Iteration 64, loss = 0.04583511\n","Iteration 65, loss = 0.04478355\n","Iteration 66, loss = 0.04252397\n","Iteration 67, loss = 0.04007469\n","Iteration 68, loss = 0.03868374\n","Iteration 69, loss = 0.03737916\n","Iteration 70, loss = 0.03589861\n","Iteration 71, loss = 0.03434204\n","Iteration 72, loss = 0.03146074\n","Iteration 73, loss = 0.03129562\n","Iteration 74, loss = 0.03003963\n","Iteration 75, loss = 0.02836753\n","Iteration 76, loss = 0.02655681\n","Iteration 77, loss = 0.02506333\n","Iteration 78, loss = 0.02414972\n","Iteration 79, loss = 0.02411729\n","Iteration 80, loss = 0.02243574\n","Iteration 81, loss = 0.02066375\n","Iteration 82, loss = 0.01924370\n","Iteration 83, loss = 0.01830394\n","Iteration 84, loss = 0.01725862\n","Iteration 85, loss = 0.01623219\n","Iteration 86, loss = 0.01610974\n","Iteration 87, loss = 0.01576840\n","Iteration 88, loss = 0.01488517\n","Iteration 89, loss = 0.01403960\n","Iteration 90, loss = 0.01334293\n","Iteration 91, loss = 0.01309117\n","Iteration 92, loss = 0.01295217\n","Iteration 93, loss = 0.01173442\n","Iteration 94, loss = 0.01117737\n","Iteration 95, loss = 0.01157988\n","Iteration 96, loss = 0.01056993\n","Iteration 97, loss = 0.01027908\n","Iteration 98, loss = 0.00988451\n","Iteration 99, loss = 0.00935813\n","Iteration 100, loss = 0.00921196\n","Iteration 101, loss = 0.00883339\n","Iteration 102, loss = 0.00849864\n","Iteration 103, loss = 0.00824209\n","Iteration 104, loss = 0.00824764\n","Iteration 105, loss = 0.00784160\n","Iteration 106, loss = 0.00774003\n","Iteration 107, loss = 0.00741317\n","Iteration 108, loss = 0.00720151\n","Iteration 109, loss = 0.00705326\n","Iteration 110, loss = 0.00703609\n","Iteration 111, loss = 0.00675217\n","Iteration 112, loss = 0.00666188\n","Iteration 113, loss = 0.00658433\n","Iteration 114, loss = 0.00626895\n","Iteration 115, loss = 0.00624621\n","Iteration 116, loss = 0.00601366\n","Iteration 117, loss = 0.00592792\n","Iteration 118, loss = 0.00584978\n","Iteration 119, loss = 0.00575885\n","Iteration 120, loss = 0.00573917\n","Iteration 121, loss = 0.00566681\n","Iteration 122, loss = 0.00546938\n","Iteration 123, loss = 0.00539613\n","Iteration 124, loss = 0.00531082\n","Iteration 125, loss = 0.00527131\n","Iteration 126, loss = 0.00516924\n","Iteration 127, loss = 0.00509705\n","Iteration 128, loss = 0.00508437\n","Iteration 129, loss = 0.00510223\n","Iteration 130, loss = 0.00494177\n","Iteration 131, loss = 0.00487265\n","Iteration 132, loss = 0.00483937\n","Iteration 133, loss = 0.00477980\n","Iteration 134, loss = 0.00471435\n","Iteration 135, loss = 0.00470115\n","Iteration 136, loss = 0.00469005\n","Iteration 137, loss = 0.00462738\n","Iteration 138, loss = 0.00459793\n","Iteration 139, loss = 0.00455430\n","Iteration 140, loss = 0.00455636\n","Iteration 141, loss = 0.00448224\n","Iteration 142, loss = 0.00447427\n","Iteration 143, loss = 0.00446082\n","Iteration 144, loss = 0.00443931\n","Iteration 145, loss = 0.00439213\n","Iteration 146, loss = 0.00435413\n","Iteration 147, loss = 0.00433900\n","Iteration 148, loss = 0.00430030\n","Iteration 149, loss = 0.00427227\n","Iteration 150, loss = 0.00426526\n","Iteration 1, loss = 0.37658236\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.23849525\n","Iteration 3, loss = 0.22211157\n","Iteration 4, loss = 0.21486861\n","Iteration 5, loss = 0.21017403\n","Iteration 6, loss = 0.20647249\n","Iteration 7, loss = 0.20258062\n","Iteration 8, loss = 0.19958643\n","Iteration 9, loss = 0.19687048\n","Iteration 10, loss = 0.19428852\n","Iteration 11, loss = 0.19187909\n","Iteration 12, loss = 0.18960330\n","Iteration 13, loss = 0.18583573\n","Iteration 14, loss = 0.18342035\n","Iteration 15, loss = 0.18135968\n","Iteration 16, loss = 0.17907770\n","Iteration 17, loss = 0.17669929\n","Iteration 18, loss = 0.17330801\n","Iteration 19, loss = 0.17100441\n","Iteration 20, loss = 0.16901946\n","Iteration 21, loss = 0.16645231\n","Iteration 22, loss = 0.16462456\n","Iteration 23, loss = 0.16154410\n","Iteration 24, loss = 0.15833548\n","Iteration 25, loss = 0.15502424\n","Iteration 26, loss = 0.15237018\n","Iteration 27, loss = 0.14894649\n","Iteration 28, loss = 0.14585018\n","Iteration 29, loss = 0.14213512\n","Iteration 30, loss = 0.14020992\n","Iteration 31, loss = 0.13646336\n","Iteration 32, loss = 0.13569157\n","Iteration 33, loss = 0.13160868\n","Iteration 34, loss = 0.12718157\n","Iteration 35, loss = 0.12357305\n","Iteration 36, loss = 0.12114931\n","Iteration 37, loss = 0.11862628\n","Iteration 38, loss = 0.11398180\n","Iteration 39, loss = 0.11153063\n","Iteration 40, loss = 0.10901066\n","Iteration 41, loss = 0.10663464\n","Iteration 42, loss = 0.10126832\n","Iteration 43, loss = 0.09856705\n","Iteration 44, loss = 0.09460927\n","Iteration 45, loss = 0.09234995\n","Iteration 46, loss = 0.08872160\n","Iteration 47, loss = 0.08804441\n","Iteration 48, loss = 0.08460819\n","Iteration 49, loss = 0.08084344\n","Iteration 50, loss = 0.07796101\n","Iteration 51, loss = 0.07507220\n","Iteration 52, loss = 0.07445429\n","Iteration 53, loss = 0.07181780\n","Iteration 54, loss = 0.06795114\n","Iteration 55, loss = 0.06563491\n","Iteration 56, loss = 0.06456637\n","Iteration 57, loss = 0.06132007\n","Iteration 58, loss = 0.05962058\n","Iteration 59, loss = 0.05593582\n","Iteration 60, loss = 0.05466272\n","Iteration 61, loss = 0.05374677\n","Iteration 62, loss = 0.05055948\n","Iteration 63, loss = 0.04801093\n","Iteration 64, loss = 0.04656064\n","Iteration 65, loss = 0.04471495\n","Iteration 66, loss = 0.04224130\n","Iteration 67, loss = 0.04091315\n","Iteration 68, loss = 0.03929993\n","Iteration 69, loss = 0.03799733\n","Iteration 70, loss = 0.03790420\n","Iteration 71, loss = 0.03562598\n","Iteration 72, loss = 0.03414725\n","Iteration 73, loss = 0.03225190\n","Iteration 74, loss = 0.03124026\n","Iteration 75, loss = 0.03012976\n","Iteration 76, loss = 0.02813259\n","Iteration 77, loss = 0.02714040\n","Iteration 78, loss = 0.02598111\n","Iteration 79, loss = 0.02511012\n","Iteration 80, loss = 0.02406946\n","Iteration 81, loss = 0.02346791\n","Iteration 82, loss = 0.02174675\n","Iteration 83, loss = 0.02134372\n","Iteration 84, loss = 0.02035784\n","Iteration 85, loss = 0.01937530\n","Iteration 86, loss = 0.01860141\n","Iteration 87, loss = 0.01796171\n","Iteration 88, loss = 0.01766282\n","Iteration 89, loss = 0.01687276\n","Iteration 90, loss = 0.01614569\n","Iteration 91, loss = 0.01536263\n","Iteration 92, loss = 0.01488001\n","Iteration 93, loss = 0.01405244\n","Iteration 94, loss = 0.01353201\n","Iteration 95, loss = 0.01373843\n","Iteration 96, loss = 0.01281293\n","Iteration 97, loss = 0.01225716\n","Iteration 98, loss = 0.01189011\n","Iteration 99, loss = 0.01162358\n","Iteration 100, loss = 0.01157331\n","Iteration 101, loss = 0.01109904\n","Iteration 102, loss = 0.01089589\n","Iteration 103, loss = 0.01012183\n","Iteration 104, loss = 0.00977463\n","Iteration 105, loss = 0.00964025\n","Iteration 106, loss = 0.00909482\n","Iteration 107, loss = 0.00923370\n","Iteration 108, loss = 0.00868717\n","Iteration 109, loss = 0.00853315\n","Iteration 110, loss = 0.00877882\n","Iteration 111, loss = 0.00808036\n","Iteration 112, loss = 0.00788876\n","Iteration 113, loss = 0.00767684\n","Iteration 114, loss = 0.00752134\n","Iteration 115, loss = 0.00732350\n","Iteration 116, loss = 0.00715455\n","Iteration 117, loss = 0.00698666\n","Iteration 118, loss = 0.00689004\n","Iteration 119, loss = 0.00672180\n","Iteration 120, loss = 0.00662883\n","Iteration 121, loss = 0.00648595\n","Iteration 122, loss = 0.00639197\n","Iteration 123, loss = 0.00631991\n","Iteration 124, loss = 0.00616756\n","Iteration 125, loss = 0.00609037\n","Iteration 126, loss = 0.00615120\n","Iteration 127, loss = 0.00589845\n","Iteration 128, loss = 0.00583553\n","Iteration 129, loss = 0.00567217\n","Iteration 130, loss = 0.00569528\n","Iteration 131, loss = 0.00556670\n","Iteration 132, loss = 0.00547414\n","Iteration 133, loss = 0.00541761\n","Iteration 134, loss = 0.00534224\n","Iteration 135, loss = 0.00532943\n","Iteration 136, loss = 0.00521423\n","Iteration 137, loss = 0.00513696\n","Iteration 138, loss = 0.00511861\n","Iteration 139, loss = 0.00505954\n","Iteration 140, loss = 0.00503431\n","Iteration 141, loss = 0.00494118\n","Iteration 142, loss = 0.00492715\n","Iteration 143, loss = 0.00485195\n","Iteration 144, loss = 0.00479452\n","Iteration 145, loss = 0.00481776\n","Iteration 146, loss = 0.00475525\n","Iteration 147, loss = 0.00473477\n","Iteration 148, loss = 0.00472366\n","Iteration 149, loss = 0.00468017\n","Iteration 150, loss = 0.00470491\n","Iteration 1, loss = 0.34549380\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.22740276\n","Iteration 3, loss = 0.21813868\n","Iteration 4, loss = 0.21271556\n","Iteration 5, loss = 0.20874968\n","Iteration 6, loss = 0.20461430\n","Iteration 7, loss = 0.20193859\n","Iteration 8, loss = 0.19896349\n","Iteration 9, loss = 0.19583820\n","Iteration 10, loss = 0.19385008\n","Iteration 11, loss = 0.19041089\n","Iteration 12, loss = 0.18832184\n","Iteration 13, loss = 0.18557889\n","Iteration 14, loss = 0.18249395\n","Iteration 15, loss = 0.18028088\n","Iteration 16, loss = 0.17674270\n","Iteration 17, loss = 0.17486215\n","Iteration 18, loss = 0.17227943\n","Iteration 19, loss = 0.16945699\n","Iteration 20, loss = 0.16569766\n","Iteration 21, loss = 0.16265467\n","Iteration 22, loss = 0.15978080\n","Iteration 23, loss = 0.15647941\n","Iteration 24, loss = 0.15443619\n","Iteration 25, loss = 0.15019503\n","Iteration 26, loss = 0.14761164\n","Iteration 27, loss = 0.14502014\n","Iteration 28, loss = 0.14103305\n","Iteration 29, loss = 0.13837364\n","Iteration 30, loss = 0.13495546\n","Iteration 31, loss = 0.13087070\n","Iteration 32, loss = 0.12797288\n","Iteration 33, loss = 0.12483590\n","Iteration 34, loss = 0.12115872\n","Iteration 35, loss = 0.11824388\n","Iteration 36, loss = 0.11616809\n","Iteration 37, loss = 0.11326553\n","Iteration 38, loss = 0.10930482\n","Iteration 39, loss = 0.10689615\n","Iteration 40, loss = 0.10236498\n","Iteration 41, loss = 0.10002476\n","Iteration 42, loss = 0.09880604\n","Iteration 43, loss = 0.09418335\n","Iteration 44, loss = 0.09111767\n","Iteration 45, loss = 0.08863825\n","Iteration 46, loss = 0.08620240\n","Iteration 47, loss = 0.08275006\n","Iteration 48, loss = 0.07875806\n","Iteration 49, loss = 0.07738472\n","Iteration 50, loss = 0.07507675\n","Iteration 51, loss = 0.07135678\n","Iteration 52, loss = 0.06807326\n","Iteration 53, loss = 0.06693721\n","Iteration 54, loss = 0.06625394\n","Iteration 55, loss = 0.06145806\n","Iteration 56, loss = 0.05794373\n","Iteration 57, loss = 0.05645277\n","Iteration 58, loss = 0.05382891\n","Iteration 59, loss = 0.05162385\n","Iteration 60, loss = 0.05061444\n","Iteration 61, loss = 0.04725235\n","Iteration 62, loss = 0.04651813\n","Iteration 63, loss = 0.04496241\n","Iteration 64, loss = 0.04325455\n","Iteration 65, loss = 0.04076850\n","Iteration 66, loss = 0.03918910\n","Iteration 67, loss = 0.03763932\n","Iteration 68, loss = 0.03552640\n","Iteration 69, loss = 0.03448381\n","Iteration 70, loss = 0.03378099\n","Iteration 71, loss = 0.03113257\n","Iteration 72, loss = 0.03017776\n","Iteration 73, loss = 0.02932038\n","Iteration 74, loss = 0.02813922\n","Iteration 75, loss = 0.02677238\n","Iteration 76, loss = 0.02539297\n","Iteration 77, loss = 0.02455817\n","Iteration 78, loss = 0.02269983\n","Iteration 79, loss = 0.02189428\n","Iteration 80, loss = 0.02166352\n","Iteration 81, loss = 0.02066393\n","Iteration 82, loss = 0.01921616\n","Iteration 83, loss = 0.01801107\n","Iteration 84, loss = 0.01720113\n","Iteration 85, loss = 0.01624453\n","Iteration 86, loss = 0.01647717\n","Iteration 87, loss = 0.01565966\n","Iteration 88, loss = 0.01473597\n","Iteration 89, loss = 0.01416328\n","Iteration 90, loss = 0.01318210\n","Iteration 91, loss = 0.01266656\n","Iteration 92, loss = 0.01261652\n","Iteration 93, loss = 0.01223858\n","Iteration 94, loss = 0.01145275\n","Iteration 95, loss = 0.01141988\n","Iteration 96, loss = 0.01102973\n","Iteration 97, loss = 0.01060035\n","Iteration 98, loss = 0.01035623\n","Iteration 99, loss = 0.00987590\n","Iteration 100, loss = 0.00943244\n","Iteration 101, loss = 0.00915171\n","Iteration 102, loss = 0.00874386\n","Iteration 103, loss = 0.00859619\n","Iteration 104, loss = 0.00867318\n","Iteration 105, loss = 0.00822768\n","Iteration 106, loss = 0.00806347\n","Iteration 107, loss = 0.00763951\n","Iteration 108, loss = 0.00743788\n","Iteration 109, loss = 0.00749190\n","Iteration 110, loss = 0.00731126\n","Iteration 111, loss = 0.00699936\n","Iteration 112, loss = 0.00703281\n","Iteration 113, loss = 0.00664084\n","Iteration 114, loss = 0.00659128\n","Iteration 115, loss = 0.00639686\n","Iteration 116, loss = 0.00636032\n","Iteration 117, loss = 0.00610161\n","Iteration 118, loss = 0.00613347\n","Iteration 119, loss = 0.00615255\n","Iteration 120, loss = 0.00581342\n","Iteration 121, loss = 0.00570674\n","Iteration 122, loss = 0.00568689\n","Iteration 123, loss = 0.00559590\n","Iteration 124, loss = 0.00551718\n","Iteration 125, loss = 0.00539593\n","Iteration 126, loss = 0.00526957\n","Iteration 127, loss = 0.00535312\n","Iteration 128, loss = 0.00519912\n","Iteration 129, loss = 0.00513181\n","Iteration 130, loss = 0.00514431\n","Iteration 131, loss = 0.00505543\n","Iteration 132, loss = 0.00497726\n","Iteration 133, loss = 0.00495758\n","Iteration 134, loss = 0.00484400\n","Iteration 135, loss = 0.00481845\n","Iteration 136, loss = 0.00477780\n","Iteration 137, loss = 0.00470981\n","Iteration 138, loss = 0.00468563\n","Iteration 139, loss = 0.00461399\n","Iteration 140, loss = 0.00459168\n","Iteration 141, loss = 0.00458849\n","Iteration 142, loss = 0.00455336\n","Iteration 143, loss = 0.00457529\n","Iteration 144, loss = 0.00448026\n","Iteration 145, loss = 0.00444165\n","Iteration 146, loss = 0.00442928\n","Iteration 147, loss = 0.00439084\n","Iteration 148, loss = 0.00435886\n","Iteration 149, loss = 0.00434406\n","Iteration 150, loss = 0.00430083\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.35017000\n","Iteration 2, loss = 0.22575628\n","Iteration 3, loss = 0.21708972\n","Iteration 4, loss = 0.21265347\n","Iteration 5, loss = 0.20813570\n","Iteration 6, loss = 0.20529748\n","Iteration 7, loss = 0.20186036\n","Iteration 8, loss = 0.19878264\n","Iteration 9, loss = 0.19746751\n","Iteration 10, loss = 0.19431027\n","Iteration 11, loss = 0.19145496\n","Iteration 12, loss = 0.18836206\n","Iteration 13, loss = 0.18655933\n","Iteration 14, loss = 0.18484556\n","Iteration 15, loss = 0.18050168\n","Iteration 16, loss = 0.17741049\n","Iteration 17, loss = 0.17603017\n","Iteration 18, loss = 0.17212262\n","Iteration 19, loss = 0.17067290\n","Iteration 20, loss = 0.16657719\n","Iteration 21, loss = 0.16351945\n","Iteration 22, loss = 0.16108217\n","Iteration 23, loss = 0.15766450\n","Iteration 24, loss = 0.15385662\n","Iteration 25, loss = 0.15007276\n","Iteration 26, loss = 0.14755856\n","Iteration 27, loss = 0.14472690\n","Iteration 28, loss = 0.14193972\n","Iteration 29, loss = 0.13849768\n","Iteration 30, loss = 0.13421875\n","Iteration 31, loss = 0.13052983\n","Iteration 32, loss = 0.12806439\n","Iteration 33, loss = 0.12417333\n","Iteration 34, loss = 0.12144301\n","Iteration 35, loss = 0.11828931\n","Iteration 36, loss = 0.11457391\n","Iteration 37, loss = 0.11029400\n","Iteration 38, loss = 0.10617514\n","Iteration 39, loss = 0.10404710\n","Iteration 40, loss = 0.10106812\n","Iteration 41, loss = 0.09735259\n","Iteration 42, loss = 0.09396315\n","Iteration 43, loss = 0.09046124\n","Iteration 44, loss = 0.08731570\n","Iteration 45, loss = 0.08600504\n","Iteration 46, loss = 0.08238583\n","Iteration 47, loss = 0.08007756\n","Iteration 48, loss = 0.07451430\n","Iteration 49, loss = 0.07300297\n","Iteration 50, loss = 0.06873141\n","Iteration 51, loss = 0.06726087\n","Iteration 52, loss = 0.06398895\n","Iteration 53, loss = 0.06196908\n","Iteration 54, loss = 0.05800321\n","Iteration 55, loss = 0.05638230\n","Iteration 56, loss = 0.05308594\n","Iteration 57, loss = 0.05184499\n","Iteration 58, loss = 0.05104376\n","Iteration 59, loss = 0.04796704\n","Iteration 60, loss = 0.04561788\n","Iteration 61, loss = 0.04285434\n","Iteration 62, loss = 0.04116671\n","Iteration 63, loss = 0.03962111\n","Iteration 64, loss = 0.03754548\n","Iteration 65, loss = 0.03542729\n","Iteration 66, loss = 0.03397161\n","Iteration 67, loss = 0.03204719\n","Iteration 68, loss = 0.03012574\n","Iteration 69, loss = 0.02898802\n","Iteration 70, loss = 0.03169501\n","Iteration 71, loss = 0.02752352\n","Iteration 72, loss = 0.02514777\n","Iteration 73, loss = 0.02308297\n","Iteration 74, loss = 0.02235260\n","Iteration 75, loss = 0.02188768\n","Iteration 76, loss = 0.02031374\n","Iteration 77, loss = 0.01881889\n","Iteration 78, loss = 0.01844892\n","Iteration 79, loss = 0.01782706\n","Iteration 80, loss = 0.01690051\n","Iteration 81, loss = 0.01621877\n","Iteration 82, loss = 0.01571652\n","Iteration 83, loss = 0.01444549\n","Iteration 84, loss = 0.01367216\n","Iteration 85, loss = 0.01346520\n","Iteration 86, loss = 0.01275553\n","Iteration 87, loss = 0.01241333\n","Iteration 88, loss = 0.01179818\n","Iteration 89, loss = 0.01094385\n","Iteration 90, loss = 0.01061380\n","Iteration 91, loss = 0.01046771\n","Iteration 92, loss = 0.01030801\n","Iteration 93, loss = 0.00977049\n","Iteration 94, loss = 0.00929379\n","Iteration 95, loss = 0.00918926\n","Iteration 96, loss = 0.00875329\n","Iteration 97, loss = 0.00843028\n","Iteration 98, loss = 0.00811510\n","Iteration 99, loss = 0.00794682\n","Iteration 100, loss = 0.00771504\n","Iteration 101, loss = 0.00765737\n","Iteration 102, loss = 0.00720748\n","Iteration 103, loss = 0.00704028\n","Iteration 104, loss = 0.00696538\n","Iteration 105, loss = 0.00664300\n","Iteration 106, loss = 0.00647621\n","Iteration 107, loss = 0.00631272\n","Iteration 108, loss = 0.00627787\n","Iteration 109, loss = 0.00612982\n","Iteration 110, loss = 0.00602623\n","Iteration 111, loss = 0.00582328\n","Iteration 112, loss = 0.00582072\n","Iteration 113, loss = 0.00572059\n","Iteration 114, loss = 0.00556995\n","Iteration 115, loss = 0.00554241\n","Iteration 116, loss = 0.00543235\n","Iteration 117, loss = 0.00535892\n","Iteration 118, loss = 0.00523858\n","Iteration 119, loss = 0.00515023\n","Iteration 120, loss = 0.00509162\n","Iteration 121, loss = 0.00513599\n","Iteration 122, loss = 0.00499025\n","Iteration 123, loss = 0.00489303\n","Iteration 124, loss = 0.00484811\n","Iteration 125, loss = 0.00477022\n","Iteration 126, loss = 0.00470177\n","Iteration 127, loss = 0.00465194\n","Iteration 128, loss = 0.00459742\n","Iteration 129, loss = 0.00455839\n","Iteration 130, loss = 0.00458462\n","Iteration 131, loss = 0.00451226\n","Iteration 132, loss = 0.00449501\n","Iteration 133, loss = 0.00444906\n","Iteration 134, loss = 0.00444376\n","Iteration 135, loss = 0.00437825\n","Iteration 136, loss = 0.00435891\n","Iteration 137, loss = 0.00430487\n","Iteration 138, loss = 0.00426928\n","Iteration 139, loss = 0.00424877\n","Iteration 140, loss = 0.00421331\n","Iteration 141, loss = 0.00421187\n","Iteration 142, loss = 0.00416384\n","Iteration 143, loss = 0.00413832\n","Iteration 144, loss = 0.00411630\n","Iteration 145, loss = 0.00408479\n","Iteration 146, loss = 0.00407827\n","Iteration 147, loss = 0.00409103\n","Iteration 148, loss = 0.00404877\n","Training loss did not improve more than tol=0.000050 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68829115\n","Iteration 2, loss = 0.57405519\n","Iteration 3, loss = 0.50351792\n","Iteration 4, loss = 0.45319673\n","Iteration 5, loss = 0.41358932\n","Iteration 6, loss = 0.38175999\n","Iteration 7, loss = 0.35609882\n","Iteration 8, loss = 0.33519642\n","Iteration 9, loss = 0.31846545\n","Iteration 10, loss = 0.30473311\n","Iteration 11, loss = 0.29359709\n","Iteration 12, loss = 0.28434978\n","Iteration 13, loss = 0.27667849\n","Iteration 14, loss = 0.27020929\n","Iteration 15, loss = 0.26479373\n","Iteration 16, loss = 0.26001552\n","Iteration 17, loss = 0.25593275\n","Iteration 18, loss = 0.25232243\n","Iteration 19, loss = 0.24920172\n","Iteration 20, loss = 0.24642141\n","Iteration 21, loss = 0.24391353\n","Iteration 22, loss = 0.24169695\n","Iteration 23, loss = 0.23970689\n","Iteration 24, loss = 0.23793100\n","Iteration 25, loss = 0.23632773\n","Iteration 26, loss = 0.23482439\n","Iteration 27, loss = 0.23346176\n","Iteration 28, loss = 0.23221856\n","Iteration 29, loss = 0.23106817\n","Iteration 30, loss = 0.23005235\n","Iteration 31, loss = 0.22905484\n","Iteration 32, loss = 0.22815553\n","Iteration 33, loss = 0.22734996\n","Iteration 34, loss = 0.22653612\n","Iteration 35, loss = 0.22577669\n","Iteration 36, loss = 0.22511739\n","Iteration 37, loss = 0.22440780\n","Iteration 38, loss = 0.22380498\n","Iteration 39, loss = 0.22319899\n","Iteration 40, loss = 0.22265882\n","Iteration 41, loss = 0.22209448\n","Iteration 42, loss = 0.22160160\n","Iteration 43, loss = 0.22107605\n","Iteration 44, loss = 0.22063879\n","Iteration 45, loss = 0.22019524\n","Iteration 46, loss = 0.21978101\n","Iteration 47, loss = 0.21935916\n","Iteration 48, loss = 0.21896918\n","Iteration 49, loss = 0.21859069\n","Iteration 50, loss = 0.21821159\n","Iteration 51, loss = 0.21786581\n","Iteration 52, loss = 0.21750867\n","Iteration 53, loss = 0.21717439\n","Iteration 54, loss = 0.21684557\n","Iteration 55, loss = 0.21652649\n","Iteration 56, loss = 0.21624202\n","Iteration 57, loss = 0.21590049\n","Iteration 58, loss = 0.21558776\n","Iteration 59, loss = 0.21530403\n","Iteration 60, loss = 0.21502307\n","Iteration 61, loss = 0.21474415\n","Iteration 62, loss = 0.21446439\n","Iteration 63, loss = 0.21418773\n","Iteration 64, loss = 0.21393479\n","Iteration 65, loss = 0.21367243\n","Iteration 66, loss = 0.21342330\n","Iteration 67, loss = 0.21319982\n","Iteration 68, loss = 0.21289704\n","Iteration 69, loss = 0.21269553\n","Iteration 70, loss = 0.21250307\n","Iteration 71, loss = 0.21225040\n","Iteration 72, loss = 0.21196258\n","Iteration 73, loss = 0.21174693\n","Iteration 74, loss = 0.21151634\n","Iteration 75, loss = 0.21130212\n","Iteration 76, loss = 0.21107458\n","Iteration 77, loss = 0.21084000\n","Iteration 78, loss = 0.21066236\n","Iteration 79, loss = 0.21045352\n","Iteration 80, loss = 0.21024938\n","Iteration 81, loss = 0.21002979\n","Iteration 82, loss = 0.20982640\n","Iteration 83, loss = 0.20961696\n","Iteration 84, loss = 0.20942871\n","Iteration 85, loss = 0.20926026\n","Iteration 86, loss = 0.20903046\n","Iteration 87, loss = 0.20886527\n","Iteration 88, loss = 0.20865002\n","Iteration 89, loss = 0.20844397\n","Iteration 90, loss = 0.20829341\n","Iteration 91, loss = 0.20805400\n","Iteration 92, loss = 0.20789309\n","Iteration 93, loss = 0.20766021\n","Iteration 94, loss = 0.20748914\n","Iteration 95, loss = 0.20732094\n","Iteration 96, loss = 0.20714032\n","Iteration 97, loss = 0.20695516\n","Iteration 98, loss = 0.20677840\n","Iteration 99, loss = 0.20662950\n","Iteration 100, loss = 0.20643233\n","Iteration 101, loss = 0.20624737\n","Iteration 102, loss = 0.20605537\n","Iteration 103, loss = 0.20589277\n","Iteration 104, loss = 0.20574372\n","Iteration 105, loss = 0.20556764\n","Iteration 106, loss = 0.20538532\n","Iteration 107, loss = 0.20522172\n","Iteration 108, loss = 0.20503382\n","Iteration 109, loss = 0.20485536\n","Iteration 110, loss = 0.20470346\n","Iteration 111, loss = 0.20452114\n","Iteration 112, loss = 0.20436256\n","Iteration 113, loss = 0.20420519\n","Iteration 114, loss = 0.20410305\n","Iteration 115, loss = 0.20387496\n","Iteration 116, loss = 0.20379959\n","Iteration 117, loss = 0.20360725\n","Iteration 118, loss = 0.20337473\n","Iteration 119, loss = 0.20326892\n","Iteration 120, loss = 0.20306622\n","Iteration 121, loss = 0.20291479\n","Iteration 122, loss = 0.20277950\n","Iteration 123, loss = 0.20263002\n","Iteration 124, loss = 0.20246990\n","Iteration 125, loss = 0.20231302\n","Iteration 126, loss = 0.20217983\n","Iteration 127, loss = 0.20201018\n","Iteration 128, loss = 0.20182973\n","Iteration 129, loss = 0.20172122\n","Iteration 130, loss = 0.20151829\n","Iteration 131, loss = 0.20140015\n","Iteration 132, loss = 0.20121591\n","Iteration 133, loss = 0.20110259\n","Iteration 134, loss = 0.20097352\n","Iteration 135, loss = 0.20077380\n","Iteration 136, loss = 0.20067477\n","Iteration 137, loss = 0.20051956\n","Iteration 138, loss = 0.20033872\n","Iteration 139, loss = 0.20023855\n","Iteration 140, loss = 0.20003724\n","Iteration 141, loss = 0.19992433\n","Iteration 142, loss = 0.19982685\n","Iteration 143, loss = 0.19965935\n","Iteration 144, loss = 0.19948358\n","Iteration 145, loss = 0.19931636\n","Iteration 146, loss = 0.19917947\n","Iteration 147, loss = 0.19905113\n","Iteration 148, loss = 0.19888625\n","Iteration 149, loss = 0.19875962\n","Iteration 150, loss = 0.19863212\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.63382520\n","Iteration 2, loss = 0.51539409\n","Iteration 3, loss = 0.45284656\n","Iteration 4, loss = 0.41167352\n","Iteration 5, loss = 0.38060104\n","Iteration 6, loss = 0.35579741\n","Iteration 7, loss = 0.33568523\n","Iteration 8, loss = 0.31936708\n","Iteration 9, loss = 0.30591499\n","Iteration 10, loss = 0.29501133\n","Iteration 11, loss = 0.28605966\n","Iteration 12, loss = 0.27875059\n","Iteration 13, loss = 0.27266378\n","Iteration 14, loss = 0.26747786\n","Iteration 15, loss = 0.26318755\n","Iteration 16, loss = 0.25945837\n","Iteration 17, loss = 0.25615810\n","Iteration 18, loss = 0.25327320\n","Iteration 19, loss = 0.25072135\n","Iteration 20, loss = 0.24846300\n","Iteration 21, loss = 0.24641865\n","Iteration 22, loss = 0.24454844\n","Iteration 23, loss = 0.24283812\n","Iteration 24, loss = 0.24131179\n","Iteration 25, loss = 0.23988351\n","Iteration 26, loss = 0.23856350\n","Iteration 27, loss = 0.23742586\n","Iteration 28, loss = 0.23629788\n","Iteration 29, loss = 0.23529062\n","Iteration 30, loss = 0.23433630\n","Iteration 31, loss = 0.23345223\n","Iteration 32, loss = 0.23259993\n","Iteration 33, loss = 0.23183509\n","Iteration 34, loss = 0.23107205\n","Iteration 35, loss = 0.23035921\n","Iteration 36, loss = 0.22969582\n","Iteration 37, loss = 0.22908590\n","Iteration 38, loss = 0.22849185\n","Iteration 39, loss = 0.22788932\n","Iteration 40, loss = 0.22736985\n","Iteration 41, loss = 0.22680750\n","Iteration 42, loss = 0.22629658\n","Iteration 43, loss = 0.22584966\n","Iteration 44, loss = 0.22536764\n","Iteration 45, loss = 0.22488169\n","Iteration 46, loss = 0.22441996\n","Iteration 47, loss = 0.22400285\n","Iteration 48, loss = 0.22357501\n","Iteration 49, loss = 0.22317882\n","Iteration 50, loss = 0.22277938\n","Iteration 51, loss = 0.22237946\n","Iteration 52, loss = 0.22200331\n","Iteration 53, loss = 0.22159623\n","Iteration 54, loss = 0.22123656\n","Iteration 55, loss = 0.22087747\n","Iteration 56, loss = 0.22054829\n","Iteration 57, loss = 0.22024129\n","Iteration 58, loss = 0.21988240\n","Iteration 59, loss = 0.21956888\n","Iteration 60, loss = 0.21924942\n","Iteration 61, loss = 0.21891106\n","Iteration 62, loss = 0.21858310\n","Iteration 63, loss = 0.21833488\n","Iteration 64, loss = 0.21797492\n","Iteration 65, loss = 0.21764967\n","Iteration 66, loss = 0.21736450\n","Iteration 67, loss = 0.21704875\n","Iteration 68, loss = 0.21677943\n","Iteration 69, loss = 0.21651200\n","Iteration 70, loss = 0.21622846\n","Iteration 71, loss = 0.21594597\n","Iteration 72, loss = 0.21568625\n","Iteration 73, loss = 0.21540228\n","Iteration 74, loss = 0.21513387\n","Iteration 75, loss = 0.21488049\n","Iteration 76, loss = 0.21466044\n","Iteration 77, loss = 0.21436038\n","Iteration 78, loss = 0.21409969\n","Iteration 79, loss = 0.21387796\n","Iteration 80, loss = 0.21360822\n","Iteration 81, loss = 0.21341665\n","Iteration 82, loss = 0.21318470\n","Iteration 83, loss = 0.21290777\n","Iteration 84, loss = 0.21266714\n","Iteration 85, loss = 0.21244020\n","Iteration 86, loss = 0.21220975\n","Iteration 87, loss = 0.21196922\n","Iteration 88, loss = 0.21176756\n","Iteration 89, loss = 0.21154275\n","Iteration 90, loss = 0.21133185\n","Iteration 91, loss = 0.21108635\n","Iteration 92, loss = 0.21086608\n","Iteration 93, loss = 0.21063638\n","Iteration 94, loss = 0.21046293\n","Iteration 95, loss = 0.21024706\n","Iteration 96, loss = 0.20998285\n","Iteration 97, loss = 0.20978738\n","Iteration 98, loss = 0.20962396\n","Iteration 99, loss = 0.20938401\n","Iteration 100, loss = 0.20917159\n","Iteration 101, loss = 0.20894163\n","Iteration 102, loss = 0.20876610\n","Iteration 103, loss = 0.20856663\n","Iteration 104, loss = 0.20838675\n","Iteration 105, loss = 0.20821188\n","Iteration 106, loss = 0.20802212\n","Iteration 107, loss = 0.20779170\n","Iteration 108, loss = 0.20762921\n","Iteration 109, loss = 0.20741057\n","Iteration 110, loss = 0.20721542\n","Iteration 111, loss = 0.20706791\n","Iteration 112, loss = 0.20684209\n","Iteration 113, loss = 0.20661809\n","Iteration 114, loss = 0.20646262\n","Iteration 115, loss = 0.20628286\n","Iteration 116, loss = 0.20607969\n","Iteration 117, loss = 0.20587504\n","Iteration 118, loss = 0.20573443\n","Iteration 119, loss = 0.20555188\n","Iteration 120, loss = 0.20537495\n","Iteration 121, loss = 0.20517655\n","Iteration 122, loss = 0.20501747\n","Iteration 123, loss = 0.20481044\n","Iteration 124, loss = 0.20469198\n","Iteration 125, loss = 0.20448849\n","Iteration 126, loss = 0.20431789\n","Iteration 127, loss = 0.20413918\n","Iteration 128, loss = 0.20395941\n","Iteration 129, loss = 0.20377173\n","Iteration 130, loss = 0.20361325\n","Iteration 131, loss = 0.20345395\n","Iteration 132, loss = 0.20326330\n","Iteration 133, loss = 0.20314388\n","Iteration 134, loss = 0.20293136\n","Iteration 135, loss = 0.20275757\n","Iteration 136, loss = 0.20261299\n","Iteration 137, loss = 0.20244493\n","Iteration 138, loss = 0.20225458\n","Iteration 139, loss = 0.20211271\n","Iteration 140, loss = 0.20195500\n","Iteration 141, loss = 0.20174311\n","Iteration 142, loss = 0.20165375\n","Iteration 143, loss = 0.20149028\n","Iteration 144, loss = 0.20127901\n","Iteration 145, loss = 0.20113094\n","Iteration 146, loss = 0.20096779\n","Iteration 147, loss = 0.20082571\n","Iteration 148, loss = 0.20066324\n","Iteration 149, loss = 0.20051896\n","Iteration 150, loss = 0.20035741\n","Iteration 1, loss = 0.62389850\n","Iteration 2, loss = 0.46539992\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 3, loss = 0.39596098\n","Iteration 4, loss = 0.35655202\n","Iteration 5, loss = 0.33045895\n","Iteration 6, loss = 0.31147757\n","Iteration 7, loss = 0.29725054\n","Iteration 8, loss = 0.28615207\n","Iteration 9, loss = 0.27731474\n","Iteration 10, loss = 0.27021967\n","Iteration 11, loss = 0.26432069\n","Iteration 12, loss = 0.25934413\n","Iteration 13, loss = 0.25509722\n","Iteration 14, loss = 0.25143111\n","Iteration 15, loss = 0.24830842\n","Iteration 16, loss = 0.24544681\n","Iteration 17, loss = 0.24291361\n","Iteration 18, loss = 0.24064811\n","Iteration 19, loss = 0.23861479\n","Iteration 20, loss = 0.23680821\n","Iteration 21, loss = 0.23509104\n","Iteration 22, loss = 0.23356977\n","Iteration 23, loss = 0.23214261\n","Iteration 24, loss = 0.23084234\n","Iteration 25, loss = 0.22966306\n","Iteration 26, loss = 0.22858703\n","Iteration 27, loss = 0.22754186\n","Iteration 28, loss = 0.22659673\n","Iteration 29, loss = 0.22568334\n","Iteration 30, loss = 0.22484001\n","Iteration 31, loss = 0.22406320\n","Iteration 32, loss = 0.22334915\n","Iteration 33, loss = 0.22261806\n","Iteration 34, loss = 0.22197558\n","Iteration 35, loss = 0.22129921\n","Iteration 36, loss = 0.22069244\n","Iteration 37, loss = 0.22012232\n","Iteration 38, loss = 0.21955681\n","Iteration 39, loss = 0.21904625\n","Iteration 40, loss = 0.21855597\n","Iteration 41, loss = 0.21808495\n","Iteration 42, loss = 0.21762159\n","Iteration 43, loss = 0.21717830\n","Iteration 44, loss = 0.21674921\n","Iteration 45, loss = 0.21632264\n","Iteration 46, loss = 0.21592998\n","Iteration 47, loss = 0.21556834\n","Iteration 48, loss = 0.21519516\n","Iteration 49, loss = 0.21479976\n","Iteration 50, loss = 0.21444423\n","Iteration 51, loss = 0.21407201\n","Iteration 52, loss = 0.21374329\n","Iteration 53, loss = 0.21340871\n","Iteration 54, loss = 0.21309091\n","Iteration 55, loss = 0.21279636\n","Iteration 56, loss = 0.21247326\n","Iteration 57, loss = 0.21216621\n","Iteration 58, loss = 0.21191337\n","Iteration 59, loss = 0.21155674\n","Iteration 60, loss = 0.21129863\n","Iteration 61, loss = 0.21100411\n","Iteration 62, loss = 0.21075679\n","Iteration 63, loss = 0.21045551\n","Iteration 64, loss = 0.21024078\n","Iteration 65, loss = 0.20996626\n","Iteration 66, loss = 0.20970841\n","Iteration 67, loss = 0.20941530\n","Iteration 68, loss = 0.20918686\n","Iteration 69, loss = 0.20895190\n","Iteration 70, loss = 0.20872899\n","Iteration 71, loss = 0.20850950\n","Iteration 72, loss = 0.20828032\n","Iteration 73, loss = 0.20801746\n","Iteration 74, loss = 0.20778713\n","Iteration 75, loss = 0.20755167\n","Iteration 76, loss = 0.20736421\n","Iteration 77, loss = 0.20714276\n","Iteration 78, loss = 0.20690477\n","Iteration 79, loss = 0.20668957\n","Iteration 80, loss = 0.20650497\n","Iteration 81, loss = 0.20628819\n","Iteration 82, loss = 0.20606090\n","Iteration 83, loss = 0.20589505\n","Iteration 84, loss = 0.20569456\n","Iteration 85, loss = 0.20549762\n","Iteration 86, loss = 0.20524152\n","Iteration 87, loss = 0.20505431\n","Iteration 88, loss = 0.20488949\n","Iteration 89, loss = 0.20473495\n","Iteration 90, loss = 0.20449823\n","Iteration 91, loss = 0.20432214\n","Iteration 92, loss = 0.20415885\n","Iteration 93, loss = 0.20395371\n","Iteration 94, loss = 0.20377391\n","Iteration 95, loss = 0.20356301\n","Iteration 96, loss = 0.20341488\n","Iteration 97, loss = 0.20324378\n","Iteration 98, loss = 0.20307871\n","Iteration 99, loss = 0.20290121\n","Iteration 100, loss = 0.20272997\n","Iteration 101, loss = 0.20251674\n","Iteration 102, loss = 0.20237594\n","Iteration 103, loss = 0.20221708\n","Iteration 104, loss = 0.20201636\n","Iteration 105, loss = 0.20189960\n","Iteration 106, loss = 0.20173541\n","Iteration 107, loss = 0.20157425\n","Iteration 108, loss = 0.20136701\n","Iteration 109, loss = 0.20122450\n","Iteration 110, loss = 0.20104293\n","Iteration 111, loss = 0.20091075\n","Iteration 112, loss = 0.20073505\n","Iteration 113, loss = 0.20059274\n","Iteration 114, loss = 0.20040352\n","Iteration 115, loss = 0.20025577\n","Iteration 116, loss = 0.20014052\n","Iteration 117, loss = 0.19998172\n","Iteration 118, loss = 0.19981777\n","Iteration 119, loss = 0.19968210\n","Iteration 120, loss = 0.19947643\n","Iteration 121, loss = 0.19936276\n","Iteration 122, loss = 0.19926496\n","Iteration 123, loss = 0.19909489\n","Iteration 124, loss = 0.19893075\n","Iteration 125, loss = 0.19880754\n","Iteration 126, loss = 0.19865192\n","Iteration 127, loss = 0.19854869\n","Iteration 128, loss = 0.19839219\n","Iteration 129, loss = 0.19822898\n","Iteration 130, loss = 0.19808804\n","Iteration 131, loss = 0.19798641\n","Iteration 132, loss = 0.19781006\n","Iteration 133, loss = 0.19766770\n","Iteration 134, loss = 0.19753642\n","Iteration 135, loss = 0.19742511\n","Iteration 136, loss = 0.19727254\n","Iteration 137, loss = 0.19715445\n","Iteration 138, loss = 0.19700721\n","Iteration 139, loss = 0.19687711\n","Iteration 140, loss = 0.19671805\n","Iteration 141, loss = 0.19663589\n","Iteration 142, loss = 0.19648658\n","Iteration 143, loss = 0.19635462\n","Iteration 144, loss = 0.19626969\n","Iteration 145, loss = 0.19608581\n","Iteration 146, loss = 0.19598817\n","Iteration 147, loss = 0.19582231\n","Iteration 148, loss = 0.19571856\n","Iteration 149, loss = 0.19559105\n","Iteration 150, loss = 0.19545843\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.72610139\n","Iteration 2, loss = 0.48049449\n","Iteration 3, loss = 0.39034037\n","Iteration 4, loss = 0.34582120\n","Iteration 5, loss = 0.31840969\n","Iteration 6, loss = 0.29979011\n","Iteration 7, loss = 0.28628004\n","Iteration 8, loss = 0.27608304\n","Iteration 9, loss = 0.26814855\n","Iteration 10, loss = 0.26177984\n","Iteration 11, loss = 0.25658247\n","Iteration 12, loss = 0.25232326\n","Iteration 13, loss = 0.24878416\n","Iteration 14, loss = 0.24574058\n","Iteration 15, loss = 0.24309641\n","Iteration 16, loss = 0.24079518\n","Iteration 17, loss = 0.23880306\n","Iteration 18, loss = 0.23709477\n","Iteration 19, loss = 0.23555105\n","Iteration 20, loss = 0.23419812\n","Iteration 21, loss = 0.23296830\n","Iteration 22, loss = 0.23187247\n","Iteration 23, loss = 0.23083591\n","Iteration 24, loss = 0.22993589\n","Iteration 25, loss = 0.22909722\n","Iteration 26, loss = 0.22832383\n","Iteration 27, loss = 0.22761491\n","Iteration 28, loss = 0.22696352\n","Iteration 29, loss = 0.22638883\n","Iteration 30, loss = 0.22582101\n","Iteration 31, loss = 0.22521005\n","Iteration 32, loss = 0.22468163\n","Iteration 33, loss = 0.22422232\n","Iteration 34, loss = 0.22370650\n","Iteration 35, loss = 0.22329500\n","Iteration 36, loss = 0.22284866\n","Iteration 37, loss = 0.22245807\n","Iteration 38, loss = 0.22202465\n","Iteration 39, loss = 0.22165196\n","Iteration 40, loss = 0.22125144\n","Iteration 41, loss = 0.22092308\n","Iteration 42, loss = 0.22056171\n","Iteration 43, loss = 0.22023436\n","Iteration 44, loss = 0.21988934\n","Iteration 45, loss = 0.21952662\n","Iteration 46, loss = 0.21921070\n","Iteration 47, loss = 0.21890647\n","Iteration 48, loss = 0.21857265\n","Iteration 49, loss = 0.21828866\n","Iteration 50, loss = 0.21795554\n","Iteration 51, loss = 0.21769617\n","Iteration 52, loss = 0.21736632\n","Iteration 53, loss = 0.21707549\n","Iteration 54, loss = 0.21686080\n","Iteration 55, loss = 0.21661185\n","Iteration 56, loss = 0.21626454\n","Iteration 57, loss = 0.21597950\n","Iteration 58, loss = 0.21573904\n","Iteration 59, loss = 0.21545593\n","Iteration 60, loss = 0.21521284\n","Iteration 61, loss = 0.21495473\n","Iteration 62, loss = 0.21469728\n","Iteration 63, loss = 0.21445569\n","Iteration 64, loss = 0.21422566\n","Iteration 65, loss = 0.21396681\n","Iteration 66, loss = 0.21371935\n","Iteration 67, loss = 0.21351397\n","Iteration 68, loss = 0.21328539\n","Iteration 69, loss = 0.21305532\n","Iteration 70, loss = 0.21278020\n","Iteration 71, loss = 0.21257348\n","Iteration 72, loss = 0.21235767\n","Iteration 73, loss = 0.21211320\n","Iteration 74, loss = 0.21188741\n","Iteration 75, loss = 0.21166843\n","Iteration 76, loss = 0.21150917\n","Iteration 77, loss = 0.21121981\n","Iteration 78, loss = 0.21103568\n","Iteration 79, loss = 0.21078221\n","Iteration 80, loss = 0.21060147\n","Iteration 81, loss = 0.21040883\n","Iteration 82, loss = 0.21017356\n","Iteration 83, loss = 0.20997620\n","Iteration 84, loss = 0.20980629\n","Iteration 85, loss = 0.20958454\n","Iteration 86, loss = 0.20934775\n","Iteration 87, loss = 0.20916778\n","Iteration 88, loss = 0.20898327\n","Iteration 89, loss = 0.20875237\n","Iteration 90, loss = 0.20857140\n","Iteration 91, loss = 0.20844145\n","Iteration 92, loss = 0.20821224\n","Iteration 93, loss = 0.20806010\n","Iteration 94, loss = 0.20779262\n","Iteration 95, loss = 0.20761229\n","Iteration 96, loss = 0.20744928\n","Iteration 97, loss = 0.20729032\n","Iteration 98, loss = 0.20708335\n","Iteration 99, loss = 0.20692973\n","Iteration 100, loss = 0.20674868\n","Iteration 101, loss = 0.20652574\n","Iteration 102, loss = 0.20634198\n","Iteration 103, loss = 0.20617087\n","Iteration 104, loss = 0.20598226\n","Iteration 105, loss = 0.20580769\n","Iteration 106, loss = 0.20564940\n","Iteration 107, loss = 0.20550223\n","Iteration 108, loss = 0.20525858\n","Iteration 109, loss = 0.20512355\n","Iteration 110, loss = 0.20492805\n","Iteration 111, loss = 0.20477919\n","Iteration 112, loss = 0.20456478\n","Iteration 113, loss = 0.20441673\n","Iteration 114, loss = 0.20421952\n","Iteration 115, loss = 0.20408032\n","Iteration 116, loss = 0.20392213\n","Iteration 117, loss = 0.20375570\n","Iteration 118, loss = 0.20359276\n","Iteration 119, loss = 0.20339840\n","Iteration 120, loss = 0.20324452\n","Iteration 121, loss = 0.20307397\n","Iteration 122, loss = 0.20290679\n","Iteration 123, loss = 0.20275329\n","Iteration 124, loss = 0.20257231\n","Iteration 125, loss = 0.20238183\n","Iteration 126, loss = 0.20223198\n","Iteration 127, loss = 0.20209187\n","Iteration 128, loss = 0.20195083\n","Iteration 129, loss = 0.20177516\n","Iteration 130, loss = 0.20165018\n","Iteration 131, loss = 0.20148755\n","Iteration 132, loss = 0.20127605\n","Iteration 133, loss = 0.20114537\n","Iteration 134, loss = 0.20096813\n","Iteration 135, loss = 0.20079585\n","Iteration 136, loss = 0.20067281\n","Iteration 137, loss = 0.20054294\n","Iteration 138, loss = 0.20034368\n","Iteration 139, loss = 0.20019087\n","Iteration 140, loss = 0.20003968\n","Iteration 141, loss = 0.19987933\n","Iteration 142, loss = 0.19971933\n","Iteration 143, loss = 0.19963064\n","Iteration 144, loss = 0.19949036\n","Iteration 145, loss = 0.19930051\n","Iteration 146, loss = 0.19914227\n","Iteration 147, loss = 0.19898114\n","Iteration 148, loss = 0.19881821\n","Iteration 149, loss = 0.19871548\n","Iteration 150, loss = 0.19854642\n","Iteration 1, loss = 0.56704710\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.41989960\n","Iteration 3, loss = 0.35877263\n","Iteration 4, loss = 0.32690738\n","Iteration 5, loss = 0.30622707\n","Iteration 6, loss = 0.29187657\n","Iteration 7, loss = 0.28112014\n","Iteration 8, loss = 0.27289833\n","Iteration 9, loss = 0.26636137\n","Iteration 10, loss = 0.26115460\n","Iteration 11, loss = 0.25679793\n","Iteration 12, loss = 0.25314907\n","Iteration 13, loss = 0.25002616\n","Iteration 14, loss = 0.24733533\n","Iteration 15, loss = 0.24495866\n","Iteration 16, loss = 0.24285011\n","Iteration 17, loss = 0.24095962\n","Iteration 18, loss = 0.23923560\n","Iteration 19, loss = 0.23777291\n","Iteration 20, loss = 0.23637087\n","Iteration 21, loss = 0.23510955\n","Iteration 22, loss = 0.23393333\n","Iteration 23, loss = 0.23281957\n","Iteration 24, loss = 0.23182946\n","Iteration 25, loss = 0.23088229\n","Iteration 26, loss = 0.23000589\n","Iteration 27, loss = 0.22912452\n","Iteration 28, loss = 0.22834658\n","Iteration 29, loss = 0.22760906\n","Iteration 30, loss = 0.22694563\n","Iteration 31, loss = 0.22626097\n","Iteration 32, loss = 0.22565211\n","Iteration 33, loss = 0.22501075\n","Iteration 34, loss = 0.22444178\n","Iteration 35, loss = 0.22387651\n","Iteration 36, loss = 0.22333760\n","Iteration 37, loss = 0.22285392\n","Iteration 38, loss = 0.22231813\n","Iteration 39, loss = 0.22184066\n","Iteration 40, loss = 0.22138683\n","Iteration 41, loss = 0.22092697\n","Iteration 42, loss = 0.22046970\n","Iteration 43, loss = 0.22009960\n","Iteration 44, loss = 0.21965723\n","Iteration 45, loss = 0.21932282\n","Iteration 46, loss = 0.21892537\n","Iteration 47, loss = 0.21851866\n","Iteration 48, loss = 0.21814535\n","Iteration 49, loss = 0.21778153\n","Iteration 50, loss = 0.21743313\n","Iteration 51, loss = 0.21708589\n","Iteration 52, loss = 0.21675458\n","Iteration 53, loss = 0.21643937\n","Iteration 54, loss = 0.21612784\n","Iteration 55, loss = 0.21581119\n","Iteration 56, loss = 0.21548981\n","Iteration 57, loss = 0.21517908\n","Iteration 58, loss = 0.21487481\n","Iteration 59, loss = 0.21460801\n","Iteration 60, loss = 0.21430049\n","Iteration 61, loss = 0.21404364\n","Iteration 62, loss = 0.21375242\n","Iteration 63, loss = 0.21349113\n","Iteration 64, loss = 0.21321227\n","Iteration 65, loss = 0.21297430\n","Iteration 66, loss = 0.21269128\n","Iteration 67, loss = 0.21246280\n","Iteration 68, loss = 0.21218071\n","Iteration 69, loss = 0.21193895\n","Iteration 70, loss = 0.21171642\n","Iteration 71, loss = 0.21145646\n","Iteration 72, loss = 0.21122241\n","Iteration 73, loss = 0.21102699\n","Iteration 74, loss = 0.21073939\n","Iteration 75, loss = 0.21050696\n","Iteration 76, loss = 0.21028376\n","Iteration 77, loss = 0.21005148\n","Iteration 78, loss = 0.20987448\n","Iteration 79, loss = 0.20967670\n","Iteration 80, loss = 0.20935759\n","Iteration 81, loss = 0.20919149\n","Iteration 82, loss = 0.20896761\n","Iteration 83, loss = 0.20878089\n","Iteration 84, loss = 0.20855272\n","Iteration 85, loss = 0.20832397\n","Iteration 86, loss = 0.20813763\n","Iteration 87, loss = 0.20794918\n","Iteration 88, loss = 0.20774416\n","Iteration 89, loss = 0.20756162\n","Iteration 90, loss = 0.20733758\n","Iteration 91, loss = 0.20720200\n","Iteration 92, loss = 0.20696848\n","Iteration 93, loss = 0.20676724\n","Iteration 94, loss = 0.20655144\n","Iteration 95, loss = 0.20638170\n","Iteration 96, loss = 0.20620207\n","Iteration 97, loss = 0.20602830\n","Iteration 98, loss = 0.20582126\n","Iteration 99, loss = 0.20563013\n","Iteration 100, loss = 0.20551140\n","Iteration 101, loss = 0.20529340\n","Iteration 102, loss = 0.20509714\n","Iteration 103, loss = 0.20491910\n","Iteration 104, loss = 0.20476154\n","Iteration 105, loss = 0.20453666\n","Iteration 106, loss = 0.20438238\n","Iteration 107, loss = 0.20420826\n","Iteration 108, loss = 0.20405250\n","Iteration 109, loss = 0.20387723\n","Iteration 110, loss = 0.20370780\n","Iteration 111, loss = 0.20357171\n","Iteration 112, loss = 0.20336102\n","Iteration 113, loss = 0.20323705\n","Iteration 114, loss = 0.20304931\n","Iteration 115, loss = 0.20287770\n","Iteration 116, loss = 0.20270795\n","Iteration 117, loss = 0.20256255\n","Iteration 118, loss = 0.20242563\n","Iteration 119, loss = 0.20226330\n","Iteration 120, loss = 0.20208108\n","Iteration 121, loss = 0.20191193\n","Iteration 122, loss = 0.20175802\n","Iteration 123, loss = 0.20160817\n","Iteration 124, loss = 0.20145469\n","Iteration 125, loss = 0.20134390\n","Iteration 126, loss = 0.20112918\n","Iteration 127, loss = 0.20100954\n","Iteration 128, loss = 0.20081948\n","Iteration 129, loss = 0.20067308\n","Iteration 130, loss = 0.20058919\n","Iteration 131, loss = 0.20038202\n","Iteration 132, loss = 0.20021320\n","Iteration 133, loss = 0.20008717\n","Iteration 134, loss = 0.19993842\n","Iteration 135, loss = 0.19975231\n","Iteration 136, loss = 0.19964730\n","Iteration 137, loss = 0.19947467\n","Iteration 138, loss = 0.19931960\n","Iteration 139, loss = 0.19916916\n","Iteration 140, loss = 0.19901228\n","Iteration 141, loss = 0.19887628\n","Iteration 142, loss = 0.19875762\n","Iteration 143, loss = 0.19862229\n","Iteration 144, loss = 0.19844838\n","Iteration 145, loss = 0.19832804\n","Iteration 146, loss = 0.19817459\n","Iteration 147, loss = 0.19803955\n","Iteration 148, loss = 0.19789548\n","Iteration 149, loss = 0.19775818\n","Iteration 150, loss = 0.19759388\n","Iteration 1, loss = 0.50957860\n","Iteration 2, loss = 0.29622069\n","Iteration 3, loss = 0.24332391\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 4, loss = 0.22793087\n","Iteration 5, loss = 0.22025169\n","Iteration 6, loss = 0.21646151\n","Iteration 7, loss = 0.21287232\n","Iteration 8, loss = 0.21010159\n","Iteration 9, loss = 0.20757181\n","Iteration 10, loss = 0.20492877\n","Iteration 11, loss = 0.20263311\n","Iteration 12, loss = 0.20109369\n","Iteration 13, loss = 0.19893763\n","Iteration 14, loss = 0.19703729\n","Iteration 15, loss = 0.19536060\n","Iteration 16, loss = 0.19312878\n","Iteration 17, loss = 0.19161686\n","Iteration 18, loss = 0.18957109\n","Iteration 19, loss = 0.18785114\n","Iteration 20, loss = 0.18572056\n","Iteration 21, loss = 0.18413974\n","Iteration 22, loss = 0.18246603\n","Iteration 23, loss = 0.18043079\n","Iteration 24, loss = 0.17932992\n","Iteration 25, loss = 0.17753393\n","Iteration 26, loss = 0.17596173\n","Iteration 27, loss = 0.17397332\n","Iteration 28, loss = 0.17308885\n","Iteration 29, loss = 0.16996728\n","Iteration 30, loss = 0.16868569\n","Iteration 31, loss = 0.16626746\n","Iteration 32, loss = 0.16501857\n","Iteration 33, loss = 0.16271473\n","Iteration 34, loss = 0.16061952\n","Iteration 35, loss = 0.15890961\n","Iteration 36, loss = 0.15725558\n","Iteration 37, loss = 0.15591652\n","Iteration 38, loss = 0.15315559\n","Iteration 39, loss = 0.15244432\n","Iteration 40, loss = 0.14854104\n","Iteration 41, loss = 0.14797489\n","Iteration 42, loss = 0.14616442\n","Iteration 43, loss = 0.14473400\n","Iteration 44, loss = 0.14179856\n","Iteration 45, loss = 0.13928609\n","Iteration 46, loss = 0.13745487\n","Iteration 47, loss = 0.13651642\n","Iteration 48, loss = 0.13333782\n","Iteration 49, loss = 0.13214364\n","Iteration 50, loss = 0.13128042\n","Iteration 51, loss = 0.12860790\n","Iteration 52, loss = 0.12780395\n","Iteration 53, loss = 0.12534388\n","Iteration 54, loss = 0.12418578\n","Iteration 55, loss = 0.12202736\n","Iteration 56, loss = 0.12109297\n","Iteration 57, loss = 0.11918727\n","Iteration 58, loss = 0.11786050\n","Iteration 59, loss = 0.11599851\n","Iteration 60, loss = 0.11485366\n","Iteration 61, loss = 0.11280630\n","Iteration 62, loss = 0.11236968\n","Iteration 63, loss = 0.11000298\n","Iteration 64, loss = 0.10855825\n","Iteration 65, loss = 0.10781180\n","Iteration 66, loss = 0.10654416\n","Iteration 67, loss = 0.10606494\n","Iteration 68, loss = 0.10386164\n","Iteration 69, loss = 0.10376652\n","Iteration 70, loss = 0.10031127\n","Iteration 71, loss = 0.10031923\n","Iteration 72, loss = 0.09826626\n","Iteration 73, loss = 0.09725647\n","Iteration 74, loss = 0.09628864\n","Iteration 75, loss = 0.09476686\n","Iteration 76, loss = 0.09379033\n","Iteration 77, loss = 0.09246746\n","Iteration 78, loss = 0.09206118\n","Iteration 79, loss = 0.08955420\n","Iteration 80, loss = 0.08879390\n","Iteration 81, loss = 0.08811189\n","Iteration 82, loss = 0.08653588\n","Iteration 83, loss = 0.08602850\n","Iteration 84, loss = 0.08445237\n","Iteration 85, loss = 0.08315220\n","Iteration 86, loss = 0.08273734\n","Iteration 87, loss = 0.08101751\n","Iteration 88, loss = 0.07996369\n","Iteration 89, loss = 0.07885655\n","Iteration 90, loss = 0.07745563\n","Iteration 91, loss = 0.07693257\n","Iteration 92, loss = 0.07733530\n","Iteration 93, loss = 0.07552321\n","Iteration 94, loss = 0.07414856\n","Iteration 95, loss = 0.07239627\n","Iteration 96, loss = 0.07191075\n","Iteration 97, loss = 0.07185679\n","Iteration 98, loss = 0.07043090\n","Iteration 99, loss = 0.06897424\n","Iteration 100, loss = 0.06857966\n","Iteration 101, loss = 0.06716973\n","Iteration 102, loss = 0.06592095\n","Iteration 103, loss = 0.06505248\n","Iteration 104, loss = 0.06517092\n","Iteration 105, loss = 0.06455354\n","Iteration 106, loss = 0.06255783\n","Iteration 107, loss = 0.06209724\n","Iteration 108, loss = 0.06214283\n","Iteration 109, loss = 0.06149689\n","Iteration 110, loss = 0.05950523\n","Iteration 111, loss = 0.05899475\n","Iteration 112, loss = 0.05832792\n","Iteration 113, loss = 0.05756974\n","Iteration 114, loss = 0.05708306\n","Iteration 115, loss = 0.05543608\n","Iteration 116, loss = 0.05553215\n","Iteration 117, loss = 0.05462707\n","Iteration 118, loss = 0.05481011\n","Iteration 119, loss = 0.05315821\n","Iteration 120, loss = 0.05265676\n","Iteration 121, loss = 0.05183244\n","Iteration 122, loss = 0.05187841\n","Iteration 123, loss = 0.04990850\n","Iteration 124, loss = 0.05031680\n","Iteration 125, loss = 0.04937002\n","Iteration 126, loss = 0.04890888\n","Iteration 127, loss = 0.04867794\n","Iteration 128, loss = 0.04683041\n","Iteration 129, loss = 0.04603164\n","Iteration 130, loss = 0.04556468\n","Iteration 131, loss = 0.04597060\n","Iteration 132, loss = 0.04491131\n","Iteration 133, loss = 0.04421366\n","Iteration 134, loss = 0.04316363\n","Iteration 135, loss = 0.04289668\n","Iteration 136, loss = 0.04287140\n","Iteration 137, loss = 0.04197217\n","Iteration 138, loss = 0.04117547\n","Iteration 139, loss = 0.04053486\n","Iteration 140, loss = 0.03980093\n","Iteration 141, loss = 0.03963152\n","Iteration 142, loss = 0.03935036\n","Iteration 143, loss = 0.03910930\n","Iteration 144, loss = 0.03814456\n","Iteration 145, loss = 0.03824740\n","Iteration 146, loss = 0.03730613\n","Iteration 147, loss = 0.03643764\n","Iteration 148, loss = 0.03624014\n","Iteration 149, loss = 0.03566366\n","Iteration 150, loss = 0.03533433\n","Iteration 1, loss = 0.47686858\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.27538814\n","Iteration 3, loss = 0.24473189\n","Iteration 4, loss = 0.23311876\n","Iteration 5, loss = 0.22602541\n","Iteration 6, loss = 0.22128414\n","Iteration 7, loss = 0.21716931\n","Iteration 8, loss = 0.21398169\n","Iteration 9, loss = 0.21110277\n","Iteration 10, loss = 0.20804443\n","Iteration 11, loss = 0.20627555\n","Iteration 12, loss = 0.20431215\n","Iteration 13, loss = 0.20203391\n","Iteration 14, loss = 0.19928417\n","Iteration 15, loss = 0.19800607\n","Iteration 16, loss = 0.19595709\n","Iteration 17, loss = 0.19393864\n","Iteration 18, loss = 0.19304468\n","Iteration 19, loss = 0.19039116\n","Iteration 20, loss = 0.18911596\n","Iteration 21, loss = 0.18731065\n","Iteration 22, loss = 0.18593520\n","Iteration 23, loss = 0.18397123\n","Iteration 24, loss = 0.18243028\n","Iteration 25, loss = 0.18091099\n","Iteration 26, loss = 0.17892358\n","Iteration 27, loss = 0.17743324\n","Iteration 28, loss = 0.17497427\n","Iteration 29, loss = 0.17411310\n","Iteration 30, loss = 0.17231394\n","Iteration 31, loss = 0.17016373\n","Iteration 32, loss = 0.16886950\n","Iteration 33, loss = 0.16715185\n","Iteration 34, loss = 0.16564025\n","Iteration 35, loss = 0.16356280\n","Iteration 36, loss = 0.16237705\n","Iteration 37, loss = 0.16227836\n","Iteration 38, loss = 0.15852562\n","Iteration 39, loss = 0.15633907\n","Iteration 40, loss = 0.15423303\n","Iteration 41, loss = 0.15429556\n","Iteration 42, loss = 0.15177830\n","Iteration 43, loss = 0.14970684\n","Iteration 44, loss = 0.14903024\n","Iteration 45, loss = 0.14649660\n","Iteration 46, loss = 0.14420417\n","Iteration 47, loss = 0.14348366\n","Iteration 48, loss = 0.14188674\n","Iteration 49, loss = 0.14030461\n","Iteration 50, loss = 0.13807071\n","Iteration 51, loss = 0.13741863\n","Iteration 52, loss = 0.13606893\n","Iteration 53, loss = 0.13382949\n","Iteration 54, loss = 0.13269214\n","Iteration 55, loss = 0.13068222\n","Iteration 56, loss = 0.12964283\n","Iteration 57, loss = 0.12854467\n","Iteration 58, loss = 0.12615502\n","Iteration 59, loss = 0.12559941\n","Iteration 60, loss = 0.12443404\n","Iteration 61, loss = 0.12311759\n","Iteration 62, loss = 0.12204243\n","Iteration 63, loss = 0.12022133\n","Iteration 64, loss = 0.11876475\n","Iteration 65, loss = 0.11683199\n","Iteration 66, loss = 0.11665646\n","Iteration 67, loss = 0.11497316\n","Iteration 68, loss = 0.11355875\n","Iteration 69, loss = 0.11255660\n","Iteration 70, loss = 0.11222620\n","Iteration 71, loss = 0.10971554\n","Iteration 72, loss = 0.10831833\n","Iteration 73, loss = 0.10819971\n","Iteration 74, loss = 0.10573177\n","Iteration 75, loss = 0.10517581\n","Iteration 76, loss = 0.10362670\n","Iteration 77, loss = 0.10382115\n","Iteration 78, loss = 0.10329225\n","Iteration 79, loss = 0.10054603\n","Iteration 80, loss = 0.10026274\n","Iteration 81, loss = 0.09875076\n","Iteration 82, loss = 0.09723810\n","Iteration 83, loss = 0.09560828\n","Iteration 84, loss = 0.09514743\n","Iteration 85, loss = 0.09431642\n","Iteration 86, loss = 0.09346632\n","Iteration 87, loss = 0.09276921\n","Iteration 88, loss = 0.09133755\n","Iteration 89, loss = 0.09021397\n","Iteration 90, loss = 0.08934423\n","Iteration 91, loss = 0.08811472\n","Iteration 92, loss = 0.08814890\n","Iteration 93, loss = 0.08678001\n","Iteration 94, loss = 0.08557231\n","Iteration 95, loss = 0.08540624\n","Iteration 96, loss = 0.08438448\n","Iteration 97, loss = 0.08272013\n","Iteration 98, loss = 0.08189325\n","Iteration 99, loss = 0.08115877\n","Iteration 100, loss = 0.08054320\n","Iteration 101, loss = 0.08045161\n","Iteration 102, loss = 0.07856726\n","Iteration 103, loss = 0.07779464\n","Iteration 104, loss = 0.07626756\n","Iteration 105, loss = 0.07654864\n","Iteration 106, loss = 0.07563420\n","Iteration 107, loss = 0.07358940\n","Iteration 108, loss = 0.07405783\n","Iteration 109, loss = 0.07197987\n","Iteration 110, loss = 0.07187293\n","Iteration 111, loss = 0.07113995\n","Iteration 112, loss = 0.06979205\n","Iteration 113, loss = 0.06929825\n","Iteration 114, loss = 0.06864614\n","Iteration 115, loss = 0.06734584\n","Iteration 116, loss = 0.06612219\n","Iteration 117, loss = 0.06552818\n","Iteration 118, loss = 0.06488715\n","Iteration 119, loss = 0.06408579\n","Iteration 120, loss = 0.06340897\n","Iteration 121, loss = 0.06248555\n","Iteration 122, loss = 0.06247950\n","Iteration 123, loss = 0.06105494\n","Iteration 124, loss = 0.05994890\n","Iteration 125, loss = 0.05918589\n","Iteration 126, loss = 0.05937642\n","Iteration 127, loss = 0.05873465\n","Iteration 128, loss = 0.05678273\n","Iteration 129, loss = 0.05651299\n","Iteration 130, loss = 0.05621371\n","Iteration 131, loss = 0.05538423\n","Iteration 132, loss = 0.05411189\n","Iteration 133, loss = 0.05384146\n","Iteration 134, loss = 0.05290413\n","Iteration 135, loss = 0.05194783\n","Iteration 136, loss = 0.05201418\n","Iteration 137, loss = 0.05129952\n","Iteration 138, loss = 0.05061686\n","Iteration 139, loss = 0.05001572\n","Iteration 140, loss = 0.04961472\n","Iteration 141, loss = 0.04856534\n","Iteration 142, loss = 0.04794520\n","Iteration 143, loss = 0.04782225\n","Iteration 144, loss = 0.04688601\n","Iteration 145, loss = 0.04641567\n","Iteration 146, loss = 0.04622965\n","Iteration 147, loss = 0.04561217\n","Iteration 148, loss = 0.04446024\n","Iteration 149, loss = 0.04425862\n","Iteration 150, loss = 0.04323662\n","Iteration 1, loss = 0.49014683\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 2, loss = 0.31079678\n","Iteration 3, loss = 0.26543977\n","Iteration 4, loss = 0.24561117\n","Iteration 5, loss = 0.23336085\n","Iteration 6, loss = 0.22530039\n","Iteration 7, loss = 0.21948422\n","Iteration 8, loss = 0.21487671\n","Iteration 9, loss = 0.21059749\n","Iteration 10, loss = 0.20730495\n","Iteration 11, loss = 0.20416316\n","Iteration 12, loss = 0.20144533\n","Iteration 13, loss = 0.19889525\n","Iteration 14, loss = 0.19608628\n","Iteration 15, loss = 0.19394574\n","Iteration 16, loss = 0.19176085\n","Iteration 17, loss = 0.18951330\n","Iteration 18, loss = 0.18712376\n","Iteration 19, loss = 0.18501245\n","Iteration 20, loss = 0.18388115\n","Iteration 21, loss = 0.18134037\n","Iteration 22, loss = 0.17993842\n","Iteration 23, loss = 0.17811281\n","Iteration 24, loss = 0.17523422\n","Iteration 25, loss = 0.17398949\n","Iteration 26, loss = 0.17192321\n","Iteration 27, loss = 0.16956386\n","Iteration 28, loss = 0.16784714\n","Iteration 29, loss = 0.16633132\n","Iteration 30, loss = 0.16432123\n","Iteration 31, loss = 0.16283220\n","Iteration 32, loss = 0.16197355\n","Iteration 33, loss = 0.15921080\n","Iteration 34, loss = 0.15720614\n","Iteration 35, loss = 0.15527131\n","Iteration 36, loss = 0.15367776\n","Iteration 37, loss = 0.15152162\n","Iteration 38, loss = 0.14919171\n","Iteration 39, loss = 0.14871895\n","Iteration 40, loss = 0.14645528\n","Iteration 41, loss = 0.14483282\n","Iteration 42, loss = 0.14219929\n","Iteration 43, loss = 0.14033263\n","Iteration 44, loss = 0.13937269\n","Iteration 45, loss = 0.13809755\n","Iteration 46, loss = 0.13619464\n","Iteration 47, loss = 0.13482319\n","Iteration 48, loss = 0.13334691\n","Iteration 49, loss = 0.13174763\n","Iteration 50, loss = 0.12958597\n","Iteration 51, loss = 0.12792901\n","Iteration 52, loss = 0.12601989\n","Iteration 53, loss = 0.12505162\n","Iteration 54, loss = 0.12418340\n","Iteration 55, loss = 0.12324550\n","Iteration 56, loss = 0.12059905\n","Iteration 57, loss = 0.12011560\n","Iteration 58, loss = 0.11781447\n","Iteration 59, loss = 0.11575498\n","Iteration 60, loss = 0.11473957\n","Iteration 61, loss = 0.11422138\n","Iteration 62, loss = 0.11177802\n","Iteration 63, loss = 0.11046722\n","Iteration 64, loss = 0.10895149\n","Iteration 65, loss = 0.10771934\n","Iteration 66, loss = 0.10601068\n","Iteration 67, loss = 0.10497365\n","Iteration 68, loss = 0.10421656\n","Iteration 69, loss = 0.10270522\n","Iteration 70, loss = 0.10201957\n","Iteration 71, loss = 0.10063193\n","Iteration 72, loss = 0.09854386\n","Iteration 73, loss = 0.09782840\n","Iteration 74, loss = 0.09688230\n","Iteration 75, loss = 0.09533153\n","Iteration 76, loss = 0.09416705\n","Iteration 77, loss = 0.09324907\n","Iteration 78, loss = 0.09248201\n","Iteration 79, loss = 0.09138809\n","Iteration 80, loss = 0.08962540\n","Iteration 81, loss = 0.08868071\n","Iteration 82, loss = 0.08807447\n","Iteration 83, loss = 0.08725798\n","Iteration 84, loss = 0.08781410\n","Iteration 85, loss = 0.08554311\n","Iteration 86, loss = 0.08487928\n","Iteration 87, loss = 0.08327016\n","Iteration 88, loss = 0.08196758\n","Iteration 89, loss = 0.08076507\n","Iteration 90, loss = 0.08100122\n","Iteration 91, loss = 0.08045868\n","Iteration 92, loss = 0.07958539\n","Iteration 93, loss = 0.07792050\n","Iteration 94, loss = 0.07801569\n","Iteration 95, loss = 0.07611207\n","Iteration 96, loss = 0.07526776\n","Iteration 97, loss = 0.07483406\n","Iteration 98, loss = 0.07441981\n","Iteration 99, loss = 0.07298652\n","Iteration 100, loss = 0.07315989\n","Iteration 101, loss = 0.07173733\n","Iteration 102, loss = 0.07038805\n","Iteration 103, loss = 0.07064449\n","Iteration 104, loss = 0.06998722\n","Iteration 105, loss = 0.06858874\n","Iteration 106, loss = 0.06789732\n","Iteration 107, loss = 0.06774445\n","Iteration 108, loss = 0.06709783\n","Iteration 109, loss = 0.06569655\n","Iteration 110, loss = 0.06452843\n","Iteration 111, loss = 0.06348957\n","Iteration 112, loss = 0.06366768\n","Iteration 113, loss = 0.06274174\n","Iteration 114, loss = 0.06149041\n","Iteration 115, loss = 0.06144003\n","Iteration 116, loss = 0.06071736\n","Iteration 117, loss = 0.05906579\n","Iteration 118, loss = 0.05902262\n","Iteration 119, loss = 0.05744890\n","Iteration 120, loss = 0.05713446\n","Iteration 121, loss = 0.05701679\n","Iteration 122, loss = 0.05649700\n","Iteration 123, loss = 0.05686283\n","Iteration 124, loss = 0.05463521\n","Iteration 125, loss = 0.05487906\n","Iteration 126, loss = 0.05410511\n","Iteration 127, loss = 0.05339774\n","Iteration 128, loss = 0.05267603\n","Iteration 129, loss = 0.05082240\n","Iteration 130, loss = 0.05062561\n","Iteration 131, loss = 0.05001389\n","Iteration 132, loss = 0.04875844\n","Iteration 133, loss = 0.04965154\n","Iteration 134, loss = 0.04957647\n","Iteration 135, loss = 0.04745950\n","Iteration 136, loss = 0.04637832\n","Iteration 137, loss = 0.04563770\n","Iteration 138, loss = 0.04552746\n","Iteration 139, loss = 0.04466071\n","Iteration 140, loss = 0.04388205\n","Iteration 141, loss = 0.04403760\n","Iteration 142, loss = 0.04281475\n","Iteration 143, loss = 0.04308650\n","Iteration 144, loss = 0.04228937\n","Iteration 145, loss = 0.04090216\n","Iteration 146, loss = 0.04027148\n","Iteration 147, loss = 0.04031931\n","Iteration 148, loss = 0.04003050\n","Iteration 149, loss = 0.03930960\n","Iteration 150, loss = 0.03835073\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.44905555\n","Iteration 2, loss = 0.28493429\n","Iteration 3, loss = 0.25150727\n","Iteration 4, loss = 0.23788795\n","Iteration 5, loss = 0.22954536\n","Iteration 6, loss = 0.22351157\n","Iteration 7, loss = 0.21890519\n","Iteration 8, loss = 0.21437663\n","Iteration 9, loss = 0.21117014\n","Iteration 10, loss = 0.20758280\n","Iteration 11, loss = 0.20462034\n","Iteration 12, loss = 0.20160257\n","Iteration 13, loss = 0.19969694\n","Iteration 14, loss = 0.19694966\n","Iteration 15, loss = 0.19500706\n","Iteration 16, loss = 0.19331401\n","Iteration 17, loss = 0.19108331\n","Iteration 18, loss = 0.18964667\n","Iteration 19, loss = 0.18756053\n","Iteration 20, loss = 0.18561072\n","Iteration 21, loss = 0.18385120\n","Iteration 22, loss = 0.18206005\n","Iteration 23, loss = 0.17980293\n","Iteration 24, loss = 0.17830409\n","Iteration 25, loss = 0.17689737\n","Iteration 26, loss = 0.17469193\n","Iteration 27, loss = 0.17244067\n","Iteration 28, loss = 0.17115219\n","Iteration 29, loss = 0.16894018\n","Iteration 30, loss = 0.16734570\n","Iteration 31, loss = 0.16557303\n","Iteration 32, loss = 0.16403587\n","Iteration 33, loss = 0.16158489\n","Iteration 34, loss = 0.15970368\n","Iteration 35, loss = 0.15798816\n","Iteration 36, loss = 0.15641552\n","Iteration 37, loss = 0.15541365\n","Iteration 38, loss = 0.15343114\n","Iteration 39, loss = 0.15126058\n","Iteration 40, loss = 0.14878466\n","Iteration 41, loss = 0.14798610\n","Iteration 42, loss = 0.14527458\n","Iteration 43, loss = 0.14444246\n","Iteration 44, loss = 0.14204335\n","Iteration 45, loss = 0.14131453\n","Iteration 46, loss = 0.13926069\n","Iteration 47, loss = 0.13676030\n","Iteration 48, loss = 0.13515336\n","Iteration 49, loss = 0.13338753\n","Iteration 50, loss = 0.13139434\n","Iteration 51, loss = 0.12943199\n","Iteration 52, loss = 0.12829124\n","Iteration 53, loss = 0.12816148\n","Iteration 54, loss = 0.12548493\n","Iteration 55, loss = 0.12361465\n","Iteration 56, loss = 0.12119745\n","Iteration 57, loss = 0.11931419\n","Iteration 58, loss = 0.11979990\n","Iteration 59, loss = 0.11760318\n","Iteration 60, loss = 0.11524095\n","Iteration 61, loss = 0.11489957\n","Iteration 62, loss = 0.11270794\n","Iteration 63, loss = 0.11067474\n","Iteration 64, loss = 0.11017415\n","Iteration 65, loss = 0.10705853\n","Iteration 66, loss = 0.10715395\n","Iteration 67, loss = 0.10514305\n","Iteration 68, loss = 0.10399020\n","Iteration 69, loss = 0.10292834\n","Iteration 70, loss = 0.10103480\n","Iteration 71, loss = 0.10010335\n","Iteration 72, loss = 0.09950821\n","Iteration 73, loss = 0.09718450\n","Iteration 74, loss = 0.09530908\n","Iteration 75, loss = 0.09416012\n","Iteration 76, loss = 0.09287049\n","Iteration 77, loss = 0.09158640\n","Iteration 78, loss = 0.09126872\n","Iteration 79, loss = 0.08869836\n","Iteration 80, loss = 0.08867175\n","Iteration 81, loss = 0.08705108\n","Iteration 82, loss = 0.08722252\n","Iteration 83, loss = 0.08513894\n","Iteration 84, loss = 0.08435699\n","Iteration 85, loss = 0.08235512\n","Iteration 86, loss = 0.08172906\n","Iteration 87, loss = 0.08136615\n","Iteration 88, loss = 0.08110669\n","Iteration 89, loss = 0.07972509\n","Iteration 90, loss = 0.07806961\n","Iteration 91, loss = 0.07740182\n","Iteration 92, loss = 0.07608274\n","Iteration 93, loss = 0.07489986\n","Iteration 94, loss = 0.07333079\n","Iteration 95, loss = 0.07311648\n","Iteration 96, loss = 0.07250746\n","Iteration 97, loss = 0.07071481\n","Iteration 98, loss = 0.07000211\n","Iteration 99, loss = 0.07007655\n","Iteration 100, loss = 0.06831635\n","Iteration 101, loss = 0.06826988\n","Iteration 102, loss = 0.06667489\n","Iteration 103, loss = 0.06595499\n","Iteration 104, loss = 0.06490052\n","Iteration 105, loss = 0.06308908\n","Iteration 106, loss = 0.06296686\n","Iteration 107, loss = 0.06069874\n","Iteration 108, loss = 0.06161530\n","Iteration 109, loss = 0.06040689\n","Iteration 110, loss = 0.06011187\n","Iteration 111, loss = 0.05954333\n","Iteration 112, loss = 0.05696185\n","Iteration 113, loss = 0.05683906\n","Iteration 114, loss = 0.05569455\n","Iteration 115, loss = 0.05547510\n","Iteration 116, loss = 0.05626133\n","Iteration 117, loss = 0.05343489\n","Iteration 118, loss = 0.05268407\n","Iteration 119, loss = 0.05223232\n","Iteration 120, loss = 0.05075017\n","Iteration 121, loss = 0.05086388\n","Iteration 122, loss = 0.04983764\n","Iteration 123, loss = 0.04964017\n","Iteration 124, loss = 0.04965960\n","Iteration 125, loss = 0.04730662\n","Iteration 126, loss = 0.04705028\n","Iteration 127, loss = 0.04733063\n","Iteration 128, loss = 0.04608216\n","Iteration 129, loss = 0.04436385\n","Iteration 130, loss = 0.04506975\n","Iteration 131, loss = 0.04368295\n","Iteration 132, loss = 0.04389730\n","Iteration 133, loss = 0.04209594\n","Iteration 134, loss = 0.04126391\n","Iteration 135, loss = 0.04127033\n","Iteration 136, loss = 0.04039832\n","Iteration 137, loss = 0.04039597\n","Iteration 138, loss = 0.03989922\n","Iteration 139, loss = 0.03990725\n","Iteration 140, loss = 0.03871131\n","Iteration 141, loss = 0.03768556\n","Iteration 142, loss = 0.03719615\n","Iteration 143, loss = 0.03723992\n","Iteration 144, loss = 0.03687647\n","Iteration 145, loss = 0.03643163\n","Iteration 146, loss = 0.03555015\n","Iteration 147, loss = 0.03488824\n","Iteration 148, loss = 0.03406433\n","Iteration 149, loss = 0.03380299\n","Iteration 150, loss = 0.03434946\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.43349583\n","Iteration 2, loss = 0.26223598\n","Iteration 3, loss = 0.23646694\n","Iteration 4, loss = 0.22633643\n","Iteration 5, loss = 0.22005060\n","Iteration 6, loss = 0.21622802\n","Iteration 7, loss = 0.21283408\n","Iteration 8, loss = 0.21011540\n","Iteration 9, loss = 0.20734322\n","Iteration 10, loss = 0.20483669\n","Iteration 11, loss = 0.20309712\n","Iteration 12, loss = 0.20020478\n","Iteration 13, loss = 0.19780761\n","Iteration 14, loss = 0.19563366\n","Iteration 15, loss = 0.19383526\n","Iteration 16, loss = 0.19187975\n","Iteration 17, loss = 0.19009104\n","Iteration 18, loss = 0.18765591\n","Iteration 19, loss = 0.18674552\n","Iteration 20, loss = 0.18443665\n","Iteration 21, loss = 0.18253897\n","Iteration 22, loss = 0.18110423\n","Iteration 23, loss = 0.18015707\n","Iteration 24, loss = 0.17742748\n","Iteration 25, loss = 0.17544747\n","Iteration 26, loss = 0.17335665\n","Iteration 27, loss = 0.17229266\n","Iteration 28, loss = 0.16998064\n","Iteration 29, loss = 0.16855366\n","Iteration 30, loss = 0.16726430\n","Iteration 31, loss = 0.16565905\n","Iteration 32, loss = 0.16367645\n","Iteration 33, loss = 0.16211900\n","Iteration 34, loss = 0.16105279\n","Iteration 35, loss = 0.15775020\n","Iteration 36, loss = 0.15697541\n","Iteration 37, loss = 0.15486830\n","Iteration 38, loss = 0.15320683\n","Iteration 39, loss = 0.15169523\n","Iteration 40, loss = 0.14915756\n","Iteration 41, loss = 0.14794554\n","Iteration 42, loss = 0.14611266\n","Iteration 43, loss = 0.14411184\n","Iteration 44, loss = 0.14284640\n","Iteration 45, loss = 0.14049105\n","Iteration 46, loss = 0.14081309\n","Iteration 47, loss = 0.13766820\n","Iteration 48, loss = 0.13681757\n","Iteration 49, loss = 0.13584551\n","Iteration 50, loss = 0.13342496\n","Iteration 51, loss = 0.13337444\n","Iteration 52, loss = 0.13080938\n","Iteration 53, loss = 0.12888904\n","Iteration 54, loss = 0.12772170\n","Iteration 55, loss = 0.12639727\n","Iteration 56, loss = 0.12465829\n","Iteration 57, loss = 0.12281430\n","Iteration 58, loss = 0.12183763\n","Iteration 59, loss = 0.11991786\n","Iteration 60, loss = 0.11957881\n","Iteration 61, loss = 0.11774049\n","Iteration 62, loss = 0.11632726\n","Iteration 63, loss = 0.11502962\n","Iteration 64, loss = 0.11367524\n","Iteration 65, loss = 0.11265909\n","Iteration 66, loss = 0.11180431\n","Iteration 67, loss = 0.11010202\n","Iteration 68, loss = 0.10889790\n","Iteration 69, loss = 0.10792875\n","Iteration 70, loss = 0.10603358\n","Iteration 71, loss = 0.10569305\n","Iteration 72, loss = 0.10425179\n","Iteration 73, loss = 0.10174988\n","Iteration 74, loss = 0.10180676\n","Iteration 75, loss = 0.10062235\n","Iteration 76, loss = 0.09863249\n","Iteration 77, loss = 0.09948621\n","Iteration 78, loss = 0.09726883\n","Iteration 79, loss = 0.09529879\n","Iteration 80, loss = 0.09437334\n","Iteration 81, loss = 0.09495646\n","Iteration 82, loss = 0.09271621\n","Iteration 83, loss = 0.09168279\n","Iteration 84, loss = 0.09028740\n","Iteration 85, loss = 0.08931966\n","Iteration 86, loss = 0.08832005\n","Iteration 87, loss = 0.08739586\n","Iteration 88, loss = 0.08675515\n","Iteration 89, loss = 0.08581315\n","Iteration 90, loss = 0.08535441\n","Iteration 91, loss = 0.08351144\n","Iteration 92, loss = 0.08279918\n","Iteration 93, loss = 0.08114978\n","Iteration 94, loss = 0.08035186\n","Iteration 95, loss = 0.07871933\n","Iteration 96, loss = 0.07830609\n","Iteration 97, loss = 0.07724064\n","Iteration 98, loss = 0.07682245\n","Iteration 99, loss = 0.07528636\n","Iteration 100, loss = 0.07458058\n","Iteration 101, loss = 0.07445285\n","Iteration 102, loss = 0.07395634\n","Iteration 103, loss = 0.07201606\n","Iteration 104, loss = 0.07184630\n","Iteration 105, loss = 0.07129277\n","Iteration 106, loss = 0.06902443\n","Iteration 107, loss = 0.06895411\n","Iteration 108, loss = 0.06851482\n","Iteration 109, loss = 0.06805265\n","Iteration 110, loss = 0.06643427\n","Iteration 111, loss = 0.06598962\n","Iteration 112, loss = 0.06470689\n","Iteration 113, loss = 0.06463113\n","Iteration 114, loss = 0.06325343\n","Iteration 115, loss = 0.06236771\n","Iteration 116, loss = 0.06175020\n","Iteration 117, loss = 0.06081680\n","Iteration 118, loss = 0.06074152\n","Iteration 119, loss = 0.06064984\n","Iteration 120, loss = 0.05966542\n","Iteration 121, loss = 0.05753430\n","Iteration 122, loss = 0.05711453\n","Iteration 123, loss = 0.05633953\n","Iteration 124, loss = 0.05543412\n","Iteration 125, loss = 0.05536427\n","Iteration 126, loss = 0.05542911\n","Iteration 127, loss = 0.05454628\n","Iteration 128, loss = 0.05288917\n","Iteration 129, loss = 0.05338160\n","Iteration 130, loss = 0.05157790\n","Iteration 131, loss = 0.05177091\n","Iteration 132, loss = 0.05046561\n","Iteration 133, loss = 0.04955009\n","Iteration 134, loss = 0.04913281\n","Iteration 135, loss = 0.04864283\n","Iteration 136, loss = 0.04783034\n","Iteration 137, loss = 0.04664856\n","Iteration 138, loss = 0.04610984\n","Iteration 139, loss = 0.04582450\n","Iteration 140, loss = 0.04580507\n","Iteration 141, loss = 0.04572713\n","Iteration 142, loss = 0.04374673\n","Iteration 143, loss = 0.04287567\n","Iteration 144, loss = 0.04304118\n","Iteration 145, loss = 0.04192652\n","Iteration 146, loss = 0.04135042\n","Iteration 147, loss = 0.04115862\n","Iteration 148, loss = 0.04004075\n","Iteration 149, loss = 0.04026781\n","Iteration 150, loss = 0.03939137\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Iteration 1, loss = 0.57220645\n","Iteration 2, loss = 0.34911748\n","Iteration 3, loss = 0.29282000\n","Iteration 4, loss = 0.26814296\n","Iteration 5, loss = 0.25483032\n","Iteration 6, loss = 0.24675926\n","Iteration 7, loss = 0.24143827\n","Iteration 8, loss = 0.23764481\n","Iteration 9, loss = 0.23466200\n","Iteration 10, loss = 0.23233978\n","Iteration 11, loss = 0.23041697\n","Iteration 12, loss = 0.22876809\n","Iteration 13, loss = 0.22735309\n","Iteration 14, loss = 0.22610099\n","Iteration 15, loss = 0.22498351\n","Iteration 16, loss = 0.22400455\n","Iteration 17, loss = 0.22313001\n","Iteration 18, loss = 0.22222638\n","Iteration 19, loss = 0.22147287\n","Iteration 20, loss = 0.22077793\n","Iteration 21, loss = 0.22011710\n","Iteration 22, loss = 0.21950517\n","Iteration 23, loss = 0.21889368\n","Iteration 24, loss = 0.21837656\n","Iteration 25, loss = 0.21779706\n","Iteration 26, loss = 0.21731642\n","Iteration 27, loss = 0.21685102\n","Iteration 28, loss = 0.21640196\n","Iteration 29, loss = 0.21595237\n","Iteration 30, loss = 0.21550420\n","Iteration 31, loss = 0.21511412\n","Iteration 32, loss = 0.21471167\n","Iteration 33, loss = 0.21431455\n","Iteration 34, loss = 0.21399632\n","Iteration 35, loss = 0.21369172\n","Iteration 36, loss = 0.21324177\n","Iteration 37, loss = 0.21294785\n","Iteration 38, loss = 0.21260678\n","Iteration 39, loss = 0.21230782\n","Iteration 40, loss = 0.21195923\n","Iteration 41, loss = 0.21166416\n","Iteration 42, loss = 0.21136694\n","Iteration 43, loss = 0.21114480\n","Iteration 44, loss = 0.21077611\n","Iteration 45, loss = 0.21050247\n","Iteration 46, loss = 0.21024519\n","Iteration 47, loss = 0.20998592\n","Iteration 48, loss = 0.20972997\n","Iteration 49, loss = 0.20942063\n","Iteration 50, loss = 0.20919658\n","Iteration 51, loss = 0.20898744\n","Iteration 52, loss = 0.20863622\n","Iteration 53, loss = 0.20842684\n","Iteration 54, loss = 0.20817408\n","Iteration 55, loss = 0.20792768\n","Iteration 56, loss = 0.20767233\n","Iteration 57, loss = 0.20745976\n","Iteration 58, loss = 0.20723129\n","Iteration 59, loss = 0.20705012\n","Iteration 60, loss = 0.20680053\n","Iteration 61, loss = 0.20653818\n","Iteration 62, loss = 0.20631623\n","Iteration 63, loss = 0.20604132\n","Iteration 64, loss = 0.20586790\n","Iteration 65, loss = 0.20563226\n","Iteration 66, loss = 0.20542941\n","Iteration 67, loss = 0.20518789\n","Iteration 68, loss = 0.20498309\n","Iteration 69, loss = 0.20477500\n","Iteration 70, loss = 0.20455938\n","Iteration 71, loss = 0.20440591\n","Iteration 72, loss = 0.20415019\n","Iteration 73, loss = 0.20391228\n","Iteration 74, loss = 0.20376004\n","Iteration 75, loss = 0.20351686\n","Iteration 76, loss = 0.20335234\n","Iteration 77, loss = 0.20311827\n","Iteration 78, loss = 0.20293393\n","Iteration 79, loss = 0.20271940\n","Iteration 80, loss = 0.20252580\n","Iteration 81, loss = 0.20231324\n","Iteration 82, loss = 0.20210386\n","Iteration 83, loss = 0.20192387\n","Iteration 84, loss = 0.20177875\n","Iteration 85, loss = 0.20155362\n","Iteration 86, loss = 0.20132376\n","Iteration 87, loss = 0.20113005\n","Iteration 88, loss = 0.20096247\n","Iteration 89, loss = 0.20079459\n","Iteration 90, loss = 0.20065802\n","Iteration 91, loss = 0.20039933\n","Iteration 92, loss = 0.20021535\n","Iteration 93, loss = 0.20002217\n","Iteration 94, loss = 0.19986121\n","Iteration 95, loss = 0.19965154\n","Iteration 96, loss = 0.19950666\n","Iteration 97, loss = 0.19930849\n","Iteration 98, loss = 0.19915234\n","Iteration 99, loss = 0.19893460\n","Iteration 100, loss = 0.19874820\n","Iteration 101, loss = 0.19858104\n","Iteration 102, loss = 0.19839242\n","Iteration 103, loss = 0.19820426\n","Iteration 104, loss = 0.19803305\n","Iteration 105, loss = 0.19794542\n","Iteration 106, loss = 0.19770587\n","Iteration 107, loss = 0.19751844\n","Iteration 108, loss = 0.19735051\n","Iteration 109, loss = 0.19712962\n","Iteration 110, loss = 0.19696284\n","Iteration 111, loss = 0.19680318\n","Iteration 112, loss = 0.19662487\n","Iteration 113, loss = 0.19648629\n","Iteration 114, loss = 0.19630649\n","Iteration 115, loss = 0.19611502\n","Iteration 116, loss = 0.19597264\n","Iteration 117, loss = 0.19579146\n","Iteration 118, loss = 0.19562015\n","Iteration 119, loss = 0.19544609\n","Iteration 120, loss = 0.19528993\n","Iteration 121, loss = 0.19510827\n","Iteration 122, loss = 0.19493816\n","Iteration 123, loss = 0.19488000\n","Iteration 124, loss = 0.19458747\n","Iteration 125, loss = 0.19444041\n","Iteration 126, loss = 0.19430166\n","Iteration 127, loss = 0.19416282\n","Iteration 128, loss = 0.19396186\n","Iteration 129, loss = 0.19375319\n","Iteration 130, loss = 0.19362879\n","Iteration 131, loss = 0.19344416\n","Iteration 132, loss = 0.19334325\n","Iteration 133, loss = 0.19315904\n","Iteration 134, loss = 0.19296333\n","Iteration 135, loss = 0.19283257\n","Iteration 136, loss = 0.19262914\n","Iteration 137, loss = 0.19248764\n","Iteration 138, loss = 0.19234327\n","Iteration 139, loss = 0.19219448\n","Iteration 140, loss = 0.19203486\n","Iteration 141, loss = 0.19180174\n","Iteration 142, loss = 0.19164520\n","Iteration 143, loss = 0.19155594\n","Iteration 144, loss = 0.19134017\n","Iteration 145, loss = 0.19122244\n","Iteration 146, loss = 0.19104160\n","Iteration 147, loss = 0.19093939\n","Iteration 148, loss = 0.19075485\n","Iteration 149, loss = 0.19057393\n","Iteration 150, loss = 0.19037075\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(estimator=MLPClassifier(),\n","             param_grid={'activation': ['relu', 'tanh'],\n","                         'alpha': [0.0001, 0.0005, 0.005],\n","                         'hidden_layer_sizes': [(60, 20), (30, 10)],\n","                         'max_iter': [150], 'solver': ['sgd', 'adam'],\n","                         'tol': [5e-05], 'verbose': [1]})"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["clf.best_params_"],"metadata":{"id":"Df-1-Wet-ctS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b39625c-7638-4abe-c67b-ccd49512d693"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'activation': 'tanh',\n"," 'alpha': 0.0005,\n"," 'hidden_layer_sizes': (60, 20),\n"," 'max_iter': 150,\n"," 'solver': 'sgd',\n"," 'tol': 5e-05,\n"," 'verbose': 1}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["print(f1_score(list(y_test), list(clf.predict(X_test))))"],"metadata":{"id":"ioctGo5T5tiP","outputId":"b7c1beb6-5f83-4ca6-8c49-6dad710dbced","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9000951474785918\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"hIXHs_5pbDLP"},"execution_count":null,"outputs":[]}]}